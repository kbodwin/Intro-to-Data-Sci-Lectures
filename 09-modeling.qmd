---
title: "Modeling"
format:
  revealjs: 
    theme: [default, spring.scss]
    incremental: true 
    output-file: slides-09-modeling.html
    shift-heading-level-by: -1
execute:
  echo: true
---

```{python}

```

```{r}
#| include: false
library(reticulate)
use_python("/usr/local/bin/python3")
```

```{python}
#| include: false
import numpy as np
import pandas as pd
from plotnine import *
```

## The story so far

### Steps for data analysis

* **Read** and then **clean** the data
  + Are there missing values?  Will we drop those rows, or replace the missing values with something?
  + Are there *quantitative* variables that python thinks are *categorical*?
  + Are there *categorical* variables that python thinks are *quantitative*?
  + Are there any *anomalies* in the data that concern you?
  
### Steps for data analysis (cont'd)

* **Explore** the data by **visualizing** and **summarizing**.
  + Different approaches for different combos of *quantitative* and *categorical* variables
  + Think about *conditional* calculations (split-apply-combine)
  
### Steps for data analysis (cont'd)

* Identify a **research question** of interest.

* Perform **preprocessing** steps
  + Should we *scale* the quantitative variables?
  + Should we *one-hot-encode* the categorical variables?
  + Should we *log-transform* any variables?


* Measure similarity between **observations** by calculating **distances**.
  + Which *features* should be included?
  + Which *distance metric* should we use?



## Machine Learning and Statistical Modeling

### Modeling

Every analysis we will do assumes a structure like:

::: {style="font-size: 150%;"}
(**output**) = f(**input**) + (noise)
:::

... or, if you prefer...

::: {style="font-size: 150%;"}
(**target**) = f(**predictors**) + (noise)
:::

### Generative process

In any case: we are trying to reconstruct information in **data**, and we are hindered by **random noise**.

The function $f$ might be very simple...

$$y_i = \mu + \epsilon_i$$

"A person's height is the true average height of people in the world, plus some randomness."

### Generative process

... or more complex...

$$y_i = 0.5*x_{1i} + 0.5*x_{2i} + \epsilon_i$$

"A person's height is equal to the average of their biological mother's height and biological father's height, plus some randomness"

* Do you think there is "more randomness" in the first function or this one?

### Generative process

... or extremely, ridiculously complex...

![](./images/complex_function.png)

### Generative process

... and it doesn't have to be a *mathematical* function at all; just a procedure:

$$y_i = \text{(average of heights of 5 people with most similar weights)} + \epsilon_i$$

### Modeling

* Our goal is to **reconstruct** or **estimate** or **approximate** the function/process $f$ based on **training data**.

* For example: Instead of the 5 most similar weights *in the whole world*, we can estimate with the 5 most similar weights *in our training set*.

* Instead of committing to one $f$ to estimate, we might propose **many** options and see which one "leaves behind" the least randomness.

## Data: Wine price prediction

### Setup

```{python}
import pandas as pd
from sklearn.compose import make_column_transformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler


import pandas as pd

df = pd.read_csv("https://dlsun.github.io/pods/data/bordeaux.csv",
                 index_col="year")
                 
df_train = df.loc[:1980].copy()
df_unknown = df.loc[1981:].copy()
```

### KNN revisited

```{python}
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsRegressor

ct = make_column_transformer(
  (StandardScaler(), ['summer', 'har', 'sep', 'win', 'age']),
  remainder = "drop"
)

pipeline = make_pipeline(
          ct,
          KNeighborsRegressor(n_neighbors=5))
pipeline.fit(X=df_train, y=df_train['price'])
pipeline.predict(X=df_unknown)
```

### Measuring error

The most common way to measure "leftover noise" is the **sum of squared error** or equivalently, the **mean squared error**

```{python}
#| code-fold: true
pred_y_train = pipeline.predict(X=df_train)
results = pd.DataFrame({
  "real_prices": df_train['price'],
  "predicted_prices": pred_y_train,
})
results["error"] = results["predicted_prices"] - results["real_prices"]
results["squared error"]= (results["error"])**2
results
```

### Measuring error

The most common way to measure "leftover noise" is the **sum of squared error** or equivalently, the **mean squared error**

```{python}
results["squared error"].mean()
```

### Best K

Now let's try it for some different values of $k$

```{python}
for k in [1, 3, 5, 10, 25]:
#| code-fold: true
  pipeline = make_pipeline(
    ct,
    KNeighborsRegressor(n_neighbors=k))
  pipeline = pipeline.fit(X=df_train, y=df_train['price'])
  pred_y_train = pipeline.predict(X=df_train)
  ((df_train['price'] - pred_y_train)**2).mean()

```

### Training error vs test error

* Oh no! Why did we get an error of 0 for $k = 1$?

* Because the closest wine in the training set is... itself.

* So, our problem is:
    + If we predict on the **new data**, we don't know the **true prices** and we can't **evaluate** our models
    + If we predict on the **training data**, we are "cheating", because we are using the data to both **train** and **test**.
    
* Solution: Let's make a pretend **test data** set!

### Test/Training split

```{python}
df_train = df.loc[:1970].copy()
df_test = df.loc[1971:1980].copy()
```

* We will **train** on the years up to 1970

* We will **test** on the years 1971 to 1980

* We will **evaluate** based on model performance on the **test data**.

### Try again: Best K

```{python}
for k in range(1,15):
#| code-fold: true
  pipeline = make_pipeline(
    ct,
    KNeighborsRegressor(n_neighbors=k))
  pipeline = pipeline.fit(X=df_train, y=df_train['price'])
  
  pred_y_test = pipeline.predict(X=df_test)
  ((df_test['price'] - pred_y_test)**2).mean()
```

### Tuning

* Here we tried the same **type** of model (KNN) each time.

* But we tried different **models** because we used different values of $k$

* This is called **tuning**

### Activity

Perform tuning for a KNN model, but with **all possible values of k**.

Do this for three *recipes* or *column transformers*:

1. Using all predictors.

2. Using just winter rainfall and summer temperature.

3. Using only age.

Which of the many model options performed best?

```{python}
ct2 = make_column_transformer(
  (StandardScaler(), ['summer', 'win']),
  remainder = "drop"
)
ct3 = make_column_transformer(
  (StandardScaler(), ['age']),
  remainder = "drop"
)
#| include: false
for k in range(1,17):
#| code-fold: true
  pipeline = make_pipeline(
    ct,
    KNeighborsRegressor(n_neighbors=k))
  pipeline = pipeline.fit(X=df_train, y=df_train['price'])
  pred_y_test = pipeline.predict(X=df_test)
  print(str(k) + ":" + str(((df_test['price'] - pred_y_test)**2).mean()))
  
for k in range(1,17):
#| code-fold: true
  pipeline = make_pipeline(
    ct2,
    KNeighborsRegressor(n_neighbors=k))
  pipeline = pipeline.fit(X=df_train, y=df_train['price'])
  pred_y_test = pipeline.predict(X=df_test)
  print(str(k) + ":" + str(((df_test['price'] - pred_y_test)**2).mean()))
  
for k in range(1,17):
#| code-fold: true
  pipeline = make_pipeline(
    ct3,
    KNeighborsRegressor(n_neighbors=k))
  pipeline = pipeline.fit(X=df_train, y=df_train['price'])
  pred_y_test = pipeline.predict(X=df_test)
  print(str(k) + ":" + str(((df_test['price'] - pred_y_test)**2).mean()))
```

### Things to think about{.smaller}

* What other **types of models** could we have tried?

* *Linear regression*, *decision tree*, *neural network*, ...

* What other **column transformers** could we have tried?

* *Different combinations of variables, different standardizing, log transforming...*

* What if we measure **error** differently?

* *Mean absolute error*, *log-error*, *percent error*, ...

* What if we had used a **different test set**?

* *Coming soon: Cross-validation*

* What if our target variable was **categorical**?


## Modeling: General Procedure

### Modeling{.smaller}

We apply the process:

<u>For each model proposed: </u>

* Establish a **pipeline** with **transformers** and a **model**.

* **Fit** the pipeline on the **training data** (with known outcome)

* **Predict** with the fitted pipeline on **test data** (with known outcome)

* **Evaluate** our success; i.e., measure noise "left over"

<u>Then: </u>

* **Select** the best model

* **Fit** on *all* the data

* **Predict** on any future data (with unknown outcome)

### Big decisions

* Which models to try

* Which column transformers to try

* How much to tune

* How to measure "success" of a model

