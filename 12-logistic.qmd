---
title: "Logistic Regression"
format:
  revealjs: 
    theme: [default, spring.scss]
    incremental: true 
    output-file: slides-12-logistic.html
    shift-heading-level-by: -1
execute:
  echo: true
---

```{python}

```

```{r}
#| include: false
library(reticulate)
use_python("/usr/local/bin/python3")
```

```{python}
#| include: false
import numpy as np
import pandas as pd
from plotnine import *
```

## The story so far

### Classification

* We can do **KNN for Classification** by letting the nearest neighbors "vote"

* The number of votes is a "probability"

* A **classification model** must be evaluated differently than a **regression model**.

* One possible metric is **accuracy**, but this is a bad choice in situations with **imbalanced data**.

* **Precision** measures "if we say it's in Class A, is it really?"

* **Recall** measures "if it's really in Class A, did we find it?"

* **F1 Score** is a balance of precision and recall

* **Macro F1 Score** averages the F1 scores of all classes

## Revisiting the Breast Cancer Data


### Breast Tissue Classification

Electrical signals can be used to detect whether tissue is cancerous.

::: center
![image](./images/breast_diagram.jpeg){width=".4\\textwidth"}
:::

The goal is to determine whether a sample of breast tissue is:

::::{.columns}
:::{.column}
**Not cancerous**
1. connective tissue
2. adipose tissue
3. glandular tissue
:::

:::{.column}
**Cancerous**
4. carcinoma
5. fibro-adenoma
6. mastopathy
:::

### Binary response: Cancer or not

Let's read the data, and also make a new variable called "Cancerous".

``` {python}
import pandas as pd
df = pd.read_csv("https://datasci112.stanford.edu/data/BreastTissue.csv")

df['Cancerous'] = (df['Class'] == "car") | (df['Class'] == "fad") | (df['Class'] == "mas")

df.head()
```
## Why not use "regular" regression?

**You should NOT use ordinary regression for a classification problem!  This slide section is to show you why it does NOT work.**

### Counter-Example: Linear Regression

We know that in computers, `True` = `1` and `False` = `0`.  So, why not convert our response variable, `Cancerous`, to numbers and fit a regression?

```{python}
df['Cancerous'] = df['Cancerous'].astype('int')
df.head()
```
### Example: OLS Regression

```{python}
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(
  LinearRegression())

pipeline = pipeline.fit(df[["I0", "PA500"]], df['Cancerous'])
```

### Counter-Example: Linear Regression

**Problem 1:** How do we predict *categories*???

```{python}
pred_cancer = pipeline.predict(df[["I0", "PA500"]])
pred_cancer
```


### Counter-Example: Linear Regression

**Problem 2:** Was the relationship really *linear*???

```{python}
#| code-fold: true

from plotnine import *
(ggplot(df, aes(x = "I0", y = "Cancerous")) 
+ geom_point()
+ geom_smooth(method = "lm", se = False)
+ theme_classic())
```


### Counter-Example: Linear Regression

**Problem 3:** Was the error really *random*???

```{python}
#| code-fold: true
residuals = df['Cancerous'] - pred_cancer

(ggplot(df, aes(x = "I0", y = residuals))
+ geom_point())
```

## Logistic Regression

### Logistic Regression

* **Idea:** Instead of predicting 0 or 1, try to predict the *probability* of cancer.

* **Problem:** We don't observe probabilities before diagnosis; we only know if that person ended up with cancer or not.

* **Solution:** (Fancy statistics and math.)

* Why is it called **Logistic Regression**?

* Because the "fancy math" uses a *logistic function* in it.

### Logistic Regression{.smaller}

What you need to know:

* It's used for **binary classification** problems.

* The predicted **values** are the "log-odds" of having cancer, i.e.

$$\text{log-odds} = \log \left(\frac{p}{1-p}\right)$$

* We are more interested in the **predicted probabilities**.

* As with KNN, we predict **categories** by choosing a **threshold**

* By default:  Probability > 0.5 -> Predict cancerous.

### Logistic Regression in `sklearn`

```{python}
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(
  LogisticRegression(penalty=None))

pipeline.fit(df[["I0", "PA500"]], df['Cancerous'])
```

### Logistic Regression in `sklearn`

```{python}
pred_cancer = pipeline.predict(df[["I0", "PA500"]])
pred_cancer
```

## Precison and Recall Revisited

### Confusion matrix{.smaller}

```{python}
from sklearn.metrics import confusion_matrix
pd.DataFrame(confusion_matrix(df['Cancerous'], pred_cancer), columns = pipeline.classes_, index = pipeline.classes_)
```

* Calculate the *precision* for predicting cancer.

* Calculate the *recall* for predicting cancer.

* Calculate the *precision* for predicting non-cancer.

* Calculate the *recall* for predicting non-cancer.

### Threshold

What if we had used different cutoffs besides $p > 0.5$?

```{python}
prob_cancer = pipeline.predict_proba(df[["I0", "PA500"]])
prob_cancer.round(2)
```

### Higher Threshold

What we used $p > 0.7$?

```{python}
prob_cancer = pipeline.predict_proba(df[["I0", "PA500"]])
pred_cancer_70 = prob_cancer[:,1] > .7
pred_cancer_70
```


### Higher Threshold

What we used $p > 0.7$?

```{python}
conf_mat = confusion_matrix(df['Cancerous'], pred_cancer_70)
pd.DataFrame(conf_mat, columns = pipeline.classes_, index = pipeline.classes_)
```

```{python}
precision_1 = conf_mat[1,1]/conf_mat[:,1].sum()
precision_1

recall_1 = conf_mat[1,1]/conf_mat[1, :].sum()
recall_1
```


### Lower Threshold

What we used $p > 0.2$?

```{python}
prob_cancer = pipeline.predict_proba(df[["I0", "PA500"]])
pred_cancer_20 = prob_cancer[:,1] > .2
pred_cancer_20
```

### Lower Threshold

```{python}
conf_mat = confusion_matrix(df['Cancerous'], pred_cancer_20)
pd.DataFrame(conf_mat, columns = pipeline.classes_, index = pipeline.classes_)
```

```{python}
precision_1 = conf_mat[1,1]/conf_mat[:,1].sum()
precision_1

recall_1 = conf_mat[1,1]/conf_mat[1, :].sum()
recall_1
```

### Precision-Recall Curve

```{python}
from sklearn.metrics import precision_recall_curve

precision, recall, thresholds = precision_recall_curve(
    df['Cancerous'], prob_cancer[:, 1])
    
df_pr = pd.DataFrame({
  "precision": precision,
  "recall": recall
})

```


### Precision-Recall Curve

```{python}

(ggplot(df_pr, aes(x = "recall", y = "precision"))
+ geom_point())
```


## Your turn

### Activity{.smaller}

Suppose you want to predict Cancer vs. No Cancer from breast tissue using a Logistic Regression. Should you use...

* Just `I0` and `PA500`?

* Just `DA` and `DP`?

* `I0`, `PA500`, `DA`, and `DP`?

* or all predictors?


Use **cross-validation** with **F1 Score** to decide!  

Then, fit your **final model** and report the **confusion matrix**.


*Completely optional - you will not be tested on this:  change `penalty = None` to `penalty = 'l1'` or `penalty = 'l2'` and tune the `alpha` parameter for your Logistic Regression models*

## Interpreting Logistic Regression

### Looking at coefficients

```{python}
pd.DataFrame({
  "Coefficients":pipeline['logisticregression'].coef_[0],
  "Column": ["I0", "PA500"]
  })
```


* "For every unit of I0 higher, we predict 0.003 **lower** log-odds of cancer"

* "For every unit of PA500 higher, we predict 11.73 **higher** log-odds of cancer."

### Feature Importance

* Does this mean that `PA500` is *more important* than `I0`?

::::{.columns}
:::{.column width="50%"}

```{python}
#| code-fold: true
(ggplot(df, aes(x = "PA500", group = "Cancerous", fill = "Cancerous"))
+ geom_density(alpha = 0.5, show_legend=False))
```

:::

:::{.column width="50%"}

```{python}
#| code-fold: true
(ggplot(df, aes(x = "I0", group = "Cancerous", fill = "Cancerous"))
+ geom_density(alpha = 0.5, show_legend=False))
```

:::

::::

### Standardization

* Does this mean that `PA500` is *more important* than `I0`?

* **Not necessarily.**  They have different *units* and so the coefficients mean different things.

* "For every **1000** units of I0 higher, we predict **3.0 lower** log-odds of cancer"

* "For every **0.1** unit of PA500 higher, we predict **1.1 higher** log-odds of cancer."

* What if we had *standardized* `I0` and `PA500`?

### Standardization{.smaller}

```{python}
from sklearn.preprocessing import StandardScaler

pipeline2 = make_pipeline(
  StandardScaler(),
  LogisticRegression(penalty=None)
  )

pipeline2 = pipeline2.fit(df[["I0", "PA500"]], df['Cancerous'])

pd.DataFrame({
  "Coefficients":pipeline2['logisticregression'].coef_[0],
  "Column": ["I0", "PA500"]
  })

```


* "For every **standard deviation above the mean** someone's `I0` is, we predict **2.3 lower** log-odds of cancer"

* "For every **standard deviation above the mean** someone's `PA500` is, we predict **0.80 higher** log-odds of cancer."

* Therefore, `I0` is more important!

### Standardization: Do you need it?

But - did this approach change our predictions at all?

```{python}
old_probs = pipeline.predict_proba(df[["I0", "PA500"]])
new_probs = pipeline2.predict_proba(df[["I0", "PA500"]])

pd.DataFrame({
  "without_stdize": old_probs[:,1], 
  "with_stdize": new_probs[:,1]
  })
```

### Standardization: Do you need it?{.smaller}

* Standardizing will **not change the predictions** for Linear or Logistic Regression!

* This is because the **coefficients** are chosen relative to the **units** of the predictors.  (Unlike in KNN!)

* Advantage of *not* standardizing:  More interpretable coefficients

* "For each unit of... " instead of "For each sd above the mean..."

* Advantage of *standardizing*:  Compare relative importance of predictors

* **It's up to you!**

* *(Don't use cross-validation to decide - you'll get the same metrics for both!)*

## Your turn

### Activity

For your Logistic Regression using **all** predictors, which two were most important?

How would you interpret the coefficients?

```{python}
#| include: false
#| eval: false

from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline

pipeline3 = make_pipeline(
  StandardScaler(),
  LogisticRegression(penalty=None)
  )

pipeline3 = pipeline3.fit(df[["I0", "PA500", 'HFS', 'DA', 'Area', 'A/DA', 'Max IP',
       'DR', 'P']], df['Cancerous'])

pd.DataFrame({
  "Coefficients":pipeline3['logisticregression'].coef_[0],
  "Column": ["I0", "PA500", 'HFS', 'DA', 'Area', 'A/DA', 'Max IP',
       'DR', 'P']
  })
```



## Takeaways

### Takeaways

* To fit a **regression model** (i.e., coefficients times predictors) to a **categorical response**, we use **Logistic Regression**.

* Coefficients are interpreted as "One unit increase in predictor is associated with a [something] increase in the log-odds of Category 1"

* We still use **cross-validated metrics** to decide between KNN and Logistic regression, and between different feature sets.

* We still report **confusion matrices** and sometimes **precision-recall curves** of our final model.
