---
title: "DS 112 Lecture Notes"
format: 
  html: 
    theme: [minty]
    toc: true
    toc-depth: 1
    number-sections: true
    css: styles.css
editor: visual
---



::: lectitle
# Tabular Data Summaries

[Slides](slides-01-tabular-data-summaries.html)
:::

---
title: "Tabular data and variable summaries"
format:  
  revealjs: 
    theme: [default, spring.scss]
    incremental: true 
    shift-heading-level-by: -1
execute:
  echo: true
---

```{r}
#| include: false
library(reticulate)
use_python("/usr/local/bin/python3")
```

```{python}
#| include: false
import numpy as np
import pandas as pd
```



### Data "on disk"

### Data is stored in plain text files

```
name,pclass,survived,sex,age,sibsp,parch,ticket,fare,cabin,embarked,boat,body,home.dest
"Allen, Miss. Elisabeth Walton",1,1,female,29,0,0,24160,211.3375,B5,S,2,,"St Louis, MO"
"Allison, Master. Hudson Trevor",1,1,male,0.9167,1,2,113781,151.5500,C22 C26,S,11,,"Montreal, PQ / Chesterville, ON"
"Allison, Miss. Helen Loraine",1,0,female,2,1,2,113781,151.5500,C22 C26,S,,,"Montreal, PQ / Chesterville, ON"
"Allison, Mr. Hudson Joshua Creighton",1,0,male,30,1,2,113781,151.5500,C22 C26,S,,135,"Montreal, PQ / Chesterville, ON"
"Allison, Mrs. Hudson J C (Bessie Waldo Daniels)",1,0,female,25,1,2,113781,151.5500,C22 C26,S,,,"Montreal, PQ / Chesterville, ON"
"Anderson, Mr. Harry",1,1,male,48,0,0,19952,26.5500,E12,S,3,,"New York, NY"
"Andrews, Miss. Kornelia Theodosia",1,1,female,63,1,0,13502,77.9583,D7,S,10,,"Hudson, NY"
"Andrews, Mr. Thomas Jr",1,0,male,39,0,0,112050,0.0000,A36,S,,,"Belfast, NI"
"Appleton, Mrs. Edward Dale (Charlotte Lamson)",1,1,female,53,2,0,11769,51.4792,C101,S,D,,"Bayside, Queens, NY"
"Artagaveytia, Mr. Ramon",1,0,male,71,0,0,PC 17609,49.5042,,C,,22,"Montevideo, Uruguay"
"Astor, Col. John Jacob",1,0,male,47,1,0,PC 17757,227.5250,C62 C64,C,,124,"New York, NY"
```

* This is called a **csv** (*comma-separated*) file.

* You might see it as `something.csv` or `something.txt`

* `.txt` files might have different separators

### Reading data

We *read the data* into a program like `python` by specifying:

* what **type** of file it is

* **where** the csv file is located (the **"path"**)

* if the file has a **header**

* ... and other information in special cases!

### Example using `pandas` data frame:

```{python}
df = pd.read_csv("https://datasci112.stanford.edu/data/titanic.csv")
df
```

### Check in:

```{python}
df = pd.read_csv("https://datasci112.stanford.edu/data/titanic.csv")
```

:::{.callout}

* What if this file lived on a computer instead of online?

* Why didn't we have to specify that this dataset has a header?

:::

### Looking at rows

```{python}
df.loc[1, :]
df.iloc[1, :]
```

### Looking at rows

:::{.callout}

* What is the difference between `.loc` and `.iloc`?

* What **type** of object is returned?

:::

### loc, iloc, and index

```{python}
#| error: true
df2 = df.set_index('name')
df2.loc[1, :]
df2.iloc[1, :]
```
### loc, iloc, and index

Think of `iloc` as **integer location**.

```{python}
#| error: true
df2.loc["Allison, Master. Hudson Trevor", :]
```


### Looking at columns

```{python}
df.columns
df['home.dest']
```

### Caution: Object types

```{python}

type(df)
type(df.loc[1, :])
type(df['name'])
```




## Summarizing a data frame

### Questions to ask

* Which variables (columns) are **categorical**?

* Which variables are **quantitative**?

* Which variables are **labels** (e.g. names or ID numbers)?

* Which variables are **text**?




### A quick look at the data

```{python}
df.describe()
```
:::{.callout}

* What percent of *Titanic* passengers survived?

* What was the average (mean) fare paid for a ticket?

* What percent of *Titanic* passengers were in First Class?

:::

### Variable types

* The variable `pclass` was **categorical**, but python assumed it was *quantitative*.

* It's our job to check and fix data!

```{python}
df["pclass"] = df["pclass"].astype("category")
```

### Summary of categorical variable

```{python}
df["pclass"].value_counts()
df["pclass"].value_counts(normalize = True)
```




::: lectitle
# Visualization and Conditional Distributions

[Slides](slides-02-conditional_distributions.html)
:::

---
title: "Visualizing and Comparing Categorical Variables"
format:
  revealjs: 
    theme: [default, spring.scss]
    incremental: true 
    output-file: slides-02-conditional-distributions.html
    shift-heading-level-by: -1
execute:
  echo: true
---

```{r}
#| include: false
library(reticulate)
use_python("/usr/local/bin/python3")
```

```{python}
#| include: false
import numpy as np
import pandas as pd
```

## The story so far

### Getting and prepping data

```{python}
df = pd.read_csv("https://datasci112.stanford.edu/data/titanic.csv")
```

```{python}
df["pclass"] = df["pclass"].astype("category")
df["survived"] = df["survived"].astype("category")
```

### Thinking about variable types

```{r}
#| echo: false
library(knitr)
kable(py$df)
```

### Accessing rows and columns

::: columns
::: {.column width="50%"}
```{python}
df.iloc[5,]
```
:::

::: {.column width="50%"}
```{python}
df["name"]
```
:::
:::

### Quick summary of quantitative variables

```{python}
df.describe()
```

### Summarizing categorical variables

The list of percents for each category is called the **distribution** of the variable.

```{python}
df["pclass"].value_counts()
df["pclass"].value_counts(normalize = True)
```

## Visualizing categorical variables

### The Grammar of Graphics

The *grammar of graphics* is a framework for creating data visualizations.

A visualization consists of:

-   The **aesthetic**: Which *variables* are dictating which *plot elements*.

-   The **geometry**: What *shape* of plot you are making.

-   The **theme**: Other choices about the appearance.

### Example

::: columns
::: {.column width="50%"}
The **aesthetic** is **species** on the x-axis, **bill_length_mm** on the y-axis, colored by **species**.

The **geometry** is a **boxplot**.
:::

::: {.column width="50%"}
```{python}
#| code-fold: true
import pandas as pd
from palmerpenguins import load_penguins
from plotnine import ggplot, geom_point, aes, geom_boxplot

penguins = load_penguins()

(ggplot(penguins, aes(x = "species", y = "bill_length_mm", fill = "species"))
+ geom_boxplot()
)
```
:::
:::

### plotnine

The `plotnine` library implements the *grammar of graphics* in Python.

Code for the previous example:

```{python}
#| eval: false
(ggplot(penguins, aes(x = "species", y = "bill_length_mm", fill = "species"))
+ geom_boxplot()
)
```

-   The `aes()` function is the place to specify aesthetics.

-   `x`, `y`, and `fill` are three possible aesthetics that can be specified, that map variables in our data set to plot elements.

-   A variety of `geom_*` functions allow for different plotting shapes (e.g. boxplot, histogram, etc.)

#### Themes

::: columns
::: {.column width="50%"}
```{python}
#| code-fold: true
(ggplot(penguins, aes(x = "species", y = "bill_length_mm", fill = "species"))
+ geom_boxplot()
)
```
:::

::: {.column width="50%"}
```{python}
#| code-fold: true
from plotnine import theme_classic
(ggplot(penguins, aes(x = "species", y = "bill_length_mm", fill = "species"))
+ geom_boxplot()
+ theme_classic()
)
```
:::
:::

### Check-In

What are the *aesthetics* and *geometry* in the cartoon plot below?

![An xkcd comic of time spent going up the down escalator](images/xkcd.png)

### Bar Plots

To visualize the **distribution** of a categorical variable, we should use a **bar plot**.

```{python}
#| code-fold: true
from plotnine import *
(ggplot(df, aes(x = "pclass"))
+ geom_bar()
+ theme_classic()
)
```

### Percents

```{python}
pclass_dist = df['pclass'].value_counts(normalize=True).to_frame().reset_index()
pclass_dist
```

### Percents

```{python}
#| code-fold: true
(ggplot(pclass_dist, aes(x = "pclass", y = "proportion"))
+ geom_col()   ### notice this change to a column plot!
+ theme_classic()
)
```

## Visualizing two categorical variables

### Option 1: Stacked Bar Plot

```{python}
#| code-fold: true
(ggplot(df, aes(x = "pclass", fill = "sex"))
+ geom_bar()
+ theme_classic()
)
```

### Option 1: Stacked Bar Plot

::: callout
What are some pros and cons of the stacked barplot?
:::

-   Pros:

-   We can still see the total counts in each class

-   We can easily compare the `male` counts in each class, since those bars are on the bottom.

-   Cons:

-   It is hard to compare the `female` counts, since those bars are stacked on top.

-   It is hard to estimate the *distributions*.

### Option 2: Side-by-side barplot

```{python}
#| code-fold: true
(ggplot(df, aes(x = "pclass", fill = "sex"))
+ geom_bar(position = "dodge")
+ theme_classic()
)
```

### Option 2: Side-by-side barplot

::: callout
What are some pros and cons of the side-by-side barplot?
:::

-   Pros:

-   We can easily compare the `female` counts in each class

-   We can easily compare the `male` counts in each class

-   We can easily see counts of each within each class

-   Cons:

-   It is hard to see *total* counts in each class.

-   It is hard to estimate the *distributions*.

### Option 3: Stacked percentage barplot

```{python}
#| code-fold: true
(ggplot(df, aes(x = "pclass", fill = "sex"))
+ geom_bar(position = "fill")
+ theme_classic()
)
```

### Option 3: Stacked percentage barplot

-   Cons:

-   We can no longer see **any** counts!

-   Pros:

-   This is the **best** way to compare sex balance across classes!

-   This is the option I use the most, because it can answer "Are you more likely to find \_\_\_\_\_\_ in \_\_\_\_\_\_ ?" type questions.

## Activity {.smaller}

Choose one of the plots from lecture so far and "upgrade" it.

::: columns
::: {.column width="60%"}
You can do this by:

-   Finding and using a different `theme`

-   Using `+ labs(...)` to change the axis labels and title

-   Trying different variables

-   Trying a different *geometry*

-   Using `+ scale_fill_manual(...)` to change the colors being used


```{r}
#| echo: false
library(countdown)
countdown(minutes = 15, left = "0")
```
:::

::: {.column width="40%"}
Hints:

-   You will need to use documentation of `plotnine` and online resources!

-   Check out <https://www.data-to-viz.com/> for ideas and example code.

-   Ask GenAI questions like, "What do I add to a plotnine bar plot to change the colors?" *(But of course, make sure you understand the code you use!)*

:::
:::

## Joint distributions

### Two categorical variables

```{python}
df[["pclass", "sex"]].value_counts()
```
### Two-way Table

```{python}
df[["pclass", "sex"]].value_counts().unstack()
```
* This is sometimes called a *cross-tab* or *cross-tabulation*.

### Two-way Table - Percents

```{python}
df[["pclass", "sex"]].value_counts(normalize=True).unstack()
```
* This should add up to 1, aka, 100%!

### Switching variables

```{python}
df[["sex", "pclass"]].value_counts(normalize=True).unstack()
```
### Interpretation

We call this the **joint distribution** of the two variables.

```{python}
#| echo: false
df[["pclass", "sex"]].value_counts(normalize=True).unstack()
```

> Of all the passengers on the Titanic, 11% were females riding in first class.

* NOT "11% of all females on Titanic..."
* NOT "11% of all first class passengers..."


## Conditional distributions

### Conditional distribution from counts

We know that:

* 466 passengers were **female**

* 144 passengers were **females in first class**

So:

* 144/466 = 31% **of female passengers** rode in first class

Here we **conditioned on** the passenger being female, and then looked at the **conditional distribution** of `pclass`.


### Conditional distribution from counts

We know that:

* 35.5% of all passengers were **female**

* 11% of all passengers were **females in first class**

So:

* 0.11/0.355 = 31% **of female passengers** rode in first class

Here we **conditioned on** the passenger being female, and then looked at the **conditional distribution** of `pclass`.

### Swapping variables

We know that:

* 323 passengers **rode in first class**

* 144 passengers were **females in first class**

So:

* 144/323 = 44.6% **of first class passengers** were female

Here we **conditioned on** the passenger being in first class, and then looked at the **conditional distribution** of `sex`.

### Which one to condition on?

* This depends on the **research question** you are trying to answer.

* "What class did most female passengers ride in?"

* ->  Of all *female passengers*, what is the conditional distribution of *class*?

* "What was the gender breakdown of first class?"

* -> Of all *first class passengers*, what is the conditional distribution of *sex*?

### Calculating in python

When we study two variables, we call the individual one-variable distributions the **marginal distribution** of that variable.

::::{.columns}

:::{.column width="50%"}

```{python}
marginal_class = df['pclass'].value_counts(normalize = True)
marginal_class
```

:::

:::{.column width="50%"}

```{python}
marginal_sex = df['sex'].value_counts(normalize = True)
marginal_sex
```

:::

::::

### Calculating in python

We need to divide the **joint distribution** (e.g. "11% of passengers were first class female") by the **marginal distribution** of the variable we want to **condition on** (e.g. 35.5% of passengers were female).

```{python}
joint_class_sex = df[["pclass", "sex"]].value_counts(normalize=True).unstack()
joint_class_sex.divide(marginal_sex)
```
### Conditional on sex

Check:  Should the **rows** or **columns** add up to 100%?  Why?

```{python}
#| echo: false
joint_class_sex = df[["pclass","sex"]].value_counts(normalize=True).unstack()
joint_class_sex.divide(marginal_sex)
```

### Check-In (on the board)

How does `.divide()` work?


### Conditional on class

```{python}
joint_class_sex = df[["sex","pclass"]].value_counts(normalize=True).unstack()
joint_class_sex.divide(marginal_class)
```
### What if you get it backwards?

```{python}
joint_class_sex = df[["pclass","sex"]].value_counts(normalize=True).unstack()
joint_class_sex.divide(marginal_class)
```

### Visualization

Which plot better answers the question, "Did women tend to ride in first class more than men?"

::::{.columns}

:::{.column width="50%"}
```{python}
#| code-fold: true
(ggplot(df, aes(x = "pclass", fill = "sex"))
+ geom_bar(position = "fill")
+ theme_classic()
)
```
:::

:::{.column width="50%"}

```{python}
#| code-fold: true
(ggplot(df, aes(x = "sex", fill = "pclass"))
+ geom_bar(position = "fill")
+ theme_classic()
)
```

:::
::::


## Takeaways

### Takeaways{.smaller}

* We use `plotnine` and the **grammar of graphics** to make visuals.

* For two categorical variables, we might use a **stacked bar plot**, a **side-by-side bar plot**, or a **stacked percentage bar plot** - depending on what we are trying to show.

* The **joint distribution** of two variables gives the percents in each subcategory.

* The **marginal distribution** of a variable is its individual distribution.

* The **conditional distribution** of a variable is its distribution among *only one category* of a different variable.

* We calculate the **conditional distribution** by dividing the **joint** by the **marginal**.



::: lectitle
# Visualizing and Summarizing Quantitative Variables

[Slides](slides-03-quantitative_variables.html)
:::

---
title: "Visualizing and Summarizing Quantitative Variables"
format:
  revealjs: 
    theme: [default, spring.scss]
    incremental: true 
    output-file: slides-03-quantitative_variables.html
    shift-heading-level-by: -1
execute:
  echo: true
---

```{r}
#| include: false
library(reticulate)
use_python("/usr/local/bin/python3")
```

```{python}
#| include: false
import numpy as np
import pandas as pd
from plotnine import *
```

## The story so far

### Getting, prepping, and summarizing data

```{python}
df = pd.read_csv("https://datasci112.stanford.edu/data/titanic.csv")
df["pclass"] = df["pclass"].astype("category")
df["survived"] = df["survived"].astype("category")
```

### Marginal Distributions

If I choose a passenger **at random**, what is the **probability** they rode in 1st class?

```{python}
marginal_class = df['pclass'].value_counts(normalize = True)
marginal_class
```

### Joint Distributions

If I choose a passenger **at random**, what is the **probability** they are a woman who rode in first class?

```{python}
joint_class_sex = df[["pclass", "sex"]].value_counts(normalize=True).unstack()
joint_class_sex
```


### Conditional Distributions

If I choose a **woman** at random, what is the probability they rode in first class?

```{python}
marginal_sex = df['sex'].value_counts(normalize = True)
joint_class_sex.divide(marginal_sex)

```

### Visualizing with the Grammar of Graphics

```{python}
(ggplot(df, aes(x = "sex", fill = "pclass"))
+ geom_bar(position = "fill")
+ theme_classic()
)
```


## Quantitative Variables

We have analyzed a quantitative variable already. Where?

In the Colombia COVID data!

``` {python}
df_CO = pd.read_csv("http://dlsun.github.io/pods/data/covid/colombia_2020-05-28.csv")
df_CO
```

## Visualizing One Quantitative Variable

### Option: Convert it to categorical

To visualize the age variable, we did the following:

``` {python}
df_CO["age"] = pd.cut(
    df_CO["Edad"],
    bins=[0, 10, 20, 30, 40, 50, 60, 70, 80, 120],
    labels=["0-9", "10-19", "20-29", "30-39", "40-49", "50-59", "60-69", "70-79", "80+"],
    right=False)
    
```

### Option: Convert it to categorical

Then, we could treat `age` as categorical and make a **barplot**:

```{python}
#| code-fold: true
(ggplot(df_CO, aes(x = "age"))
+ geom_bar()
+ theme_classic()
)
```


### Better option:  Histogram

A **histogram** uses *equal sized bins* to summarize a *quantitative variable*.

``` {python}
(ggplot(df_CO, aes(x = "Edad"))
+ geom_histogram()
+ theme_classic()
)
```

### Histogram

A histogram **must** use a **quantitative variable** to look right:

``` {python}
#| error: true
(ggplot(df_CO, aes(x = "age"))
+ geom_histogram()
+ theme_classic()
)
```

### Histogram

To tweak your histogram, you can change the **number of bins**:

::::{.columns}

:::{.column width="50%"}

``` {python}
#| code-fold: true
(ggplot(df_CO, aes(x = "Edad"))
+ geom_histogram(bins = 10)
+ theme_classic()
)
```
:::

:::{.column width="50%"}

``` {python}
#| code-fold: true
(ggplot(df_CO, aes(x = "Edad"))
+ geom_histogram(bins = 100)
+ theme_classic()
)
```

:::
::::

### Percents instead of counts

```{python}
(ggplot(df_CO, aes(x = "Edad", y = '..density..'))
+ geom_histogram(bins = 10)
+ theme_classic()
)
```


### Distributions

* Recall the distribution of a categorical variable: What are the **possible values** and **how common** is each?

* The **distribution** of a quantitative variable is similar: The  total *area* in the histogram is 1.0 (or 100%).

``` {python}
#| code-fold: true
(ggplot(df_CO, aes(x = "Edad", y = '..density..'))
+ geom_histogram(bins = 10)
+ theme_classic()
)
```

### Densities

* In this example, we have a limited set of possible values for `age`:  0, 1, 2, ...., 100.  We call this **discrete**.

* What if had a **quantitative variable** with **infinite values**?

* For example:  Price of a ticket on Titanic.

* We call this **continuous**.

* In this case, it is not possible to list all **possible values** and **how likely each one is**.
    + One person paid $2.35
    + Two people paid $12.50
    + One person paid $34.98
    + .....?
    
* Instead, we talk about **ranges** of values.


### Densities

About what percent of people in this dataset are below 18?

``` {python}
#| code-fold: true
(ggplot(df_CO, aes(x = "Edad", y = '..density..'))
+ geom_histogram(bins = 10)
+ theme_classic()
)
```

### Densities

About what percent of people in this dataset are below 18?

```{python}
#| code-fold: true
(ggplot(df_CO, aes(x = "Edad"))
+ geom_density()
+ theme_classic()
)
```


## Summarizing One Quantitative Variable

### Summarizing a Quantitative Variable

If you had to summarize this variable with **one single number**, how would you pick?

```{python}
df_CO['Edad']
```


## Summaries of Center: Mean

### Mean

* One summary of the center of a quantitative variable is the **mean**.

* When you hear "The average age is..." or the "The average income is...", this probably refers to the mean.

* Suppose we have five people, ages: `4, 84, 12, 27, 7`

* The **mean age** is: $$(4 + 84 + 12 + 27 + 7)/5 = 134/5 = 26.8$$

### Notation interlude {.smaller}

* To refer to our data without having to list all the numbers, we use $x_1, x_2, ..., x_n$

* In the previous example, $x_1 = 4, x_2 = 84, x_3 = 12, x_4 = 27, x_5 = 7$.  So, $n = 5$.

* To add up all the numbers, we use the **summation notation**:
$$ \sum_{i = 1}^n x_i = 134$$

* Therefore, the **mean** is:
$$\bar{x} = \frac{1}{n} \sum_{i = 1}^n x_i$$


### Means in python

Long version: find the **sum** and the **number of observations**

``` {python}
sum_age = df_CO["Edad"].sum()
n = len(df_CO)

sum_age/n
```

Short version: use the built-in function!

``` {python}
df_CO["Edad"].mean()
```


### Activity

The mean is only **one option** for summarizing the center of a quantitative variable.  It isn't perfec!

Let's investigate this.

* Plot the density of ticket prices on titanic

* Calculate the *mean* price

* See how many people paid *more than mean price*


### What happened

* Our `fare` data was **skewed right**:  Most values were small, but a few values were very large.

* These large values "pull" the mean up; just how the value `84` pulled the average age up in our previous example.

* So, why do we *like* the mean?

### Squared Error{.smaller}

* Recall: Ages 4, 84, 12, 27, 7.

* Imagine that we had to "guess" the age of the next person.

* If we guess 26.8, then our "squared error" for these five people is:

```{python}
ages = np.array([4, 84, 12, 27, 7])
sq_error = (ages - 26.8)**2
sq_error.round(decimals = 1)
```

* If we guess 20, then our "squared error" for these five people is:

```{python}
sq_error = (ages - 20)**2
sq_error.round(decimals = 1)
```


### Minimizing squared error

```{python}
#| code-fold: true
cs = range(1, 60)
sum_squared_distances = []

for c in cs:
  sum_squared_distances.append(((df_CO["Edad"] - c) ** 2).sum())

res_df = pd.DataFrame({"center": cs, "sq_error":sum_squared_distances})

(ggplot(res_df, aes(x = 'center', y = 'sq_error'))
+ geom_line())
```


## Summaries of Center: Median

### Median{.smaller}

Another summary of center is the **median**, which is the "middle" of
the *sorted* values.

To calculate the median of a quantitative variable with
values $x_1, x_2, x_3, ..., x_n$, we do the following steps:

1.  Sort the values from smallest to largest:
    $$x_{(1)}, x_{(2)}, x_{(3)}, ..., x_{(n)}.$$

2.  The "middle" value depends on whether we have an odd or an even
    number of observations.

    -   If $n$ is odd, then the middle value is $x_{(\frac{n+1}{2})}$.

    -   If $n$ is even, then there are two middle values,
        $x_{(\frac{n}{2})}$ and $x_{(\frac{n}{2} + 1)}$. It is
        conventional to report the mean of the two values (but you can
        actually pick any value between them).

### Median

Ages: 4, 84, 12, 7, 27.  What is the **median**?

Median age in the Columbia data:

``` {python}
df_CO["Edad"].median()
```


## Summaries of Spread: Variance

### Variance

* One measure of spread is the **variance**.

* The variance of a variable whose values are $x_1, x_2, x_3, ..., x_n$ is calculated using the formula
$$\textrm{var(X)} = \frac{\sum_{i=1}^n (x_i - \bar{x})^2}{n - 1}$$

* Does this look familiar?  It's the **sum of squared error**!  (Divided by $n-1$, the "degrees of freedom")

### Variance in python

We could do this manually:

```{python}
(((df_CO["Edad"] - df_CO["Edad"].mean()) ** 2).sum() /
 (len(df_CO) - 1))
```

...or using a built-in Python function.

``` {python}
df_CO["Edad"].var()
```

    348.0870469898451

## Standard Deviation

* Notice that the variance isn't very *intuitive*: what do we mean by "The spread is 348"?

* This is because it is the **squared** error!

* So, to get it in more interpretable language, we take the square root:

```{python}
np.sqrt(df_CO["Edad"].var())
```


Or, we use the built-in function!

``` {python}
df_CO["Edad"].std()
```


## Takeaways

### Takeaway Messages

* Visualize quantitative variables with **histograms** or **densities**.

* Summarize the **center** of a quantitative variable with **mean** or **median**.

* Describe the **shape** of a quantitative variable with **skew**

* Summarize the **spread** of a quantitative variable with the **variance** or the **standard deviation**.



::: lectitle
# Multivariate Summaries

[Slides](slides-04-group_by.html)
:::

---
title: "Multivariate summaries"
format:
  revealjs: 
    theme: [default, spring.scss]
    incremental: true 
    output-file: slides-04-group_by.html
    shift-heading-level-by: -1
execute:
  echo: true
---

```{r}
#| include: false
library(reticulate)
use_python("/usr/local/bin/python3")
```

```{python}
#| include: false
import numpy as np
import pandas as pd
from plotnine import *
```

## The story so far

### Last week

* Reading in data and cleaning/prepping it

* Summarizing **one categorical variable** with a distribution

* Summarizing **two categorical variables** with joint and conditional distributions

* Using `plotnine` and the **grammar of graphics** to make **bar plots** and **column plots**.

### Quantitative variables

* Visualizing by **converting to categorical**

* Visualizing with **histograms** or **densities**

* Estimating **probabilities** from histograms and densities.

* Describing the **skew**

* Calculating and explaining the **mean** and the **median**.

* Calculating and explaining the **standard deviation** and **variance**.

## Comparing quantities across categories

### New dataset: airplane flights

RQ:  Which airline carriers are most likely to be delayed?

Let's look at a data set of all domestic flights that departed from one
of the New York City airports (JFK, LaGuardia, and Newark) on November
16, 2013.

```{python}
data_dir = "https://datasci112.stanford.edu/data/"
df = pd.read_csv(data_dir + "flights_nyc_20131116.csv")
df
```
### Delays

We already know how to **summarize** the flight delays:

*(Check-in: Interpret these numbers!)*

```{python}
df['dep_delay'].median()
df['dep_delay'].mean()
df['dep_delay'].std()
```

### Delays

We already know how to **visualize** the flight delays:

*(Check-in: How would you describe this distribution?)*

```{python}
(ggplot(df, aes(x = 'dep_delay'))
+ geom_histogram()
+ theme_classic())
```

### Delays by origin

RQ: Do the three origin airports (JFK, LGA, EWR) have different delay patterns?

*Check-in:* What could you change in this code to include the `origin` variable?

```{python}
#| eval: false
(ggplot(df, aes(x = 'dep_delay'))
+ geom_histogram()
+ theme_classic())
```

### Delays by origin

Overlapping **histograms** can be really hard to read...

```{python}
#| code-fold: true
(ggplot(df, aes(x = 'dep_delay', fill = 'origin'))
+ geom_histogram()
+ theme_classic())
```

### Delays by origin

... but overlapping **densities** often look nicer...

```{python}
#| code-fold: true
(ggplot(df, aes(x = 'dep_delay', fill = 'origin'))
+ geom_density()
+ theme_classic())
```


### Delays by origin

... especially if we make them a little see-through!

```{python}
#| code-fold: true
(ggplot(df, aes(x = 'dep_delay', fill = 'origin'))
+ geom_density(alpha = 0.5)
+ theme_classic())
```

### Variable transformations

* That last plot was okay, but it was hard to see the details, because the distribution is so **skewed right**.

* Sometimes, for easier visualization, it is worth **transforming** a variable.  

* Often we use a **log** transformation.


### Log transformation{.smaller}

* Example:  Salaries of \$10,000 and \$100,000 and \$10,000,000:

```{python}
#| code-fold: true
dat = pd.DataFrame({"salary": [10000, 100000, 10000000]})
dat["log_salary"] = np.log(dat["salary"])
```


::::{.columns}

:::{.column width="50%"}

```{python}
#| code-fold: true

(ggplot(dat, aes(x = "salary"))
+ geom_histogram(bins = 100)
+ theme_classic())

```


:::

:::{.column width="50%"}

```{python}
#| code-fold: true

(ggplot(dat, aes(x = "log_salary"))
+ geom_histogram(bins = 100)
+ theme_classic())

```
:::

::::

### Log transformations

* Usually, we use the **natural log**, just for convenience.

* Pros:  Skewed data looks **less skewed**, so it is easier to see **patterns**

* Cons: The variable is now on a **different scale** so it is not as **interpretable**

* **Caution!** Log transformations need *positive numbers*.

### Delays by origin - transformed


```{python}
#| code-fold: true

# Shift delays to be above zero
df['delay_shifted'] = df['dep_delay'] - df['dep_delay'].min() + 1

# Log transform
df['log_delay'] = np.log(df['delay_shifted'])

(ggplot(df, aes(x = 'log_delay', fill = 'origin'))
+ geom_density(alpha = 0.5)
+ theme_classic())
```
## Boxplots

### Another option: Boxplots

```{python}
#| code-fold: true

(ggplot(df, aes(y = 'log_delay', x = 'origin'))
+ geom_boxplot()
+ theme_classic())
```

## Facetting

### Facetting

* This plot still was a *little* hard to read.  

* What if we just made **separate plots** for each origin?

* Kind of annoying...


```{python}
is_jfk = (df['origin'] == "JFK")
df_jfk = df[is_jfk]
df_jfk

```

### (Aside: Boolean masking)


```{python}

is_jfk = (df['origin'] == "JFK")
is_jfk

```

### Facetting

Fortunately, `plotnine` (and other plotting packages) has a trick for you!

```{python}
(ggplot(df, aes(x = 'dep_delay'))
+ geom_density()
+ facet_wrap('origin'))
```


## Summaries by groups

### Split-apply-combine{.smaller}

* Our visualizations told us some of the story, but can we use **numeric summaries** as well?

* To do this, we want to calculate the **mean** or **median** delay time for **each** origin airport.

* We call this **split-apply-combine**:  
    + **split** the dataset up by a categorical variable `origin`
    + **apply** a calculation like `mean`
    + **combine** the results back into one dataset
    
* In `pandas`, we use the `groupby()` function to take care of the *split* and *combine* steps!

### Group-by

```{python}
df.groupby("origin")["dep_delay"].mean()
df.groupby("origin")["dep_delay"].median()
```
### Group-by

*Check-in:*  

* Which code is causing "split by origin"?

* Which code is causing "calculate the mean of delays"?

* Which code is causing the re-combining of the data?

```{python}
#| eval: false
df.groupby("origin")["dep_delay"].mean()
```

## Standardized values

### Toy example: exam scores

Hermione's exam scores are is:

* Potions class: 77/100

* Charms class: 95/100

* Herbology class: 90/100

In which class did she do best?


### Toy example: exam scores

**But wait!**

The class **means** are:

* Potions class: 75/100

* Charms class: 85/100

* Herbology class: 85/100

In which class did she do best?

### Toy example: exam scores

**But wait!**

The class **standard deviations** are:

* Potions class: 2 points

* Charms class: 5 points

* Herbology class: 1 point

In which class did she do best?


### Different variabilities by origin

* In addition to having different **centers**, the three origins also have different **spreads**.

```{python}
df.groupby("origin")["dep_delay"].std()
```


* That is, values are **less close** to the mean in general for flights from `LGA` than from `JFK`. 


### Standardized values

Suppose you fly from `LGA` and your flight is 40 minutes late

Your friend flies from `JFK` and their flight is 30 minutes late

Who got "unluckier"?

```{python}
(40 + 0.48)/26
(30 - 1.46)/18.7
```

### Standardized values

* We **standardize** values by subtracting the mean and dividing by the standard deviation.

* This tells us how much better/worse *than typical values* our target value is.

* This is also called the **z-score**. $$z_i = \frac{x_i - \bar{x}}{s_x}$$


### Activity

Research question:  What airlines have the most delays?

* Make a **plot** to answer the question.

* Calculate values to answer the question.

* The first row is a flight from EWR to CLT on US Airways.  The second row is a flight from LGA to IAH on United Airlines.  Which one was a "more extreme" delay?

## Relationships between quantitative variables

### Scatter plots

To **visualize** two quantitative variables, we make a **scatter plot** (or *point* geometry).

RQ: Did *older* passengers pay a *higher* fare on the Titanic?

```{python}
df_titanic = pd.read_csv(data_dir + "titanic.csv")

(ggplot(df_titanic, aes(x = 'age', y = 'fare'))
+ geom_point())
```

### Scatter plots

Notice:

* The **explanatory variable** was on the **x-axis**.

* The **response variable** was on the **y-axis**.

* "If you are older, you pay more" not "If you pay more, you get older".

```{python}
#| eval: false
(ggplot(df_titanic, aes(x = 'age', y = 'fare'))
+ geom_point())
```

### Scatter plots

How could we make this nicer?

1. Do a log-transformation of `fare`, because it is very skewed.

2. Add in a third variable, `pclass`.  How might you do this?


### Scatter plots

Challenge: Can you re-create this plot?

```{python}
#| echo: false
df_titanic['log_fare'] = np.log(df_titanic['fare'])
df_titanic['pclass'] = df_titanic['pclass'].astype('category')

(ggplot(df_titanic, aes(x = 'age', y = 'log_fare', color = 'pclass'))
+ geom_point()
+ theme_classic())
```
### Describing the relationship

Let's look at just third class:

```{python}
#| code-fold: true
is_third= df_titanic['pclass'] == 3
df_third = df_titanic[is_third]

(ggplot(df_third, aes(x = 'age', y = 'fare'))
+ geom_point()
+ theme_classic())
```
### Describing the relationship

This relationship was:

* Not very **strong**: the points don't follow a clear pattern

* Slightly **negative**:  When age was higher, fare was a little lower.

* Not very **linear**: the points don't form a straight line


### Correlation

* What if we want a **numerical summary** of the relationship between variables?

* Do "older than average" people pay "higher than average" fares?

* When the **z-score** of age was high, was the **z-score** of fare high?

### Correlation

```{python}
#| code-fold: true
is_third= df_titanic['pclass'] == 3
df_first = df_titanic[is_third]

mean_age = df_third['age'].mean()
mean_fare = df_third['fare'].mean()

(ggplot(df_third, aes(x = 'age', y = 'fare'))
+ geom_point()
+ geom_vline(xintercept = mean_age, color = "red")
+ geom_hline(yintercept = mean_fare, color = "red")
+ theme_classic())
```

### Correlation

Interpret this result:

```{python}

df_third[['age', 'fare']].corr()

```


### Correlation

Interpret this result:

```{python}

df_third[['age', 'fare']].corr()

```

Age and fare are **slightly negatively correlated**.

*Can you think of an explanation for this?*


### Correlation is not **slope** or **relationship**

![](./images/correlation2.png)


Just for fun:  [Guess the Correlation Game](https://www.guessthecorrelation.com/)

## Takeaways

### Takeaways

* Plot **quantitative variables** across **groups** with **overlapping density** plots, **boxplots**, or by **facetting**.

* Summarize **quantitative variables** across **groups** by using `groupby()` and then calculating summary statisics.

* Know what **split-apply-combine** means.

* Plot **relationships between quantitative variables** with a **scatter plot**

* Describe the **strength**, **direction**, and **shape** of the relationship.

* Summarize **relationships between quantitative variables** with the **correlation**





::: lectitle
# Distances

[Slides](slides-05-distances.html)
:::

---
title: "Distances between observations"
format:
  revealjs: 
    theme: [default, spring.scss]
    incremental: true 
    output-file: slides-05-distances.html
    shift-heading-level-by: -1
execute:
  echo: true
---

```{python}

```

```{r}
#| include: false
library(reticulate)
use_python("/usr/local/bin/python3")
```

```{python}
#| include: false
import numpy as np
import pandas as pd
from plotnine import *
```

## The story so far

### Summarizing

* **One categorical variable:** marginal distribution

* **Two categorical variables:** joint and conditional distributions

* **One quantitative variable:** mean, median, variance, standard deviation.

* **One quantitative, one categorical:** mean, median, and std dev across groups (`groupby()`, *split-apply-combine*)

* **Two quantitative variables:** z-scores, correlation

### Visualizing

* **One categorical variable:** bar plot or column plot

* **Two categorical variables:** stacked bar plot, side-by-side bar plot, or stacked percentage bar plot

* **One quantitative variable:** histogram, density, or boxplot

* **One quantitative, one categorical:** overlapping densities, side-by-side boxplots, or facetting

* **Two quantitative variables:** scatter plot

## Today's data: House prices

### Ames house prices

*(Notice: `read_table` not `read_csv`)*

``` {python}
df = pd.read_table("https://datasci112.stanford.edu/data/housing.tsv")
df.head()
```

### How does size relate to number of bedrooms?

What plot would you make?

. . .

```{python}
(ggplot(df, aes(x = "Gr Liv Area", y = "Bedroom AbvGr")) 
+ geom_point())
```

### How does size relate to number of bedrooms?

What *statistic* would you calculate?

. . .

```{python}
df[["Gr Liv Area", "Bedroom AbvGr"]].corr()
```

## Measuring similarity with distance

### Similarity

How might we answer the question, "Are these two houses similar?"

```{python}
df.loc[1707, ["Gr Liv Area", "Bedroom AbvGr"]]
df.loc[290, ["Gr Liv Area", "Bedroom AbvGr"]]
```
### Distance

The **distance** between the **two observations** is:

$$ \sqrt{ (2956 - 2650)^2 + (5 - 6)^2} = 306 $$

. . .

... what does this number mean?  Not much!  But we can use it to **compare** sets of houses and find the **most similar**.

### Distance

Consider House 1707 and another one:

```{python}
df.loc[1707, ["Gr Liv Area", "Bedroom AbvGr"]]
df.loc[291, ["Gr Liv Area", "Bedroom AbvGr"]]
```
$$ \sqrt{ (2956 - 1666)^2 + (5 - 3)^2} = 1290 $$
House 1707 is **more similar** to House 290 than to House 291.

### (Lecture Activity Part 1)

```{r}
#| echo: false
library(countdown)
countdown(minutes = 10, left = "0")
```

## Scaling/Standardizing

### House 160 seems more similar...

```{python}
#| code-fold: true
(
ggplot(df, aes(x = "Gr Liv Area", y = "Bedroom AbvGr")) 
+ geom_point(color = "lightgrey")
+ geom_point(df.loc[[1707]], color = "red", size = 5)
+ geom_point(df.loc[[160]], color = "blue", size = 2)
+ geom_point(df.loc[[2336]], color = "green", size = 2)
+ theme_classic() 
)
```

### ... even if we zoom in...

```{python}
#| code-fold: true
(
ggplot(df, aes(x = "Gr Liv Area", y = "Bedroom AbvGr")) 
+ geom_point(color = "lightgrey")
+ geom_point(df.loc[[1707]], color = "red", size = 5)
+ geom_point(df.loc[[160]], color = "blue", size = 2)
+ geom_point(df.loc[[2336]], color = "green", size = 2)
+ theme_classic() 
+ scale_x_continuous(limits=(2500, 3500))
)
```

### ... but not if we put the axes on the same **scale**!

```{python}
#| code-fold: true
(
ggplot(df, aes(x = "Gr Liv Area", y = "Bedroom AbvGr")) 
+ geom_point(color = "lightgrey")
+ geom_point(df.loc[[1707]], color = "red", size = 5)
+ geom_point(df.loc[[160]], color = "blue", size = 2)
+ geom_point(df.loc[[2336]], color = "green", size = 2)
+ theme_classic() 
+ scale_x_continuous(limits=(2900, 3000))
+ scale_y_continuous(limits=(0, 100))
)
```

### Scaling

* We need to make sure our features are on the same **scale** before we can use **distances** to measure **similarity**.

* Recall:  **standardizing** = subtract the mean, divide by the standard deviation.

* In this case, the **mean** doesn't really  matter. (why?)


### Scaling

```{python}
df['size_scaled'] = (df['Gr Liv Area'] - df['Gr Liv Area'].mean())/df['Gr Liv Area'].std()
df['bdrm_scaled'] = (df['Bedroom AbvGr'] - df['Bedroom AbvGr'].mean())/df['Bedroom AbvGr'].std()
```

```{python}
#| code-fold: true
(
ggplot(df, aes(x = "size_scaled", y = "bdrm_scaled")) 
+ geom_point(color = "lightgrey")
+ geom_point(df.loc[[1707]], color = "red", size = 5)
+ geom_point(df.loc[[160]], color = "blue", size = 2)
+ geom_point(df.loc[[2336]], color = "green", size = 2)
+ theme_classic() 
)
```

### (Lecture Activity Part 2)


```{r}
#| echo: false
library(countdown)
countdown(minutes = 10, left = "0")
```

## Scikit-learn

### Scikit-learn

* `scikit-learn` is a library for **machine learning** and **modeling**

* We will use it a lot in this class!

* For now, we will use it as a shortcut for *scaling* and for *computing distances*

* The philosophy of `sklearn` is:
    + **specify** your analysis
    + **fit** on the data to prepare the analysis
    + **transform** the data

### Specify

No calculations have happened yet!

```{python}
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler
```

### Fit

The `scaler` object "learns" the means and standard deviations.

We still have not altered the data at all! 

```{python}
df_orig = df[['Gr Liv Area', 'Bedroom AbvGr']]
scaler.fit(df_orig)
scaler.mean_
scaler.scale_
```


### Transform

```{python}
df_scaled = scaler.transform(df_orig)
df_scaled
```

### sklearn, numpy, and pandas

* By default, `sklearn` functions return `numpy` objects.

* This is sometimes annoying; e.g. if we want to plot things after scaling.

* Solution: remake it, with the original column names.

```{python}
pd.DataFrame(df_scaled, columns = df_orig.columns)
```
### Distances with sklearn

```{python}
from sklearn.metrics import pairwise_distances

pairwise_distances(df_scaled[[1707]], df_scaled)
```


### Finding the most similar

```{python}
dists = pairwise_distances(df_scaled[[1707]], df_scaled)
dists.argsort()
```

### Finding the most similar

```{python}
best = dists.argsort().flatten()[1:10]
df_orig.iloc[best]
```


### (Lecture Activity Part 3)

```{r}
#| echo: false
library(countdown)
countdown(minutes = 10, left = "0")
```

## Alternatives

### Other scaling

-   Standardization
    $$x_i \leftarrow \frac{x_i - \bar{X}}{\text{sd}(X)}$$

-   Min-Max Scaling
    $$x_i \leftarrow \frac{x_i - \text{min}(X)}{\text{max}(X) - \text{min}(X)}$$

### Other distances

-   Euclidean ($\ell_2$)

    $$\sqrt{\sum_{j=1}^m (x_j - x'_j)^2}$$

-   Manhattan ($\ell_1$)

    $$\sum_{j=1}^m |x_j - x'_j|$$
    
    
### (Lecture Activity Part 4)

```{r}
#| echo: false
library(countdown)
countdown(minutes = 10, left = "0")
```

## Takeaways

### Takeaways

* We measure similarity between **observations** by calculating **distances**.

* It is important that all our **features** be on the same **scale** for distances to be meaningful.

* We can use `scikit-learn` functions to **fit** and **transform** data, and to compute pairwise distances.

* There are many options of ways to *scale* data; most common is **standardizing**

* There are many options of ways to *measure distances*; most common is **Euclidean distance**.



::: lectitle
# Dummy Variables and Column Tranformers

[Slides](slides-06-preprocessing.html)
:::

---
title: "Dummy variables and Column Transformers"
format:
  revealjs: 
    theme: [default, spring.scss]
    incremental: true 
    output-file: slides-06-preprocessing.html
    shift-heading-level-by: -1
execute:
  echo: true
---

```{python}

```

```{r}
#| include: false
library(reticulate)
use_python("/usr/local/bin/python3")
```

```{python}
#| include: false
import numpy as np
import pandas as pd
from plotnine import *
```

## The story this week

### Distances


* We measure similarity between **observations** by calculating **distances**.

* **Euclidean distance**: sum of squared differences, then square root

* **Manhattan distance**: sum of absolute differences

* In **scikit-learn**, use the `pairwise_distances` function to get back a *2D numpy array* of distances.

### Scaling

* It is important that all our **features** be on the same **scale** for distances to be meaningful.

* **Standardize:**  Subtract the *mean* (of the column) and divide by the *standard deviation* (of the column).

* **MinMax:** Subtract the *minimum* value, divide by the *range*.

* In `scikit-learn`, use the `StandardScaler()` or `MinMaxScaler()` functions.

* Follow the **specify** - **fit** - **transform** code structure.

### Recall: AMES Housing data


``` {python}
df = pd.read_table("https://datasci112.stanford.edu/data/housing.tsv")
features = ["Gr Liv Area", "Bedroom AbvGr", "Full Bath", "Half Bath", "Bldg Type", "Neighborhood"]
df[features].head()
```

## Distances and Categorical Variables

### What about categorical variables?

Suppose we want to include the variable `Bldg Type` in our distance calculation...

```{python}
df["Bldg Type"].value_counts()
```
What is "single family minus townhouse squared"?

### Converting to binary

Let's instead think about "Single family, or not"

```{python}
df["is_single_fam"] = df["Bldg Type"] == "1Fam"
df["is_single_fam"].value_counts()
```
### Converting to binary

Recall that `True/False` is the same as `1/0` for computers:

```{python}
df["is_single_fam"] = df["is_single_fam"].astype("int")
df["is_single_fam"].value_counts()
```
We call this a **dummy variable** or a **one-hot-encoding**.

### Now we can do math!

```{python}
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import pairwise_distances
scaler = StandardScaler()

df_orig = df[['Gr Liv Area', 'Bedroom AbvGr', 'is_single_fam']]
scaler.fit(df_orig)

df_scaled = scaler.transform(df_orig)

dists = pairwise_distances(df_scaled[[1707]], df_scaled)
best = dists.argsort().flatten()[1:10]
df_orig.iloc[best]
```

### Quick look back

#### Where have you seen some one-hot-encoded variables already?


### Let's reset the dataset now...

```{python}
df = pd.read_table("https://datasci112.stanford.edu/data/housing.tsv")
```


## Dummifying Variables

### Dummifying Variables

* What if we don't just want to study `is_single_fam`, but rather, *all* categories of the `Bldg Type` variable?

* In principle, we just make **dummy variables** for **each category**:  `is_single_fam`, `is_twnhse`, etc.

* Each **category** becomes one **column**, with 0's and 1's to show if the *observation in that row* matches that *category*.

* This is called **dummifying** or **one-hot-encoding** a **categorical variable**

* Luckily, we have shortcuts in both `pandas` *and* `sklearn`...

### Dummifying in Pandas

```{python}
pd.get_dummies(df[["Bldg Type"]])
```

### Dummifying in Pandas

Some things to notice here...

1.  What is the **naming convention** for the new columns?

2. Does this change the original dataframe `df`?  If not, what would you need to do to add this information back in?

3. What happens if you put the whole dataframe into the `get_dummies` function?  What problems might arise from this?

### Dummifying in sklearn

```{python}
from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder()

encoder.fit(df[["Bldg Type"]])

df_bldg = encoder.transform(df[["Bldg Type"]])

df_bldg
```


### Dummifying in sklearn

```{python}
df_bldg.todense()
```

### Dummifying in sklearn

Things to notice:

1. What **object type** was the result?

2. Does this change the original dataframe `df`?  If not, what would you need to do to add this information back in?

3. What happens if you fit the whole dataframe with the OneHotEncoder?  What problems might arise from this?

4. What pros and cons do you see for the `pandas` approach vs the `sklearn` approach?


## Column Transformers

### Preprocessing

* We have now seen two **preprocessing** steps that might need to happen to do an analysis of distances:
    + **Scaling** the quantitative variables
    + **Dummifying** the categorical variables
    
* **Preprocessing** steps are things you do *only to make the following analysis/visualization better*.
    
* This is not the same as **data cleaning**, which are changes you make to *fix* the data.

* This is not the same as **data wrangling**, which are changes you make to *restructure* the data; i.e., adding or deleting rows or columns to reflect what you are trying to study.

### Quick quiz

Identify the following as *cleaning*, *wrangling*, or *preprocessing*:

1. Removing the `$` symbol from a column and converting it to numeric.

2. Narrowing your data down to only first class Titanic passengers, because you are not studying the others.

3. Converting a `Zip Code` variable from numeric to categorical using `.astype()`.

4. Creating a new column called `n_investment` that counts the number of people who invested in a project.

5. Log-transforming a column because it is very skewed.

### Preprocessing in `sklearn`

* Unlike **cleaning** and **wrangling**, the **preprocessing** steps are "temporary" changes to the dataframe.

* It would be nice if we could trigger these changes as part of our analysis, instead of doing them "by hand".

* This is why the **specify** - **fit** - **transform** process is useful!

* We will first specify **all** our preprocessing steps.

* Then we will **fit** the whole preprocess

* Then we will save the **transform** step for only when we need it.

### Column Transformers

```{python}
from sklearn.compose import make_column_transformer

preproc = make_column_transformer(
    (OneHotEncoder(), ["Bldg Type", "Neighborhood"]),
    remainder="passthrough")
    
preproc.fit(df[features])

preproc.transform(df[features])
```

### Column Transformers

Things to notice...

1. What submodule did we import `make_column_transformer` from?

2. What are the **two** arguments to the `make_column_transformer` function?  What **object structures** are they?

3. What happens if you **fit** and **transform** on the whole dataset, not just `df[features]`?  Why might this be useful?


### Column Transformers

Try the following:

1. What happens if you change `remainder = "passthrough"` to `remainder = "drop"`? 

2. What happens if you add the argument `sparse_output=False` to the `OneHotEncoder()` function?

3. What happens if you add this line before the *transform* step: *(keep the `sparse_output=False` when you try this)* 
```
preproc.set_output(transform = "pandas")
```

4. Look at the `preproc` object.  What does it show you?

### Multiple preprocessing steps

Why are **column transformers** so useful?  

Well, now we can do **multiple preprocessing steps** at once!

```{python}
from sklearn.preprocessing import StandardScaler

preproc = make_column_transformer(
        (StandardScaler(), ["Gr Liv Area"]),
        (OneHotEncoder(sparse_output=False), ["Bldg Type",
                                          "Neighborhood"]),
    remainder="passthrough")
    
preproc.fit(df[features])

preproc.set_output(transform = "pandas")

df_transformed = preproc.transform(df[features])
df_transformed
```

### Finding all variables

What if we just want to say "Please dummify **all** categorical variables?"

Use a `selector` instead of exact column names.

```{python}
from sklearn.compose import make_column_selector

preproc = make_column_transformer(
    (StandardScaler(),  make_column_selector(dtype_include=np.number)),
    (OneHotEncoder(sparse_output=False), make_column_selector(dtype_include=object)),
    remainder="passthrough")
    
preproc.fit(df[features])

preproc.set_output(transform = "pandas")

df_transformed = preproc.transform(df[features])
df_transformed

```


### Think about it

* What are the *advantages* of using a selector?

* What are the possible *disadvantages* of using a selector?

* Does the *order* matter when using selectors?  Try switching the steps and see what happens!


## Takeaways

### Takeaways

* We **dummify** or **one-hot-encode** categorical variables to make them numbers.

* We can do this with `pd.get_dummies()` or with `OneHotEncoder()`

* **Column Transformers** let us apply multiple preprocessing steps all together.
    + Think about *which variables* you want to apply the steps to
    + Think about *options* for the steps, like sparseness
    + Think about `passthrough` in your transformer



::: lectitle
# Text Data and TF-IDF

[Slides](slides-07-text_data.html)
:::

---
title: "Bag-of-words and TF-IDF"
format:
  revealjs: 
    theme: [default, spring.scss]
    incremental: true 
    output-file: slides-07-text_data.html
    shift-heading-level-by: -1
execute:
  echo: true
---

```{python}

```

```{r}
#| include: false
library(reticulate)
use_python("/usr/local/bin/python3")
```

```{python}
#| include: false
import numpy as np
import pandas as pd
from plotnine import *
```

## The story so far

### Distances


* We measure similarity between **observations** by calculating **distances**.

* **Euclidean distance**: sum of squared differences, then square root

* **Manhattan distance**: sum of absolute differences

* In **scikit-learn**, use the `pairwise_distances` function to get back a *2D numpy array* of distances.

### Scaling

* It is important that all our **features** be on the same **scale** for distances to be meaningful.

* **Standardize:**  Subtract the *mean* (of the column) and divide by the *standard deviation* (of the column).

* **MinMax:** Subtract the *minimum* value, divide by the *range*.

* In `scikit-learn`, use the `StandardScaler()` or `MinMaxScaler()` functions.

* Follow the **specify** - **fit** - **transform** code structure.

## Bag of Words

### Text data


A textual data set consists of multiple texts. Each text is called a
**document**. The collection of texts is called a **corpus**.

Example Corpus:

0.  `"I am Sam\n\nI am Sam\nSam I..."`

1.  `"The sun did not shine.\nIt was..."`

2.  `"Fox\nSocks\nBox\nKnox\n\nKnox..."`

3.  `"Every Who\nDown in Whoville\n..."`

4.  `"UP PUP Pup is up.\nCUP PUP..."`

5.  `"On the fifteenth of May, in the..."`

6.  `"Congratulations!\nToday is your..."`

7.  `"One fish, two fish, red fish..."`

### Reading Text Data

Reading in Textual Data

Documents are usually stored in different files.

``` {python}
seuss_dir = "http://dlsun.github.io/pods/data/drseuss/"
seuss_files = [
    "green_eggs_and_ham.txt", "cat_in_the_hat.txt",
    "fox_in_socks.txt", "how_the_grinch_stole_christmas.txt",
    "hop_on_pop.txt", "horton_hears_a_who.txt",
    "oh_the_places_youll_go.txt", "one_fish_two_fish.txt"]
```

We have to read them in one by one.

``` {python}
import requests

docs = {}
for filename in seuss_files:
    response = requests.get(seuss_dir + filename, "r")
    docs[filename] = response.text
```

### Reading Text Data

```{python}
docs.keys()
```

### Bag-of-Words Representation

In the **bag-of-words** representation in this data, each column represents a word, and the
values in the column are the word counts for that document.

First, we need to count the words in each document.

``` {python}
from collections import Counter
Counter(docs["hop_on_pop.txt"].split())
```

### Bag-of-Words Representation

... then, we put these counts into a `Series` and stack them into a
`DataFrame`.

This is called **bag of words** data.

``` {python}
pd.DataFrame(
    [pd.Series(Counter(doc.split())) for doc in docs.values()],
    index=docs.keys())
```

### Bag-of-Words in Scikit-Learn

Alternatively, we can use `CountVectorizer` in `scikit-learn` to
produce a bag-of-words matrix.

``` {python}
from sklearn.feature_extraction.text import CountVectorizer

vec = CountVectorizer()

vec.fit(docs.values())

vec.transform(docs.values())
```

### Vocabulary

The set of words across a corpus is called the **vocabulary**. We can
view the vocabulary in a fitted `CountVectorizer` as follows:

``` {python}
vec.vocabulary_["fish"]
vec.vocabulary_["pop"]
vec.vocabulary_["eggs"]
```

### Text Normalizing

What's wrong with the way we counted words originally?

    Counter({'UP': 1, 'PUP': 3, 'Pup': 4, 'is': 10, 'up.': 2, ...})

* It's usually good to **normalize** for punctuation and capitalization.

* Normalization options are specified when you initialize the
`CountVectorizer`. 

* By default, Scikit-Learn strips punctuation
and converts all characters to lowercase.

### Text Normalizing in sklearn

* If you don't want Scikit-Learn to normalize for punctuation and
capitalization, you can do the following:

``` {python}
vec = CountVectorizer(lowercase=False, token_pattern=r"[\S]+")
vec.fit(docs.values())
vec.transform(docs.values())
```

## N-grams

### The Shortcomings of Bag-of-Words

Bag-of-words is easy to understand and easy to implement.

What are its disadvantages?

Consider the following documents:

1.  "The dog bit her owner."

2.  "Her dog bit the owner."

Both documents have the same exact bag-of-words representation, but they mean something quite different!

### N-grams{.smaller}

* An **n-gram** is a sequence of $n$ words.

* N-grams allow us to capture more of the meaning.

* For example, if we count **bigrams** (2-grams) instead of words, we can distinguish the two documents from before:

1.  "The dog bit her owner."

2.  "Her dog bit the owner."

$$\begin{array}{l|ccccccc}
& \text{the,dog} & \text{her,dog} & \text{dog,bit} & \text{bit,the} & \text{bit,her} & \text{the,owner} & \text{her,owner} \\
\hline
\text{1} & 1 & 0 & 1 & 0 & 1 & 0 & 1 \\
\text{2} & 0 & 1 & 1 & 1 & 0 & 1 & 0 \\
\end{array}$$

### N-grams in Scikit-Learn

Scikit-Learn can create n-grams.

Pass in `ngram_range=` to the `CountVectorizer`.

To get bigrams, we set the range to `(2, 2)` ...

``` {python}
vec = CountVectorizer(ngram_range=(2, 2))
vec.fit(docs.values())
vec.transform(docs.values())
```

### N-grams in Scikit-learn

... or we can also get individual words (unigrams) alongside the bigrams:

```{python}
vec = CountVectorizer(ngram_range=(1, 2))
vec.fit(docs.values())
vec.transform(docs.values())
```


## Text Data and Distances

### Similar documents

Now, we can use this **bag-of-words** data to measure **similarities** between documents!

``` {python}
from sklearn.metrics import pairwise_distances

vec = CountVectorizer(ngram_range=(1, 2))
vec.fit(docs.values())
dat = vec.transform(docs.values())

dists = pairwise_distances(dat)
dists
```

### Similar documents

```{python}
dists[0].argsort()
docs.keys()
```
* This is how data scientists do **authorship identification**!


## Activity

### Activity 1

Using bi-grams, unigrams, and tri-grams, which Dr. Seuss document is closest to "One Fish Two Fish"?

```{python}
#| include: false

vec = CountVectorizer(ngram_range=(1,3))
vec.fit(docs.values())
bow_seuss = vec.transform(docs.values())
pairwise_distances(bow_seuss)[7]
```

## Motivating example

### Issues with the distance approach

**BUT WAIT!**

* Don't we care more about *word choice* than *total words used*?

* Wouldn't a *longer document* have *more words*, and thus be able to "match" other documents?

* Wouldn't *more common words* appear in more documents, and thus cause them to "match"?

* Recall: We have many options for **scaling**

* Recall: We have many options for **distance metrics**.

### Example{.smaller}

**Document A:**

> "Whoever has hate for his brother is in the darkness and walks in the darkness."

**Document B:**

> "Hello darkness, my old friend, I've come to talk with you again."

**Document C:**

> "Returning hate for hate multiplies hate, adding deeper darkness to
a night already devoid of stars. Darkness cannot drive out darkness; only light can do that."

**Document D:**

> "Happiness can be found in the darkest of times, if only one remembers to turn on the light."

### Example

```{python}
#| code-fold: true
documents = [
    "whoever has hate for his brother is in the darkness and walks in the darkness",
    "hello darkness my old friend",
    "returning hate for hate multiplies hate adding deeper darkness to a night already devoid of stars darkness cannot drive out darkness only light can do that",
    "happiness can be found in the darkest of times if only one remembers to turn on the light"
]
```

```{python}
#| code-fold: true
from sklearn.feature_extraction.text import CountVectorizer

vec = CountVectorizer(token_pattern=r"\w+")
vec.fit(documents)
bow_matrix = vec.transform(documents)
bow_matrix
```

### Example

```{python}
#| warning: false
bow_dataframe = pd.DataFrame(bow_matrix.todense(), columns=vec.get_feature_names_out())
bow_dataframe[["darkness", "light"]]
```

### Measuring similarity

```{python}
#| code-fold: true
from sklearn.metrics import pairwise_distances

pairwise_distances(bow_matrix)
```

## Cosine Distance

### Choosing your distance metric

Is **euclidean distance** really the best choice?!

> My name is James Bond, James Bond is my name.

> My name is James Bond.

> My name is James.

* If we count words the second two will be the most similar.

* The first document is longer, so it has "double" counts.

* But, it has the exact same words as the first document!

* Solution: **cosine distance** (on the board)

### Cosine Distance

As a rule, **cosine distance** is a better choice for bag-of-words data!

```{python}
from sklearn.metrics.pairwise import cosine_distances
cosine_distances(bow_matrix)
```

## TF-IDF


### Scaling terms

Which of these seems most important for measuring similarity?

* Document B, C, D all have the word "to"

* Documents A, B, and C all have the word **darkness**.

* Document A and Document C both have the word "hate"

* Document C and Document D both have the word "light"

* We would like to **scale** our **word counts** by the **document length** (TF).

* We would also like to **scale** our **word counts** by the **number of documents they appear in**. (IDF)


### Term Frequencies (TF)

* If a document is longer, it is more likely to share words.

* Let's use *frequencies* instead of *counts*


```{python}
bow_totals = bow_dataframe.sum(axis = 1)
bow_totals
bow_tf = bow_dataframe.divide(bow_totals, axis = 0)
bow_tf
```

### Term Frequencies (TF)

```{python}
cosine_distances(bow_tf)
```


### Inverse Document Frequency (IDF)

* In principle, if two documents **share rarer words** they are **more similar**.

* What matters is not *overall word frequency* but **how many of the documents** have that word.

* Compute document frequency:

```{python}
has_word = (bow_dataframe > 0)
has_word[["darkness", "light"]]
```

### IDF

```{python}
bow_df = has_word.sum(axis = 0)/4
bow_df
```

### IDF

Adjust for inverse document frequencies:

```{python}
bow_log_idf = np.log(1/bow_df)
bow_tf_idf = bow_tf.multiply(bow_log_idf, axis = 1)
bow_tf_idf[["darkness", "light"]]
```
### TF-IDF

```{python}
cosine_distances(bow_tf_idf).round(decimals = 2)
```

### TF-IDF in Sklearn

```{python}
from sklearn.feature_extraction.text import TfidfVectorizer

# The options ensure that the numbers match our example above.
vec = TfidfVectorizer(smooth_idf=False, norm=None)
vec.fit(documents)
tfidf_matrix = vec.transform(documents)

cosine_distances(tfidf_matrix)
```


## Activity 2

### Activity


Using bi-grams, unigrams, and tri-grams, which Dr. Seuss document is closest to "One Fish Two Fish"?

```{python}
#| include: false
from sklearn.metrics import pairwise_distances

vec = TfidfVectorizer(smooth_idf=False, norm=None, ngram_range=(1,3))
vec.fit(docs.values())
bow_seuss = vec.transform(docs.values())
cosine_distances(bow_seuss)[7]
```

## Takeaways

### Takeaways

* We represent **text data** as a **bag-of-words** or **bag-of-n-grams** matrix.

* Each row is a **document** in the **corpus**.

* We typically use **cosine distance** to measure similarity, because it captures **patterns of word choice**

* We apply **TF-IDF** transformations to **scale** the bag-of-words data, so that words that **appear in fewer documents** are **more important**



::: lectitle
# K Nearest Neighbors

[Slides](slides-08-knn.html)
:::

---
title: "K-Nearest-Neighbors"
format:
  revealjs: 
    theme: [default, spring.scss]
    incremental: true 
    output-file: slides-08-knn.html
    shift-heading-level-by: -1
execute:
  echo: true
---

```{python}

```

```{r}
#| include: false
library(reticulate)
use_python("/usr/local/bin/python3")
```

```{python}
#| include: false
import numpy as np
import pandas as pd
from plotnine import *
```

## The story so far

### Steps for data analysis

* **Read** and then **clean** the data
  + Are there missing values?  Will we drop those rows, or replace the missing values with something?
  + Are there *quantitative* variables that python thinks are *categorical*?
  + Are there *categorical* variables that python thinks are *quantitative*?
  + Are there any *anomalies* in the data that concern you?
  
### Steps for data analysis (cont'd)

* **Explore** the data by **visualizing** and **summarizing**.
  + Different approaches for different combos of *quantitative* and *categorical* variables
  + Think about *conditional* calculations (split-apply-combine)
  
### Steps for data analysis (cont'd)

* Identify a **research question** of interest.

* Perform **preprocessing** steps
  + Should we *scale* the quantitative variables?
  + Should we *one-hot-encode* the categorical variables?
  + Should we *log-transform* any variables?


* Measure similarity between **observations** by calculating **distances**.
  + Which *features* should be included?
  + Which *distance metric* should we use?

## Predicting wine prices

### Data:  Wine qualities

```{python}
df = pd.read_csv("https://dlsun.github.io/pods/data/bordeaux.csv")
df
```

### Data: Wine qualities

* Our goal is to **predict** what will be the quality (price) of wines in a **future year**.

* Idea:  Wines with similar **features** probably have similar **quality**.

* **Inputs**: Summer temperature, harvest rainfall, september temperature, winter rainfall, age of wine.

* **Output:** Price in 1992.

* "Inputs" = "Features" = "Predictors" = "Independent variables"

* "Output" = "Target" = "Dependent variable"

### Similar wines

Which wines have similar summer temps and winter rainfall to the 1989 vintage?

```{python}
#| code-fold: true
from plotnine import *

(ggplot(df, aes(x = "summer", y = "win"))
+ geom_point(color = "white")
+ geom_text(aes(label = "year"))
+ theme_classic())
```

### Predicting 1989

```{python}
df[df['year'] == 1990]

df[df['year'] == 1976]
```


### Training and test data

* The data for which we **know the target** is called the **training data**.

* The data for which we **don't know the target**  (and want to predict it) is calledthe **test data**.

``` {python}
known_prices = df['year'] < 1981
to_predict = df['year'] == 1989

df_train = df[known_prices].copy()
df_test = df[to_predict].copy()
```

### Specify steps

First we make a column transformer...

```{python}
from sklearn.compose import make_column_transformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler

preproc = make_column_transformer(
  (StandardScaler(), ['summer', 'har', 'sep', 'win','age']),
  remainder = "drop"
)
```

### Fit the preprocesser

Then we **fit** it on the **training data**

```{python}
preproc.fit(df_train)

preproc.named_transformers_['standardscaler'].mean_
preproc.named_transformers_['standardscaler'].var_
```
### Prep the data

Then we **tranform** the **training data** AND the **test data**:

```{python}
train_new = preproc.transform(df_train)
test_new = preproc.transform(df_test)

test_new
```

### Fitting vs transforming

What if we had fit on the test data?

```{python}
preproc.fit(df_test)

preproc.named_transformers_['standardscaler'].mean_
preproc.named_transformers_['standardscaler'].var_
```

### Fitting vs transforming

What if we had fit on the test data?

```{python}
preproc.transform(df_test)
```

### All together:

```{python}
preproc = make_column_transformer(
  (StandardScaler(), ['summer', 'har', 'sep', 'win','age']),
  remainder = "drop"
)

preproc.fit(df_train)

train_new = preproc.transform(df_train)
test_new = preproc.transform(df_test)
```


### Find the closest *k*

```{python}
from sklearn.metrics import pairwise_distances

pairwise_distances(test_new, train_new)
```

### Find the closest *k*

```{python}
dists = pairwise_distances(test_new, train_new)

dists[0].argsort()
```

### Find the closest *k*

```{python}
ranked_train = df_train.loc[dists[0].argsort()]
ranked_train
```
### Predict from the closest *k*

If $k = 1$ ...

```{python}
ranked_train = df_train.loc[dists[0].argsort()]
ranked_train.iloc[0]['price']
```


### Predict from the closest *k*

If $k = 100$ ...

```{python}
ranked_train = df_train.loc[dists[0].argsort()]
ranked_train.iloc[0:99]['price'].mean()
```


### Predict from the closest *k*

If $k = 5$ ...

```{python}
ranked_train = df_train.loc[dists[0].argsort()]
ranked_train.iloc[0:4]['price'].mean()
```


## Activity

### Activity 1

Find the predicted 1992 price for *all* the unknown wines, with

* $k = 1$

* $k = 5$

* $k = 10$

How close was each prediction to the right answer?

*(Optional hint: Write a function to help you!)*

### Activity 2 (together)

Find the predicted 1992 price for all the *training data*, with

* $k = 1$

* $k = 5$

* $k = 10$

How close was each prediction to the right answer?

*(Optional hint: Write a function to help you!)*


## K-Nearest-Neighbors

### KNN{.smaller}

We have existing observations

$$(X_1, y_1), ... (X_n, y_n)$$
Where $X_i$ is a set of features, and $y_i$ is a target value.

Given a new observation $X_{new}$, how do we predict $y_{new}$?

1.  Find the $k$ values in $(X_1, ..., X_n)$ that are closest to $X_{new}$

2.  Take the average of the corresponding $y_i$'s to our five closest $X_i$'s.

3. Predict $\hat{y}_{new}$ = average of these $y_i$'s

### KNN

To perform **K-Nearest-Neighbors**, we choose the **K** closest observations to our *target*, and we average their *response* values.

The Big Questions:

* What is our definition of **closest**?

* What number should we use for **K**?

* How do we evaluate the **success** of this approach?


### KNN in sklearn

```{python}
from sklearn.neighbors import KNeighborsRegressor
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(
  preproc,
  KNeighborsRegressor(n_neighbors=5)
  )
          
pipeline
```
### KNN in sklearn

```{python}
pipeline.fit(y = df_train['price'], X = df_train)
pipeline.predict(X=df_test)
```



### Activity 2

Find the predicted 1992 price for *all wines*, with

* $k = 1$

* $k = 5$

* $k = 10$

How close was each prediction to the right answer?




::: lectitle
# Modeling and Machine Learning

[Slides](slides-09-modeling.html)
:::

---
title: "Modeling"
format:
  revealjs: 
    theme: [default, spring.scss]
    incremental: true 
    output-file: slides-09-modeling.html
    shift-heading-level-by: -1
execute:
  echo: true
---

```{python}

```

```{r}
#| include: false
library(reticulate)
use_python("/usr/local/bin/python3")
```

```{python}
#| include: false
import numpy as np
import pandas as pd
from plotnine import *
```

## The story so far

### Steps for data analysis

* **Read** and then **clean** the data
  + Are there missing values?  Will we drop those rows, or replace the missing values with something?
  + Are there *quantitative* variables that python thinks are *categorical*?
  + Are there *categorical* variables that python thinks are *quantitative*?
  + Are there any *anomalies* in the data that concern you?
  
### Steps for data analysis (cont'd)

* **Explore** the data by **visualizing** and **summarizing**.
  + Different approaches for different combos of *quantitative* and *categorical* variables
  + Think about *conditional* calculations (split-apply-combine)
  
### Steps for data analysis (cont'd)

* Identify a **research question** of interest.

* Perform **preprocessing** steps
  + Should we *scale* the quantitative variables?
  + Should we *one-hot-encode* the categorical variables?
  + Should we *log-transform* any variables?


* Measure similarity between **observations** by calculating **distances**.
  + Which *features* should be included?
  + Which *distance metric* should we use?



## Machine Learning and Statistical Modeling

### Modeling

Every analysis we will do assumes a structure like:

::: {style="font-size: 150%;"}
(**output**) = f(**input**) + (noise)
:::

... or, if you prefer...

::: {style="font-size: 150%;"}
(**target**) = f(**predictors**) + (noise)
:::

### Generative process

In any case: we are trying to reconstruct information in **data**, and we are hindered by **random noise**.

The function $f$ might be very simple...

$$y_i = \mu + \epsilon_i$$

"A person's height is the true average height of people in the world, plus some randomness."

### Generative process

... or more complex...

$$y_i = 0.5*x_{1i} + 0.5*x_{2i} + \epsilon_i$$

"A person's height is equal to the average of their biological mother's height and biological father's height, plus some randomness"

* Do you think there is "more randomness" in the first function or this one?

### Generative process

... or extremely, ridiculously complex...

![](./images/complex_function.png)

### Generative process

... and it doesn't have to be a *mathematical* function at all; just a procedure:

$$y_i = \text{(average of heights of 5 people with most similar weights)} + \epsilon_i$$

### Modeling

* Our goal is to **reconstruct** or **estimate** or **approximate** the function/process $f$ based on **training data**.

* For example: Instead of the 5 most similar weights *in the whole world*, we can estimate with the 5 most similar weights *in our training set*.

* Instead of committing to one $f$ to estimate, we might propose **many** options and see which one "leaves behind" the least randomness.

## Data: Wine price prediction

### Setup

```{python}
import pandas as pd
from sklearn.compose import make_column_transformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler


import pandas as pd

df = pd.read_csv("https://dlsun.github.io/pods/data/bordeaux.csv",
                 index_col="year")
                 
df_train = df.loc[:1980].copy()
df_unknown = df.loc[1981:].copy()
```

### KNN revisited

```{python}
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsRegressor

ct = make_column_transformer(
  (StandardScaler(), ['summer', 'har', 'sep', 'win', 'age']),
  remainder = "drop"
)

pipeline = make_pipeline(
          ct,
          KNeighborsRegressor(n_neighbors=5))
pipeline.fit(X=df_train, y=df_train['price'])
pipeline.predict(X=df_unknown)
```

### Measuring error

The most common way to measure "leftover noise" is the **sum of squared error** or equivalently, the **mean squared error**

```{python}
#| code-fold: true
pred_y_train = pipeline.predict(X=df_train)
results = pd.DataFrame({
  "real_prices": df_train['price'],
  "predicted_prices": pred_y_train,
})
results["error"] = results["predicted_prices"] - results["real_prices"]
results["squared error"]= (results["error"])**2
results
```

### Measuring error

The most common way to measure "leftover noise" is the **sum of squared error** or equivalently, the **mean squared error**

```{python}
results["squared error"].mean()
```

### Best K

Now let's try it for some different values of $k$

```{python}
for k in [1, 3, 5, 10, 25]:
#| code-fold: true
  pipeline = make_pipeline(
    ct,
    KNeighborsRegressor(n_neighbors=k))
  pipeline = pipeline.fit(X=df_train, y=df_train['price'])
  pred_y_train = pipeline.predict(X=df_train)
  ((df_train['price'] - pred_y_train)**2).mean()

```

### Training error vs test error

* Oh no! Why did we get an error of 0 for $k = 1$?

* Because the closest wine in the training set is... itself.

* So, our problem is:
    + If we predict on the **new data**, we don't know the **true prices** and we can't **evaluate** our models
    + If we predict on the **training data**, we are "cheating", because we are using the data to both **train** and **test**.
    
* Solution: Let's make a pretend **test data** set!

### Test/Training split

```{python}
df_train = df.loc[:1970].copy()
df_test = df.loc[1971:1980].copy()
```

* We will **train** on the years up to 1970

* We will **test** on the years 1971 to 1980

* We will **evaluate** based on model performance on the **test data**.

### Try again: Best K

```{python}
for k in range(1,15):
#| code-fold: true
  pipeline = make_pipeline(
    ct,
    KNeighborsRegressor(n_neighbors=k))
  pipeline = pipeline.fit(X=df_train, y=df_train['price'])
  
  pred_y_test = pipeline.predict(X=df_test)
  ((df_test['price'] - pred_y_test)**2).mean()
```

### Tuning

* Here we tried the same **type** of model (KNN) each time.

* But we tried different **models** because we used different values of $k$

* This is called **tuning**

### Activity

Perform tuning for a KNN model, but with **all possible values of k**.

Do this for three *recipes* or *column transformers*:

1. Using all predictors.

2. Using just winter rainfall and summer temperature.

3. Using only age.

Which of the many model options performed best?

```{python}
ct2 = make_column_transformer(
  (StandardScaler(), ['summer', 'win']),
  remainder = "drop"
)
ct3 = make_column_transformer(
  (StandardScaler(), ['age']),
  remainder = "drop"
)
#| include: false
for k in range(1,17):
#| code-fold: true
  pipeline = make_pipeline(
    ct,
    KNeighborsRegressor(n_neighbors=k))
  pipeline = pipeline.fit(X=df_train, y=df_train['price'])
  pred_y_test = pipeline.predict(X=df_test)
  print(str(k) + ":" + str(((df_test['price'] - pred_y_test)**2).mean()))
  
for k in range(1,17):
#| code-fold: true
  pipeline = make_pipeline(
    ct2,
    KNeighborsRegressor(n_neighbors=k))
  pipeline = pipeline.fit(X=df_train, y=df_train['price'])
  pred_y_test = pipeline.predict(X=df_test)
  print(str(k) + ":" + str(((df_test['price'] - pred_y_test)**2).mean()))
  
for k in range(1,17):
#| code-fold: true
  pipeline = make_pipeline(
    ct3,
    KNeighborsRegressor(n_neighbors=k))
  pipeline = pipeline.fit(X=df_train, y=df_train['price'])
  pred_y_test = pipeline.predict(X=df_test)
  print(str(k) + ":" + str(((df_test['price'] - pred_y_test)**2).mean()))
```

### Things to think about{.smaller}

* What other **types of models** could we have tried?

* *Linear regression*, *decision tree*, *neural network*, ...

* What other **column transformers** could we have tried?

* *Different combinations of variables, different standardizing, log transforming...*

* What if we measure **error** differently?

* *Mean absolute error*, *log-error*, *percent error*, ...

* What if we had used a **different test set**?

* *Coming soon: Cross-validation*

* What if our target variable was **categorical**?


## Modeling: General Procedure

### Modeling{.smaller}

We apply the process:

<u>For each model proposed: </u>

* Establish a **pipeline** with **transformers** and a **model**.

* **Fit** the pipeline on the **training data** (with known outcome)

* **Predict** with the fitted pipeline on **test data** (with known outcome)

* **Evaluate** our success; i.e., measure noise "left over"

<u>Then: </u>

* **Select** the best model

* **Fit** on *all* the data

* **Predict** on any future data (with unknown outcome)

### Big decisions

* Which models to try

* Which column transformers to try

* How much to tune

* How to measure "success" of a model





::: lectitle
# Regression and Cross-Validation

[Slides](slides-10-cross_val.html)
:::

---
title: "Cross-Validation and Grid Search"
format:
  revealjs: 
    theme: [default, spring.scss]
    incremental: true 
    output-file: slides-10-cross_val.html
    shift-heading-level-by: -1
execute:
  echo: true
---

```{python}

```

```{r}
#| include: false
library(reticulate)
use_python("/usr/local/bin/python3")
```

```{python}
#| include: false
import numpy as np
import pandas as pd
from plotnine import *
```

## The story so far

### Modeling

We assume some process $f$ is *generating* our target variable:

::: {style="font-size: 150%;"}
(**target**) = f(**predictors**) + (noise)
:::

Our goal is to come up with an approximation of $f$.


### Test error vs training error{.smaller}

* We don't need to know how well our model does on *training data*.

* We want to know how well it will do on *test data*.

* In general, test error $>$ training error.

> Analogy: A professor posts a practice exam before an exam.

> -   If the actual exam is the same as the practice exam, how many points will students miss? That's training error.

> -   If the actual exam is different from the practice exam, how many points will students miss? That's test error.

> It's always easier to answer questions that you've seen before than
questions you haven't seen.


### Modeling Procedure {.smaller}

We apply the process:

<u>For each model proposed: </u>

* Establish a **pipeline** with **transformers** and a **model**.

* **Fit** the pipeline on the **training data** (with known outcome)

* **Predict** with the fitted pipeline on **test data** (with known outcome)

* **Evaluate** our success; i.e., measure noise "left over"

<u>Then: </u>

* **Select** the best model

* **Fit** on *all* the data

* **Predict** on any future data (with unknown outcome)


## Linear Regression

### Simple Linear Model

We assume that the target ($Y$) is generated from **an equation** of the predictor ($X$), plus random noise ($\epsilon$)

$$Y = \beta_0 + \beta_1 X + \epsilon$$


Goal:  Use observations $(x_1, y_1), ..., (x_n, y_n)$ to estimate $\beta_0$ and $\beta_1$.

### Measures of success

What is the "best" choice of $\hat{\beta}_0$ and $\hat{\beta}_1$?

* The ones that are **statistically most justified**, under certain assumptions about $Y$ and $X$?

* The ones that are "closest to" the observed points?

    + $|\hat{y}_i - y_i|$?
    + $(\hat{y}_i - y_i)^2$?
    + $(\hat{y}_i - y_i)^4$?
    
### Example: Wine data

```{python}
#| code-fold: true

df = pd.read_csv("https://dlsun.github.io/pods/data/bordeaux.csv",
                 index_col="year")
                 
df_known = df.loc[:1980].copy()
df_unknown = df.loc[1981:].copy()

(ggplot(df_known, aes(x = "age", y = "price")) 
+ geom_point())

```

### "Candidate" lines

Condsider five possible regression equations:

$$\text{price} = 25 + 0*\text{age}$$
$$\text{price} = 0 + 10*\text{age}$$
$$\text{price} = 20 + 1*\text{age}$$
$$\text{price} = -40 + 3*\text{age}$$

Which one do you think will be "closest" to the points on the scatterplot?

### "Candidate" lines

```{python}
(ggplot(df_known, aes(x = "age", y = "price")) 
+ geom_point()
+ geom_abline(intercept = 25, slope = 0)
+ geom_abline(intercept = 0, slope = 1)
+ geom_abline(intercept = 20, slope = 1)
+ geom_abline(intercept = -40, slope = 3))
```

### The "best" slope and intercept

* It's clear that some of these lines are better than others.

* How to choose the best?  **Math**

* We'll let the computer do it for us.

* **Important:** The slope and intercept are calculated from the **training data** at the `.fit()` step.

### Linear Regression in `sklearn`

```{python}
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline


pipeline = make_pipeline(
    LinearRegression())
    
pipeline.fit(X=df_known[['age']], y=df_known['price'])

pipeline.named_steps['linearregression'].intercept_
pipeline.named_steps['linearregression'].coef_
```


### Fitting and predicting

To **predict** from a linear regression, we just plug in the values to the equation...

```{python}
-0.3 + 1.16*df_unknown["age"] 
```

### Fitting and predicting

To **predict** from a linear regression, we just plug in the values to the equation...

```{python}
pipeline.predict(df_unknown[['age']])
```
### Questions to ask yourself{.smaller}

* **Q:** Is there only one "best" regression line?

* **A:** No, you can justify many choices of slope and intercept!  But there is a generally accepted approach called **Least Squares Regression** that we will always use.

* **Q:** How do you know which *variables* to include in the equation? 
* **A:** Try them all, and see what predicts best.

* **Q:** How do you know whether to use *linear regression* or *KNN* to predict?

* **A:** Try them both, and see what predicts best


## Cross-Validation

### Resampling methods

* We saw that a "fair" way to evaluate models was to **randomly split** into **training** and **test** sets.

* But what if this **randomness** was misleading?  *(e.g., a major outlier in one of the sets)*

* What do we usually do in statistics to address randomness?  Take **many samples** and compute an **average**!

* A **resampling method** is when we take *many* random test/training splits and *average* the resulting metrics.

### Resampling method example

Import all our functions:

```{python}
from sklearn.compose import make_column_transformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error


```

### Resampling method example

```{python}
X_train, X_test, y_train, y_test = train_test_split(df_known, df_known['price'], test_size = 0.1)

ct = make_column_transformer(
  (StandardScaler(), ['summer', 'har', 'sep', 'win', 'age']),
  remainder = "drop"
)

pipeline = make_pipeline(
    ct,
    LinearRegression())
    
pipeline = pipeline.fit(X=X_train, y=y_train)

pred_y_test = pipeline.predict(X=X_test)

mean_squared_error(y_test, pred_y_test)
```


### Resampling method example

```{python}

X_train, X_test, y_train, y_test = train_test_split(df_known, df_known['price'], test_size = 0.1)

ct = make_column_transformer(
  (StandardScaler(), ['summer', 'har', 'sep', 'win', 'age']),
  remainder = "drop"
)

pipeline = make_pipeline(
    ct,
    LinearRegression())
pipeline = pipeline.fit(X=X_train, y=y_train)
pred_y_test = pipeline.predict(X=X_test)
mean_squared_error(y_test, pred_y_test)
```

### Cross-Validation

* It makes sense to do test/training many times...

* But!  Remember the original reason for test/training: we don't want to use the **same data** in *fitting* and *evaluation*. 

* Idea: Let's make sure that each observation only gets to be in the test set **once**

* **Cross-validation:** Divide the data into 10 random "folds".  Each fold gets a "turn" as the test set.

### Cross-Validation

![](./images/k_fold_pic.png)

### Cross-Validation in `sklearn`

```{python}
from sklearn.model_selection import cross_val_score

cross_val_score(pipeline, X = df_known, y = df_known['price'], cv = 10)
```

* `sklearn` chooses a **default metric** for you based on the model.

* In this case: *r-squared* for Linear Regression; *negative root mean square error* for KNN.

* (Why negative?  So that we want to *maximize* this score)

### Cross-Validation in `sklearn`

* What if you want **MSE**?

```{python}
cv_scores = cross_val_score(pipeline, X = df_known, y = df_known['price'], cv = 10, scoring = "neg_mean_squared_error")
cv_scores
```

### Cross-Validation: FAQ{.smaller}

* **Q:** How many cross validations should we do?

* **A:** It doesn't matter much!  Typical choices are 5 or 10.  

* **A:** Think about the trade-offs:  larger *training sets* = *more accurate models* but smaller *test sets* = *more uncertainty in evaluation*

* **Q:** What metric should we use?

* **A:** This is your choice! What captures your idea of "successful prediction"?  (But MSE/RMSE is a good default.)

* **Q:** I took statistics before, and I remember some things like "adjusted R-Squared" or "AIC" for model selection.  What about those?

* **A:** Those are Old School, from a time when computers were not powerful enough to do cross-val.  Modern data science uses resampling.



## Activity

### Your turn

1. Use **cross-validation** to choose between Linear Regression and KNN with k = 7, for:

    + Using all predictors.
    + Using just winter rainfall and summer temperature.
    + Using only age.
    
2. Re-run #1, but instead use **mean absolute error**. (You will need to look in the documentation of `cross_val_score` for this!)

## Tuning with `GridSearchCV`

### Tuning

* In previous classes, we tried many different values of $k$ for KNN.

* We also mentioned using **absolute distance** instead of **euclidean distance**.

* Now, we would like to use **cross-validation** to decide between these options.

* `sklearn` provides a nice shortcut for this!

### `GridSearchCV`

```{python}
ct = make_column_transformer(
  (StandardScaler(), ['summer', 'har', 'sep', 'win', 'age']),
  remainder = "drop"
)

pipeline = make_pipeline(
    ct,
    KNeighborsRegressor())
    
from sklearn.model_selection import GridSearchCV

grid_cv = GridSearchCV(
    pipeline,
    param_grid={
        "kneighborsregressor__n_neighbors": range(1, 7),
        "kneighborsregressor__metric": ["euclidean", "manhattan"],
    },
    scoring="neg_mean_squared_error", cv=5)

grid_cv.fit(df_known, df_known['price'])
```

* **How many times did a model get `.fit()` to some data?**


### `GridSearchCV`

```{python}
pd.DataFrame(grid_cv.cv_results_)
```

### `GridSearchCV`

```{python}
pd.DataFrame(grid_cv.cv_results_)[['param_kneighborsregressor__metric', 'param_kneighborsregressor__n_neighbors', 'mean_test_score']]
```

### `GridSearchCV`

```{python}
grid_cv.best_params_
```



### Model evaluation

You have now encountered **three types of decisions** for finding your best model:

1. Which *predictors* should we include, and how should we preprocess them?  (**feature selection**)

2. Should we use *Linear Regression* or *KNN* or something else?  (**Model selection**)

3. Which value of $k$ should we use?  (**hyperparameter tuning**)


### Model evaluation

Think of this like a college sports bracket:

* Gather all your **candidate pipelines** (combinations of *column transformers* and *model specifications*)

* **Tune** each pipeline with cross-validation (regional championships!)

* Determine the **best model type** for each **feature set** (state championships!)

* Determine the **best pipeline** (national championships!)


### Challenge!{.larger}

Of all these options, what is the **number one best model** for wine price prediction, in your opinion?




::: lectitle
# Classification

[Slides](slides-11-classification.html)
:::

---
title: "Classification"
format:
  revealjs: 
    theme: [default, spring.scss]
    incremental: true 
    output-file: slides-11-classification.html
    shift-heading-level-by: -1
execute:
  echo: true
---

```{python}

```

```{r}
#| include: false
library(reticulate)
use_python("/usr/local/bin/python3")
```

```{python}
#| include: false
import numpy as np
import pandas as pd
from plotnine import *
```

## The story so far

### Choosing a Best Model

* We select a **best model** - aka best *prediction procedure* - by **cross-validation**.

* **Feature selection:** Which *predictors* should we include, and how should we preprocess them? 

* **Model selection:** Should we use *Linear Regression* or *KNN* or *Decision Trees* or something else? 

* **Hyperparameter tuning:** Choosing model-specific settings, like $k$ for KNN.

* Each candidate is a **pipeline**; use `GridSearchCV` or `cross_val_score` to score the options

## Case Study: Breast Tissue Classification

### Breast Tissue Classification

Electrical signals can be used to detect whether tissue is cancerous.

::: center
![image](./images/breast_diagram){width=".4\\textwidth"}
:::

The goal is to determine whether a sample of breast tissue is:

1. connective tissue
2. adipose tissue
3. glandular tissue

4. carcinoma
5. fibro-adenoma
6. mastopathy



### Reading in the Data

We will focus on two features:

-   $I_0$: impedivity at 0 kHz,
-   $PA_{500}$: phase angle at 500 kHz.

``` {python}
import pandas as pd
df = pd.read_csv("https://datasci112.stanford.edu/data/BreastTissue.csv")
df
```


### Visualizing the Data

```{python}
#| code-fold: true
from plotnine import *


(ggplot(df, aes(x = "I0", y = "PA500", fill = "Class"))
+ geom_point())
```

## K-Nearest Neighbors Classification

### K-Nearest Neighbors

What would we predict for someone with an $I_0$ of 400 and a $PA_{500}$ of 0.18?

``` {python}
X_train = df[["I0", "PA500"]]
y_train = df["Class"]

X_unknown = pd.DataFrame({"I0": [400], "PA500": [.18]})
X_unknown
```

### K-Nearest Neighbors

```{python}
#| code-fold: true
from plotnine import *

(ggplot()
+ geom_point(df, aes(x = "I0", y = "PA500", fill = "Class"))
+ geom_point(X_unknown, aes(x = "I0", y = "PA500"), size = 4)
)
```


### K-Nearest Neighbors

This process is *almost* identical to KNN *Regression*:

``` {python}
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(
    StandardScaler(),
    KNeighborsClassifier(n_neighbors=5, metric="euclidean"))

pipeline.fit(X_train, y_train)
pipeline.predict(X_unknown)
```

### Probabilities

Which of these two unknown points would we be **more sure** about in our guess?

```{python}
#| code-fold: true
from plotnine import *

X_unknown = pd.DataFrame({"I0": [400, 2200], "PA500": [.18, 0.05]})

(ggplot()
+ geom_point(df, aes(x = "I0", y = "PA500", fill = "Class"))
+ geom_point(X_unknown, aes(x = "I0", y = "PA500"), size = 4)
)
```


### Probabilities

Instead of returning a single predicted class, we can ask it to return
the predicted probabilities for each class.

``` {python}
pipeline.predict_proba(X_unknown)
```

``` {python}
pipeline.classes_
```

How did Scikit-Learn calculate these predicted probabilities?


### Cross-Validation for Classification

We need a different **scoring method** for classification. A simple one is
**accuracy**:

$$\text{accuracy} = \frac{\text{# correct predictions}}{\text{# predictions}}.$$

### Cross-Validation for Classification

``` {python}
from sklearn.model_selection import cross_val_score

scores = cross_val_score(
    pipeline, X_train, y_train,
    scoring="accuracy",
    cv=10)
    
scores
```

    array([0.63636364, 0.81818182, 0.45454545, 0.54545455, 0.63636364,
           0.54545455, 0.5       , 0.6       , 0.4       , 0.7       ])

### Cross-Validation for Classification

As before, we can get an overall estimate of test accuracy by averaging
the cross-validation accuracies:

``` {.python bgcolor="gray"}
scores.mean()
```

But!  Accuracy is not always the best measure of a classification model!

### Confusion matrix

```{python}
from sklearn.metrics import confusion_matrix
pipeline.fit(X_train, y_train)
y_train_predicted = pipeline.predict(X_train)
confusion_matrix(y_train, y_train_predicted)
```
### Confusion matrix

```{python}
pd.DataFrame(confusion_matrix(y_train, y_train_predicted), columns = pipeline.classes_, index = pipeline.classes_)
```
## Activity

### Activity

Use a **grid search** and the **accuracy** score to find the *best* k-value for this modeling problem.

## Classification Metrics

### Case Study: Credit Card Fraud

Data set of credit card transactions from Vesta.

Goal: Predict `isFraud`, where 1 indicates a fraudulent transaction.

``` {python}
df_fraud = pd.read_csv("https://datasci112.stanford.edu/data/fraud.csv")
df_fraud
```


### Classification Model

We can use $k$-nearest neighbors for classification:

``` {python}
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import make_pipeline
from sklearn.compose import make_column_transformer

pipeline = make_pipeline(
    make_column_transformer(
        (OneHotEncoder(handle_unknown="ignore", sparse_output=False),
         ["card4", "card6", "P_emaildomain"]),
        remainder="passthrough"),
    StandardScaler(),
    KNeighborsClassifier(n_neighbors=5))
```

### Training a Classifier

``` {python}
X_train = df_fraud.drop("isFraud", axis="columns")
y_train = df_fraud["isFraud"]
```

```{python}
from sklearn.model_selection import cross_val_score

cross_val_score(
    pipeline,
    X=X_train, y=y_train,
    scoring="accuracy",
    cv=10
).mean()
```


How is the accuracy so high?

### A Closer Look

Let's take a closer look at the labels.

``` {python}
y_train.value_counts()
```

The vast majority of transactions are normal (0)!


### Imbalanced data

If we just predicted that every transaction is normal, the accuracy
would be $1 - \frac{2119}{59054} = .964$.

Even though such predictions would be accurate *overall*, it is
inaccurate for fraudulent transactions. A good model is "accurate for
every class".


## Precision and Recall

* We need a score that measures "accuracy for class $c$".

* There are at least two reasonable definitions:

-   **precision**: $P(\text{correct} | \text{predicted class } c)$

    Among the observations that were predicted to be in class $c$, what
    proportion actually were?

-   **recall**: $P(\text{correct} | \text{actual class} c)$.

    Among the observations that were actually in class $c$, what
    proportion were predicted to be?


### Precision and Recall by Hand

To check our understanding of these definitions, let's calculate a few
precisions and recalls by hand.

First, summarize the results by the **confusion matrix**.

```{python}
from sklearn.metrics import confusion_matrix
pipeline.fit(X_train, y_train)
y_train_ = pipeline.predict(X_train)
confusion_matrix(y_train, y_train_)
```

-   What is the (training) accuracy?

-   What's the precision for normal transactions?

-   What's the recall for normal transactions?

-   What's the precision for fraudulent transactions?

-   What's the recall for fraudulent transactions?

### Tradeoff between Precision and Recall

Can you imagine a classifier that always has 100% recall for class $c$,
no matter the data?

In general, if the model classifies more observations as $c$,

-   recall (for class $c$) $\uparrow$

-   precision (for class $c$) $\downarrow$

How do we compare two classifiers, if one has higher precision and the
other has higher recall?

### F1 Score

The **F1 score** combines precision and recall into a single score:

$$\begin{aligned}
\text{F1 score} &= \text{harmonic mean of precision and recall} \\
\onslide<6->{&= 1 \Big/ \frac{1}{2} \big( \frac{1}{\text{precision}} + \frac{1}{\text{recall}}\big)}
\end{aligned}$$

To achieve a high F1 score, both precision and recall have to be high.
If either is low, then the harmonic mean will be low.

### Estimating Test Precision, Recall, and F1

Remember that each class has its own precision, recall, and F1.

But Scikit-Learn requires that the `scoring=` parameter be a
single number.

For this, we can use

-   `"precision_macro"`

-   `"recall_macro"`

-   `"f1_macro"`

which average the score over the classes.

### F1 Score

``` {python}
cross_val_score(
    pipeline,
    X=X_train, y=y_train,
    scoring="f1_macro",
    cv=10
).mean()
```


### Precision-Recall Curve

Another way to illustrate the tradeoff between precision and recall is
to graph the **precision-recall curve**.

First, we need the predicted probabilities.

``` {python}
y_train_probs_ = pipeline.predict_proba(X_train)
y_train_probs_
```


### Precision-Recall Curve

* By default, Scikit-Learn classifies a transaction as fraud if this
probability is $> 0.5$.

* What if we instead used a threshold $t$ other than $0.5$?

* Depending on which $t$ we pick, we'll get a different precision and
recall. We can graph this tradeoff.


### Precision-Recall Curve

Let's graph the precision-recall curve together in a Colab.

[Link](https://colab.research.google.com/drive/1w-4iGAiL3iXqAl3332teOT0gXv1wNxio?usp=sharing)


## Takeaways

### Takeaways

* We can do **KNN for Classification** by letting the nearest neighbors "vote"

* The number of votes is a "probability"

* A **classification model** must be evaluated differently than a **regression model**.

* One possible metric is **accuracy**, but this is a bad choice in situations with **imbalanced data**.

* **Precision** measures "if we say it's in Class A, is it really?"

* **Recall** measures "if it's really in Class A, did we find it?"

* **F1 Score** is a balance of precision and recall

* **Macro F1 Score** averages the F1 scores of all classes




