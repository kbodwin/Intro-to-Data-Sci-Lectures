---
title: "DS 112 Lecture Notes"
format: 
  html:
    theme: [minty]
    toc: true
    toc-depth: 1
    number-sections: true
    css: styles.css
  pdf: 
    theme: [minty]
    toc: true
    toc-depth: 1
    number-sections: true
    css: styles.css
editor: visual
#execute:
#  cache: true
---

```{python}
#| include: false
import pandas as pd
```


::: lectitle
# Tabular Data Summaries

[Slides](slides-01-tabular-data-summaries.html)
:::

---
title: "Tabular data and variable summaries"
format:  
  revealjs: 
    theme: [default, spring.scss]
    incremental: true 
    shift-heading-level-by: -1
execute:
  echo: true
---

```{r}
#| include: false
library(reticulate)
use_python("/usr/local/bin/python3")
```

```{python}
#| include: false
import numpy as np
import pandas as pd
```



### Data "on disk"

### Data is stored in plain text files

```
name,pclass,survived,sex,age,sibsp,parch,ticket,fare,cabin,embarked,boat,body,home.dest
"Allen, Miss. Elisabeth Walton",1,1,female,29,0,0,24160,211.3375,B5,S,2,,"St Louis, MO"
"Allison, Master. Hudson Trevor",1,1,male,0.9167,1,2,113781,151.5500,C22 C26,S,11,,"Montreal, PQ / Chesterville, ON"
"Allison, Miss. Helen Loraine",1,0,female,2,1,2,113781,151.5500,C22 C26,S,,,"Montreal, PQ / Chesterville, ON"
"Allison, Mr. Hudson Joshua Creighton",1,0,male,30,1,2,113781,151.5500,C22 C26,S,,135,"Montreal, PQ / Chesterville, ON"
"Allison, Mrs. Hudson J C (Bessie Waldo Daniels)",1,0,female,25,1,2,113781,151.5500,C22 C26,S,,,"Montreal, PQ / Chesterville, ON"
"Anderson, Mr. Harry",1,1,male,48,0,0,19952,26.5500,E12,S,3,,"New York, NY"
"Andrews, Miss. Kornelia Theodosia",1,1,female,63,1,0,13502,77.9583,D7,S,10,,"Hudson, NY"
"Andrews, Mr. Thomas Jr",1,0,male,39,0,0,112050,0.0000,A36,S,,,"Belfast, NI"
"Appleton, Mrs. Edward Dale (Charlotte Lamson)",1,1,female,53,2,0,11769,51.4792,C101,S,D,,"Bayside, Queens, NY"
"Artagaveytia, Mr. Ramon",1,0,male,71,0,0,PC 17609,49.5042,,C,,22,"Montevideo, Uruguay"
"Astor, Col. John Jacob",1,0,male,47,1,0,PC 17757,227.5250,C62 C64,C,,124,"New York, NY"
```

* This is called a **csv** (*comma-separated*) file.

* You might see it as `something.csv` or `something.txt`

* `.txt` files might have different separators

### Reading data

We *read the data* into a program like `python` by specifying:

* what **type** of file it is

* **where** the csv file is located (the **"path"**)

* if the file has a **header**

* ... and other information in special cases!

### Example using `pandas` data frame:

```{python}
df = pd.read_csv("https://datasci112.stanford.edu/data/titanic.csv")
df.head()
```

### Check in:

```{python}
df = pd.read_csv("https://datasci112.stanford.edu/data/titanic.csv")
```

:::{.callout}

* What if this file lived on a computer instead of online?

* Why didn't we have to specify that this dataset has a header?

:::

### Looking at rows

```{python}
df.loc[1, :]
df.iloc[1, :]
```

### Looking at rows

:::{.callout}

* What is the difference between `.loc` and `.iloc`?

* What **type** of object is returned?

:::

### loc, iloc, and index

```{python}
#| error: true
df2 = df.set_index('name')
df2.loc[1, :]
df2.iloc[1, :]
```
### loc, iloc, and index

Think of `iloc` as **integer location**.

```{python}
#| error: true
df2.loc["Allison, Master. Hudson Trevor", :]
```


### Looking at columns

```{python}
df.columns
df['home.dest']
```

### Caution: Object types

```{python}

type(df)
type(df.loc[1, :])
type(df['name'])
```




## Summarizing a data frame

### Questions to ask

* Which variables (columns) are **categorical**?

* Which variables are **quantitative**?

* Which variables are **labels** (e.g. names or ID numbers)?

* Which variables are **text**?




### A quick look at the data

```{python}
df.describe()
```
:::{.callout}

* What percent of *Titanic* passengers survived?

* What was the average (mean) fare paid for a ticket?

* What percent of *Titanic* passengers were in First Class?

:::

### Variable types

* The variable `pclass` was **categorical**, but python assumed it was *quantitative*.

* It's our job to check and fix data!

```{python}
df["pclass"] = df["pclass"].astype("category")
```

### Summary of categorical variable

```{python}
df["pclass"].value_counts()
df["pclass"].value_counts(normalize = True)
```




::: lectitle
# Visualization and Conditional Distributions

[Slides](slides-02-conditional_distributions.html)
:::

---
title: "Visualizing and Comparing Categorical Variables"
format:
  revealjs: 
    theme: [default, spring.scss]
    incremental: true 
    output-file: slides-02-conditional-distributions.html
    shift-heading-level-by: -1
execute:
  echo: true
---

```{r}
#| include: false
library(reticulate)
use_python("/usr/local/bin/python3")
```

```{python}
#| include: false
import numpy as np
import pandas as pd
```

## The story so far

### Getting and prepping data

```{python}
df = pd.read_csv("https://datasci112.stanford.edu/data/titanic.csv")
```

```{python}
df["pclass"] = df["pclass"].astype("category")
df["survived"] = df["survived"].astype("category")
```

### Thinking about variable types

```{r}
#| echo: false
library(knitr)
kable(head(py$df))
```

### Accessing rows and columns

::: columns
::: {.column width="50%"}
```{python}
df.iloc[5,]
```
:::

::: {.column width="50%"}
```{python}
df["name"].head()
```
:::
:::

### Quick summary of quantitative variables

```{python}
df.describe()
```

### Summarizing categorical variables

The list of percents for each category is called the **distribution** of the variable.

```{python}
df["pclass"].value_counts()
df["pclass"].value_counts(normalize = True)
```

## Visualizing categorical variables

### The Grammar of Graphics

The *grammar of graphics* is a framework for creating data visualizations.

A visualization consists of:

-   The **aesthetic**: Which *variables* are dictating which *plot elements*.

-   The **geometry**: What *shape* of plot you are making.

-   The **theme**: Other choices about the appearance.

### Example

::: columns
::: {.column width="50%"}
The **aesthetic** is **species** on the x-axis, **bill_length_mm** on the y-axis, colored by **species**.

The **geometry** is a **boxplot**.
:::

::: {.column width="50%"}
```{python}
#| code-fold: true
import pandas as pd
from palmerpenguins import load_penguins
from plotnine import ggplot, geom_point, aes, geom_boxplot

penguins = load_penguins()

(ggplot(penguins, aes(x = "species", y = "bill_length_mm", fill = "species"))
+ geom_boxplot()
)
```
:::
:::

### plotnine

The `plotnine` library implements the *grammar of graphics* in Python.

Code for the previous example:

```{python}
#| eval: false
(ggplot(penguins, aes(x = "species", y = "bill_length_mm", fill = "species"))
+ geom_boxplot()
)
```

-   The `aes()` function is the place to specify aesthetics.

-   `x`, `y`, and `fill` are three possible aesthetics that can be specified, that map variables in our data set to plot elements.

-   A variety of `geom_*` functions allow for different plotting shapes (e.g. boxplot, histogram, etc.)

#### Themes

::: columns
::: {.column width="50%"}
```{python}
#| code-fold: true
(ggplot(penguins, aes(x = "species", y = "bill_length_mm", fill = "species"))
+ geom_boxplot()
)
```
:::

::: {.column width="50%"}
```{python}
#| code-fold: true
from plotnine import theme_classic
(ggplot(penguins, aes(x = "species", y = "bill_length_mm", fill = "species"))
+ geom_boxplot()
+ theme_classic()
)
```
:::
:::

### Check-In

What are the *aesthetics* and *geometry* in the cartoon plot below?

![An xkcd comic of time spent going up the down escalator](images/xkcd.png)

### Bar Plots

To visualize the **distribution** of a categorical variable, we should use a **bar plot**.

```{python}
#| code-fold: true
from plotnine import *
(ggplot(df, aes(x = "pclass"))
+ geom_bar()
+ theme_classic()
)
```

### Percents

```{python}
pclass_dist = df['pclass'].value_counts(normalize=True).to_frame().reset_index()
pclass_dist
```

### Percents

```{python}
#| code-fold: true
(ggplot(pclass_dist, aes(x = "pclass", y = "proportion"))
+ geom_col()   ### notice this change to a column plot!
+ theme_classic()
)
```

## Visualizing two categorical variables

### Option 1: Stacked Bar Plot

```{python}
#| code-fold: true
(ggplot(df, aes(x = "pclass", fill = "sex"))
+ geom_bar()
+ theme_classic()
)
```

### Option 1: Stacked Bar Plot

::: callout
What are some pros and cons of the stacked barplot?
:::

-   Pros:

-   We can still see the total counts in each class

-   We can easily compare the `male` counts in each class, since those bars are on the bottom.

-   Cons:

-   It is hard to compare the `female` counts, since those bars are stacked on top.

-   It is hard to estimate the *distributions*.

### Option 2: Side-by-side barplot

```{python}
#| code-fold: true
(ggplot(df, aes(x = "pclass", fill = "sex"))
+ geom_bar(position = "dodge")
+ theme_classic()
)
```

### Option 2: Side-by-side barplot

::: callout
What are some pros and cons of the side-by-side barplot?
:::

-   Pros:

-   We can easily compare the `female` counts in each class

-   We can easily compare the `male` counts in each class

-   We can easily see counts of each within each class

-   Cons:

-   It is hard to see *total* counts in each class.

-   It is hard to estimate the *distributions*.

### Option 3: Stacked percentage barplot

```{python}
#| code-fold: true
(ggplot(df, aes(x = "pclass", fill = "sex"))
+ geom_bar(position = "fill")
+ theme_classic()
)
```

### Option 3: Stacked percentage barplot

-   Cons:

-   We can no longer see **any** counts!

-   Pros:

-   This is the **best** way to compare sex balance across classes!

-   This is the option I use the most, because it can answer "Are you more likely to find \_\_\_\_\_\_ in \_\_\_\_\_\_ ?" type questions.

## Activity {.smaller}

Choose one of the plots from lecture so far and "upgrade" it.

::: columns
::: {.column width="60%"}
You can do this by:

-   Finding and using a different `theme`

-   Using `+ labs(...)` to change the axis labels and title

-   Trying different variables

-   Trying a different *geometry*

-   Using `+ scale_fill_manual(...)` to change the colors being used

:::

::: {.column width="40%"}
Hints:

-   You will need to use documentation of `plotnine` and online resources!

-   Check out <https://www.data-to-viz.com/> for ideas and example code.

-   Ask GenAI questions like, "What do I add to a plotnine bar plot to change the colors?" *(But of course, make sure you understand the code you use!)*

:::
:::

## Joint distributions

### Two categorical variables

```{python}
df[["pclass", "sex"]].value_counts()
```
### Two-way Table

```{python}
df[["pclass", "sex"]].value_counts().unstack()
```
* This is sometimes called a *cross-tab* or *cross-tabulation*.

### Two-way Table - Percents

```{python}
df[["pclass", "sex"]].value_counts(normalize=True).unstack()
```
* This should add up to 1, aka, 100%!

### Switching variables

```{python}
df[["sex", "pclass"]].value_counts(normalize=True).unstack()
```
### Interpretation

We call this the **joint distribution** of the two variables.

```{python}
#| echo: false
df[["pclass", "sex"]].value_counts(normalize=True).unstack()
```

> Of all the passengers on the Titanic, 11% were females riding in first class.

* NOT "11% of all females on Titanic..."
* NOT "11% of all first class passengers..."


## Conditional distributions

### Conditional distribution from counts

We know that:

* 466 passengers were **female**

* 144 passengers were **females in first class**

So:

* 144/466 = 31% **of female passengers** rode in first class

Here we **conditioned on** the passenger being female, and then looked at the **conditional distribution** of `pclass`.


### Conditional distribution from counts

We know that:

* 35.5% of all passengers were **female**

* 11% of all passengers were **females in first class**

So:

* 0.11/0.355 = 31% **of female passengers** rode in first class

Here we **conditioned on** the passenger being female, and then looked at the **conditional distribution** of `pclass`.

### Swapping variables

We know that:

* 323 passengers **rode in first class**

* 144 passengers were **females in first class**

So:

* 144/323 = 44.6% **of first class passengers** were female

Here we **conditioned on** the passenger being in first class, and then looked at the **conditional distribution** of `sex`.

### Which one to condition on?

* This depends on the **research question** you are trying to answer.

* "What class did most female passengers ride in?"

* ->  Of all *female passengers*, what is the conditional distribution of *class*?

* "What was the gender breakdown of first class?"

* -> Of all *first class passengers*, what is the conditional distribution of *sex*?

### Calculating in python

When we study two variables, we call the individual one-variable distributions the **marginal distribution** of that variable.

::::{.columns}

:::{.column width="50%"}

```{python}
marginal_class = df['pclass'].value_counts(normalize = True)
marginal_class
```

:::

:::{.column width="50%"}

```{python}
marginal_sex = df['sex'].value_counts(normalize = True)
marginal_sex
```

:::

::::

### Calculating in python

We need to divide the **joint distribution** (e.g. "11% of passengers were first class female") by the **marginal distribution** of the variable we want to **condition on** (e.g. 35.5% of passengers were female).

```{python}
joint_class_sex = df[["pclass", "sex"]].value_counts(normalize=True).unstack()
joint_class_sex.divide(marginal_sex)
```
### Conditional on sex

Check:  Should the **rows** or **columns** add up to 100%?  Why?

```{python}
#| echo: false
joint_class_sex = df[["pclass","sex"]].value_counts(normalize=True).unstack()
joint_class_sex.divide(marginal_sex)
```

### Check-In (on the board)

How does `.divide()` work?


### Conditional on class

```{python}
joint_class_sex = df[["sex","pclass"]].value_counts(normalize=True).unstack()
joint_class_sex.divide(marginal_class)
```
### What if you get it backwards?

```{python}
joint_class_sex = df[["pclass","sex"]].value_counts(normalize=True).unstack()
joint_class_sex.divide(marginal_class)
```

### Visualization

Which plot better answers the question, "Did women tend to ride in first class more than men?"

::::{.columns}

:::{.column width="50%"}
```{python}
#| code-fold: true
(ggplot(df, aes(x = "pclass", fill = "sex"))
+ geom_bar(position = "fill")
+ theme_classic()
)
```
:::

:::{.column width="50%"}

```{python}
#| code-fold: true
(ggplot(df, aes(x = "sex", fill = "pclass"))
+ geom_bar(position = "fill")
+ theme_classic()
)
```

:::
::::


## Takeaways

### Takeaways{.smaller}

* We use `plotnine` and the **grammar of graphics** to make visuals.

* For two categorical variables, we might use a **stacked bar plot**, a **side-by-side bar plot**, or a **stacked percentage bar plot** - depending on what we are trying to show.

* The **joint distribution** of two variables gives the percents in each subcategory.

* The **marginal distribution** of a variable is its individual distribution.

* The **conditional distribution** of a variable is its distribution among *only one category* of a different variable.

* We calculate the **conditional distribution** by dividing the **joint** by the **marginal**.



::: lectitle
# Visualizing and Summarizing Quantitative Variables

[Slides](slides-03-quantitative_variables.html)
:::

---
title: "Visualizing and Summarizing Quantitative Variables"
format:
  revealjs: 
    theme: [default, spring.scss]
    incremental: true 
    output-file: slides-03-quantitative_variables.html
    shift-heading-level-by: -1
execute:
  echo: true
---

```{r}
#| include: false
library(reticulate)
use_python("/usr/local/bin/python3")
```

```{python}
#| include: false
import numpy as np
import pandas as pd
from plotnine import *
```

## The story so far

### Getting, prepping, and summarizing data

```{python}
df = pd.read_csv("https://datasci112.stanford.edu/data/titanic.csv")
df["pclass"] = df["pclass"].astype("category")
df["survived"] = df["survived"].astype("category")
```

### Marginal Distributions

If I choose a passenger **at random**, what is the **probability** they rode in 1st class?

```{python}
marginal_class = df['pclass'].value_counts(normalize = True)
marginal_class
```

### Joint Distributions

If I choose a passenger **at random**, what is the **probability** they are a woman who rode in first class?

```{python}
joint_class_sex = df[["pclass", "sex"]].value_counts(normalize=True).unstack()
joint_class_sex
```


### Conditional Distributions

If I choose a **woman** at random, what is the probability they rode in first class?

```{python}
marginal_sex = df['sex'].value_counts(normalize = True)
joint_class_sex.divide(marginal_sex)

```

### Visualizing with the Grammar of Graphics

```{python}
(ggplot(df, aes(x = "sex", fill = "pclass"))
+ geom_bar(position = "fill")
+ theme_classic()
)
```


## Quantitative Variables

We have analyzed a quantitative variable already. Where?

In the Colombia COVID data!

``` {python}
df_CO = pd.read_csv("http://dlsun.github.io/pods/data/covid/colombia_2020-05-28.csv")
df_CO
```

## Visualizing One Quantitative Variable

### Option: Convert it to categorical

To visualize the age variable, we did the following:

``` {python}
df_CO["age"] = pd.cut(
    df_CO["Edad"],
    bins=[0, 10, 20, 30, 40, 50, 60, 70, 80, 120],
    labels=["0-9", "10-19", "20-29", "30-39", "40-49", "50-59", "60-69", "70-79", "80+"],
    right=False)
    
```

### Option: Convert it to categorical

Then, we could treat `age` as categorical and make a **barplot**:

```{python}
#| code-fold: true
(ggplot(df_CO, aes(x = "age"))
+ geom_bar()
+ theme_classic()
)
```


### Better option:  Histogram

A **histogram** uses *equal sized bins* to summarize a *quantitative variable*.

``` {python}
(ggplot(df_CO, aes(x = "Edad"))
+ geom_histogram()
+ theme_classic()
)
```

### Histogram

A histogram **must** use a **quantitative variable** to look right:

``` {python}
#| error: true
(ggplot(df_CO, aes(x = "age"))
+ geom_histogram()
+ theme_classic()
)
```

### Histogram

To tweak your histogram, you can change the **number of bins**:

::::{.columns}

:::{.column width="50%"}

``` {python}
#| code-fold: true
(ggplot(df_CO, aes(x = "Edad"))
+ geom_histogram(bins = 10)
+ theme_classic()
)
```
:::

:::{.column width="50%"}

``` {python}
#| code-fold: true
(ggplot(df_CO, aes(x = "Edad"))
+ geom_histogram(bins = 100)
+ theme_classic()
)
```

:::
::::

### Percents instead of counts

```{python}
(ggplot(df_CO, aes(x = "Edad", y = '..density..'))
+ geom_histogram(bins = 10)
+ theme_classic()
)
```


### Distributions

* Recall the distribution of a categorical variable: What are the **possible values** and **how common** is each?

* The **distribution** of a quantitative variable is similar: The  total *area* in the histogram is 1.0 (or 100%).

``` {python}
#| code-fold: true
(ggplot(df_CO, aes(x = "Edad", y = '..density..'))
+ geom_histogram(bins = 10)
+ theme_classic()
)
```

### Densities

* In this example, we have a limited set of possible values for `age`:  0, 1, 2, ...., 100.  We call this **discrete**.

* What if had a **quantitative variable** with **infinite values**?

* For example:  Price of a ticket on Titanic.

* We call this **continuous**.

* In this case, it is not possible to list all **possible values** and **how likely each one is**.
    + One person paid $2.35
    + Two people paid $12.50
    + One person paid $34.98
    + .....?
    
* Instead, we talk about **ranges** of values.


### Densities

About what percent of people in this dataset are below 18?

``` {python}
#| code-fold: true
(ggplot(df_CO, aes(x = "Edad", y = '..density..'))
+ geom_histogram(bins = 10)
+ theme_classic()
)
```

### Densities

About what percent of people in this dataset are below 18?

```{python}
#| code-fold: true
(ggplot(df_CO, aes(x = "Edad"))
+ geom_density()
+ theme_classic()
)
```


## Summarizing One Quantitative Variable

### Summarizing a Quantitative Variable

If you had to summarize this variable with **one single number**, how would you pick?

```{python}
df_CO['Edad']
```


## Summaries of Center: Mean

### Mean

* One summary of the center of a quantitative variable is the **mean**.

* When you hear "The average age is..." or the "The average income is...", this probably refers to the mean.

* Suppose we have five people, ages: `4, 84, 12, 27, 7`

* The **mean age** is: $$(4 + 84 + 12 + 27 + 7)/5 = 134/5 = 26.8$$

### Notation interlude {.smaller}

* To refer to our data without having to list all the numbers, we use $x_1, x_2, ..., x_n$

* In the previous example, $x_1 = 4, x_2 = 84, x_3 = 12, x_4 = 27, x_5 = 7$.  So, $n = 5$.

* To add up all the numbers, we use the **summation notation**:
$$ \sum_{i = 1}^n x_i = 134$$

* Therefore, the **mean** is:
$$\bar{x} = \frac{1}{n} \sum_{i = 1}^n x_i$$


### Means in python

Long version: find the **sum** and the **number of observations**

``` {python}
sum_age = df_CO["Edad"].sum()
n = len(df_CO)

sum_age/n
```

Short version: use the built-in function!

``` {python}
df_CO["Edad"].mean()
```


### Activity

The mean is only **one option** for summarizing the center of a quantitative variable.  It isn't perfec!

Let's investigate this.

* Plot the density of ticket prices on titanic

* Calculate the *mean* price

* See how many people paid *more than mean price*


### What happened

* Our `fare` data was **skewed right**:  Most values were small, but a few values were very large.

* These large values "pull" the mean up; just how the value `84` pulled the average age up in our previous example.

* So, why do we *like* the mean?

### Squared Error{.smaller}

* Recall: Ages 4, 84, 12, 27, 7.

* Imagine that we had to "guess" the age of the next person.

* If we guess 26.8, then our "squared error" for these five people is:

```{python}
ages = np.array([4, 84, 12, 27, 7])
sq_error = (ages - 26.8)**2
sq_error.round(decimals = 1)
```

* If we guess 20, then our "squared error" for these five people is:

```{python}
sq_error = (ages - 20)**2
sq_error.round(decimals = 1)
```


### Minimizing squared error

```{python}
#| code-fold: true
cs = range(1, 60)
sum_squared_distances = []

for c in cs:
  sum_squared_distances.append(((df_CO["Edad"] - c) ** 2).sum())

res_df = pd.DataFrame({"center": cs, "sq_error":sum_squared_distances})

(ggplot(res_df, aes(x = 'center', y = 'sq_error'))
+ geom_line())
```


## Summaries of Center: Median

### Median{.smaller}

Another summary of center is the **median**, which is the "middle" of
the *sorted* values.

To calculate the median of a quantitative variable with
values $x_1, x_2, x_3, ..., x_n$, we do the following steps:

1.  Sort the values from smallest to largest:
    $$x_{(1)}, x_{(2)}, x_{(3)}, ..., x_{(n)}.$$

2.  The "middle" value depends on whether we have an odd or an even
    number of observations.

    -   If $n$ is odd, then the middle value is $x_{(\frac{n+1}{2})}$.

    -   If $n$ is even, then there are two middle values,
        $x_{(\frac{n}{2})}$ and $x_{(\frac{n}{2} + 1)}$. It is
        conventional to report the mean of the two values (but you can
        actually pick any value between them).

### Median

Ages: 4, 84, 12, 7, 27.  What is the **median**?

Median age in the Columbia data:

``` {python}
df_CO["Edad"].median()
```


## Summaries of Spread: Variance

### Variance

* One measure of spread is the **variance**.

* The variance of a variable whose values are $x_1, x_2, x_3, ..., x_n$ is calculated using the formula
$$\textrm{var(X)} = \frac{\sum_{i=1}^n (x_i - \bar{x})^2}{n - 1}$$

* Does this look familiar?  It's the **sum of squared error**!  (Divided by $n-1$, the "degrees of freedom")

### Variance in python

We could do this manually:

```{python}
(((df_CO["Edad"] - df_CO["Edad"].mean()) ** 2).sum() /
 (len(df_CO) - 1))
```

...or using a built-in Python function.

``` {python}
df_CO["Edad"].var()
```

    348.0870469898451

## Standard Deviation

* Notice that the variance isn't very *intuitive*: what do we mean by "The spread is 348"?

* This is because it is the **squared** error!

* So, to get it in more interpretable language, we take the square root:

```{python}
np.sqrt(df_CO["Edad"].var())
```


Or, we use the built-in function!

``` {python}
df_CO["Edad"].std()
```


## Takeaways

### Takeaway Messages

* Visualize quantitative variables with **histograms** or **densities**.

* Summarize the **center** of a quantitative variable with **mean** or **median**.

* Describe the **shape** of a quantitative variable with **skew**

* Summarize the **spread** of a quantitative variable with the **variance** or the **standard deviation**.



::: lectitle
# Multivariate Summaries

[Slides](slides-04-group_by.html)
:::

---
title: "Multivariate summaries"
format:
  revealjs: 
    theme: [default, spring.scss]
    incremental: true 
    output-file: slides-04-group_by.html
    shift-heading-level-by: -1
execute:
  echo: true
---

```{r}
#| include: false
library(reticulate)
use_python("/usr/local/bin/python3")
```

```{python}
#| include: false
import numpy as np
import pandas as pd
from plotnine import *
```

## The story so far

### Last week

* Reading in data and cleaning/prepping it

* Summarizing **one categorical variable** with a distribution

* Summarizing **two categorical variables** with joint and conditional distributions

* Using `plotnine` and the **grammar of graphics** to make **bar plots** and **column plots**.

### Quantitative variables

* Visualizing by **converting to categorical**

* Visualizing with **histograms** or **densities**

* Estimating **probabilities** from histograms and densities.

* Describing the **skew**

* Calculating and explaining the **mean** and the **median**.

* Calculating and explaining the **standard deviation** and **variance**.

## Comparing quantities across categories

### New dataset: airplane flights

RQ:  Which airline carriers are most likely to be delayed?

Let's look at a data set of all domestic flights that departed from one
of the New York City airports (JFK, LaGuardia, and Newark) on November
16, 2013.

```{python}
data_dir = "https://datasci112.stanford.edu/data/"
df = pd.read_csv(data_dir + "flights_nyc_20131116.csv")
df
```
### Delays

We already know how to **summarize** the flight delays:

*(Check-in: Interpret these numbers!)*

```{python}
df['dep_delay'].median()
df['dep_delay'].mean()
df['dep_delay'].std()
```

### Delays

We already know how to **visualize** the flight delays:

*(Check-in: How would you describe this distribution?)*

```{python}
(ggplot(df, aes(x = 'dep_delay'))
+ geom_histogram()
+ theme_classic())
```

### Delays by origin

RQ: Do the three origin airports (JFK, LGA, EWR) have different delay patterns?

*Check-in:* What could you change in this code to include the `origin` variable?

```{python}
#| eval: false
(ggplot(df, aes(x = 'dep_delay'))
+ geom_histogram()
+ theme_classic())
```

### Delays by origin

Overlapping **histograms** can be really hard to read...

```{python}
#| code-fold: true
(ggplot(df, aes(x = 'dep_delay', fill = 'origin'))
+ geom_histogram()
+ theme_classic())
```

### Delays by origin

... but overlapping **densities** often look nicer...

```{python}
#| code-fold: true
(ggplot(df, aes(x = 'dep_delay', fill = 'origin'))
+ geom_density()
+ theme_classic())
```


### Delays by origin

... especially if we make them a little see-through!

```{python}
#| code-fold: true
(ggplot(df, aes(x = 'dep_delay', fill = 'origin'))
+ geom_density(alpha = 0.5)
+ theme_classic())
```

### Variable transformations

* That last plot was okay, but it was hard to see the details, because the distribution is so **skewed right**.

* Sometimes, for easier visualization, it is worth **transforming** a variable.  

* Often we use a **log** transformation.


### Log transformation{.smaller}

* Example:  Salaries of \$10,000 and \$100,000 and \$10,000,000:

```{python}
#| code-fold: true
dat = pd.DataFrame({"salary": [10000, 100000, 10000000]})
dat["log_salary"] = np.log(dat["salary"])
```


::::{.columns}

:::{.column width="50%"}

```{python}
#| code-fold: true

(ggplot(dat, aes(x = "salary"))
+ geom_histogram(bins = 100)
+ theme_classic())

```


:::

:::{.column width="50%"}

```{python}
#| code-fold: true

(ggplot(dat, aes(x = "log_salary"))
+ geom_histogram(bins = 100)
+ theme_classic())

```
:::

::::

### Log transformations

* Usually, we use the **natural log**, just for convenience.

* Pros:  Skewed data looks **less skewed**, so it is easier to see **patterns**

* Cons: The variable is now on a **different scale** so it is not as **interpretable**

* **Caution!** Log transformations need *positive numbers*.

### Delays by origin - transformed


```{python}
#| code-fold: true

# Shift delays to be above zero
df['delay_shifted'] = df['dep_delay'] - df['dep_delay'].min() + 1

# Log transform
df['log_delay'] = np.log(df['delay_shifted'])

(ggplot(df, aes(x = 'log_delay', fill = 'origin'))
+ geom_density(alpha = 0.5)
+ theme_classic())
```
## Boxplots

### Another option: Boxplots

```{python}
#| code-fold: true

(ggplot(df, aes(y = 'log_delay', x = 'origin'))
+ geom_boxplot()
+ theme_classic())
```

## Facetting

### Facetting

* This plot still was a *little* hard to read.  

* What if we just made **separate plots** for each origin?

* Kind of annoying...


```{python}
is_jfk = (df['origin'] == "JFK")
df_jfk = df[is_jfk]
df_jfk

```

### (Aside: Boolean masking)


```{python}

is_jfk = (df['origin'] == "JFK")
is_jfk

```

### Facetting

Fortunately, `plotnine` (and other plotting packages) has a trick for you!

```{python}
(ggplot(df, aes(x = 'dep_delay'))
+ geom_density()
+ facet_wrap('origin'))
```


## Summaries by groups

### Split-apply-combine{.smaller}

* Our visualizations told us some of the story, but can we use **numeric summaries** as well?

* To do this, we want to calculate the **mean** or **median** delay time for **each** origin airport.

* We call this **split-apply-combine**:  
    + **split** the dataset up by a categorical variable `origin`
    + **apply** a calculation like `mean`
    + **combine** the results back into one dataset
    
* In `pandas`, we use the `groupby()` function to take care of the *split* and *combine* steps!

### Group-by

```{python}
df.groupby("origin")["dep_delay"].mean()
df.groupby("origin")["dep_delay"].median()
```
### Group-by

*Check-in:*  

* Which code is causing "split by origin"?

* Which code is causing "calculate the mean of delays"?

* Which code is causing the re-combining of the data?

```{python}
#| eval: false
df.groupby("origin")["dep_delay"].mean()
```

## Standardized values

### Toy example: exam scores

Hermione's exam scores are is:

* Potions class: 77/100

* Charms class: 95/100

* Herbology class: 90/100

In which class did she do best?


### Toy example: exam scores

**But wait!**

The class **means** are:

* Potions class: 75/100

* Charms class: 85/100

* Herbology class: 85/100

In which class did she do best?

### Toy example: exam scores

**But wait!**

The class **standard deviations** are:

* Potions class: 2 points

* Charms class: 5 points

* Herbology class: 1 point

In which class did she do best?


### Different variabilities by origin

* In addition to having different **centers**, the three origins also have different **spreads**.

```{python}
df.groupby("origin")["dep_delay"].std()
```


* That is, values are **less close** to the mean in general for flights from `LGA` than from `JFK`. 


### Standardized values

Suppose you fly from `LGA` and your flight is 40 minutes late

Your friend flies from `JFK` and their flight is 30 minutes late

Who got "unluckier"?

```{python}
(40 + 0.48)/26
(30 - 1.46)/18.7
```

### Standardized values

* We **standardize** values by subtracting the mean and dividing by the standard deviation.

* This tells us how much better/worse *than typical values* our target value is.

* This is also called the **z-score**. $$z_i = \frac{x_i - \bar{x}}{s_x}$$


### Activity

Research question:  What airlines have the most delays?

* Make a **plot** to answer the question.

* Calculate values to answer the question.

* The first row is a flight from EWR to CLT on US Airways.  The second row is a flight from LGA to IAH on United Airlines.  Which one was a "more extreme" delay?

## Relationships between quantitative variables

### Scatter plots

To **visualize** two quantitative variables, we make a **scatter plot** (or *point* geometry).

RQ: Did *older* passengers pay a *higher* fare on the Titanic?

```{python}
df_titanic = pd.read_csv(data_dir + "titanic.csv")

(ggplot(df_titanic, aes(x = 'age', y = 'fare'))
+ geom_point())
```

### Scatter plots

Notice:

* The **explanatory variable** was on the **x-axis**.

* The **response variable** was on the **y-axis**.

* "If you are older, you pay more" not "If you pay more, you get older".

```{python}
#| eval: false
(ggplot(df_titanic, aes(x = 'age', y = 'fare'))
+ geom_point())
```

### Scatter plots

How could we make this nicer?

1. Do a log-transformation of `fare`, because it is very skewed.

2. Add in a third variable, `pclass`.  How might you do this?


### Scatter plots

Challenge: Can you re-create this plot?

```{python}
#| echo: false
df_titanic['log_fare'] = np.log(df_titanic['fare'])
df_titanic['pclass'] = df_titanic['pclass'].astype('category')

(ggplot(df_titanic, aes(x = 'age', y = 'log_fare', color = 'pclass'))
+ geom_point()
+ theme_classic())
```
### Describing the relationship

Let's look at just third class:

```{python}
#| code-fold: true
is_third= df_titanic['pclass'] == 3
df_third = df_titanic[is_third]

(ggplot(df_third, aes(x = 'age', y = 'fare'))
+ geom_point()
+ theme_classic())
```
### Describing the relationship

This relationship was:

* Not very **strong**: the points don't follow a clear pattern

* Slightly **negative**:  When age was higher, fare was a little lower.

* Not very **linear**: the points don't form a straight line


### Correlation

* What if we want a **numerical summary** of the relationship between variables?

* Do "older than average" people pay "higher than average" fares?

* When the **z-score** of age was high, was the **z-score** of fare high?

### Correlation

```{python}
#| code-fold: true
is_third= df_titanic['pclass'] == 3
df_first = df_titanic[is_third]

mean_age = df_third['age'].mean()
mean_fare = df_third['fare'].mean()

(ggplot(df_third, aes(x = 'age', y = 'fare'))
+ geom_point()
+ geom_vline(xintercept = mean_age, color = "red")
+ geom_hline(yintercept = mean_fare, color = "red")
+ theme_classic())
```

### Correlation

Interpret this result:

```{python}

df_third[['age', 'fare']].corr()

```


### Correlation

Interpret this result:

```{python}

df_third[['age', 'fare']].corr()

```

Age and fare are **slightly negatively correlated**.

*Can you think of an explanation for this?*


### Correlation is not **slope** or **relationship**

![](./images/correlation2.png)


Just for fun:  [Guess the Correlation Game](https://www.guessthecorrelation.com/)

## Takeaways

### Takeaways

* Plot **quantitative variables** across **groups** with **overlapping density** plots, **boxplots**, or by **facetting**.

* Summarize **quantitative variables** across **groups** by using `groupby()` and then calculating summary statisics.

* Know what **split-apply-combine** means.

* Plot **relationships between quantitative variables** with a **scatter plot**

* Describe the **strength**, **direction**, and **shape** of the relationship.

* Summarize **relationships between quantitative variables** with the **correlation**





::: lectitle
# Distances

[Slides](slides-05-distances.html)
:::

---
title: "Distances between observations"
format:
  revealjs: 
    theme: [default, spring.scss]
    incremental: true 
    output-file: slides-05-distances.html
    shift-heading-level-by: -1
execute:
  echo: true
---

```{python}

```

```{r}
#| include: false
library(reticulate)
use_python("/usr/local/bin/python3")
```

```{python}
#| include: false
import numpy as np
import pandas as pd
from plotnine import *
```

## The story so far

### Summarizing

* **One categorical variable:** marginal distribution

* **Two categorical variables:** joint and conditional distributions

* **One quantitative variable:** mean, median, variance, standard deviation.

* **One quantitative, one categorical:** mean, median, and std dev across groups (`groupby()`, *split-apply-combine*)

* **Two quantitative variables:** z-scores, correlation

### Visualizing

* **One categorical variable:** bar plot or column plot

* **Two categorical variables:** stacked bar plot, side-by-side bar plot, or stacked percentage bar plot

* **One quantitative variable:** histogram, density, or boxplot

* **One quantitative, one categorical:** overlapping densities, side-by-side boxplots, or facetting

* **Two quantitative variables:** scatter plot

## Today's data: House prices

### Ames house prices

*(Notice: `read_table` not `read_csv`)*

``` {python}
df = pd.read_table("https://datasci112.stanford.edu/data/housing.tsv")
df.head()
```

### How does size relate to number of bedrooms?

What plot would you make?

. . .

```{python}
(ggplot(df, aes(x = "Gr Liv Area", y = "Bedroom AbvGr")) 
+ geom_point())
```

### How does size relate to number of bedrooms?

What *statistic* would you calculate?

. . .

```{python}
df[["Gr Liv Area", "Bedroom AbvGr"]].corr()
```

## Measuring similarity with distance

### Similarity

How might we answer the question, "Are these two houses similar?"

```{python}
df.loc[1707, ["Gr Liv Area", "Bedroom AbvGr"]]
df.loc[290, ["Gr Liv Area", "Bedroom AbvGr"]]
```
### Distance

The **distance** between the **two observations** is:

$$ \sqrt{ (2956 - 2650)^2 + (5 - 6)^2} = 306 $$

. . .

... what does this number mean?  Not much!  But we can use it to **compare** sets of houses and find the **most similar**.

### Distance

Consider House 1707 and another one:

```{python}
df.loc[1707, ["Gr Liv Area", "Bedroom AbvGr"]]
df.loc[291, ["Gr Liv Area", "Bedroom AbvGr"]]
```
$$ \sqrt{ (2956 - 1666)^2 + (5 - 3)^2} = 1290 $$
House 1707 is **more similar** to House 290 than to House 291.

### (Lecture Activity Part 1)

```{r}
#| echo: false
library(countdown)
countdown(minutes = 10, left = "0")
```

## Scaling/Standardizing

### House 160 seems more similar...

```{python}
#| code-fold: true
(
ggplot(df, aes(x = "Gr Liv Area", y = "Bedroom AbvGr")) 
+ geom_point(color = "lightgrey")
+ geom_point(df.loc[[1707]], color = "red", size = 5)
+ geom_point(df.loc[[160]], color = "blue", size = 2)
+ geom_point(df.loc[[2336]], color = "green", size = 2)
+ theme_classic() 
)
```

### ... even if we zoom in...

```{python}
#| code-fold: true
(
ggplot(df, aes(x = "Gr Liv Area", y = "Bedroom AbvGr")) 
+ geom_point(color = "lightgrey")
+ geom_point(df.loc[[1707]], color = "red", size = 5)
+ geom_point(df.loc[[160]], color = "blue", size = 2)
+ geom_point(df.loc[[2336]], color = "green", size = 2)
+ theme_classic() 
+ scale_x_continuous(limits=(2500, 3500))
)
```

### ... but not if we put the axes on the same **scale**!

```{python}
#| code-fold: true
(
ggplot(df, aes(x = "Gr Liv Area", y = "Bedroom AbvGr")) 
+ geom_point(color = "lightgrey")
+ geom_point(df.loc[[1707]], color = "red", size = 5)
+ geom_point(df.loc[[160]], color = "blue", size = 2)
+ geom_point(df.loc[[2336]], color = "green", size = 2)
+ theme_classic() 
+ scale_x_continuous(limits=(2900, 3000))
+ scale_y_continuous(limits=(0, 100))
)
```

### Scaling

* We need to make sure our features are on the same **scale** before we can use **distances** to measure **similarity**.

* Recall:  **standardizing** = subtract the mean, divide by the standard deviation.

* In this case, the **mean** doesn't really  matter. (why?)


### Scaling

```{python}
df['size_scaled'] = (df['Gr Liv Area'] - df['Gr Liv Area'].mean())/df['Gr Liv Area'].std()
df['bdrm_scaled'] = (df['Bedroom AbvGr'] - df['Bedroom AbvGr'].mean())/df['Bedroom AbvGr'].std()
```

```{python}
#| code-fold: true
(
ggplot(df, aes(x = "size_scaled", y = "bdrm_scaled")) 
+ geom_point(color = "lightgrey")
+ geom_point(df.loc[[1707]], color = "red", size = 5)
+ geom_point(df.loc[[160]], color = "blue", size = 2)
+ geom_point(df.loc[[2336]], color = "green", size = 2)
+ theme_classic() 
)
```

### (Lecture Activity Part 2)


```{r}
#| echo: false
library(countdown)
countdown(minutes = 10, left = "0")
```

## Scikit-learn

### Scikit-learn

* `scikit-learn` is a library for **machine learning** and **modeling**

* We will use it a lot in this class!

* For now, we will use it as a shortcut for *scaling* and for *computing distances*

* The philosophy of `sklearn` is:
    + **specify** your analysis
    + **fit** on the data to prepare the analysis
    + **transform** the data

### Specify

No calculations have happened yet!

```{python}
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler
```

### Fit

The `scaler` object "learns" the means and standard deviations.

We still have not altered the data at all! 

```{python}
df_orig = df[['Gr Liv Area', 'Bedroom AbvGr']]
scaler.fit(df_orig)
scaler.mean_
scaler.scale_
```


### Transform

```{python}
df_scaled = scaler.transform(df_orig)
df_scaled
```

### sklearn, numpy, and pandas

* By default, `sklearn` functions return `numpy` objects.

* This is sometimes annoying; e.g. if we want to plot things after scaling.

* Solution: remake it, with the original column names.

```{python}
pd.DataFrame(df_scaled, columns = df_orig.columns)
```
### Distances with sklearn

```{python}
from sklearn.metrics import pairwise_distances

pairwise_distances(df_scaled[[1707]], df_scaled)
```


### Finding the most similar

```{python}
dists = pairwise_distances(df_scaled[[1707]], df_scaled)
dists.argsort()
```

### Finding the most similar

```{python}
best = dists.argsort().flatten()[1:10]
df_orig.iloc[best]
```


### (Lecture Activity Part 3)

```{r}
#| echo: false
library(countdown)
countdown(minutes = 10, left = "0")
```

## Alternatives

### Other scaling

-   Standardization
    $$x_i \leftarrow \frac{x_i - \bar{X}}{\text{sd}(X)}$$

-   Min-Max Scaling
    $$x_i \leftarrow \frac{x_i - \text{min}(X)}{\text{max}(X) - \text{min}(X)}$$

### Other distances

-   Euclidean ($\ell_2$)

    $$\sqrt{\sum_{j=1}^m (x_j - x'_j)^2}$$

-   Manhattan ($\ell_1$)

    $$\sum_{j=1}^m |x_j - x'_j|$$
    
    
### (Lecture Activity Part 4)

```{r}
#| echo: false
library(countdown)
countdown(minutes = 10, left = "0")
```

## Takeaways

### Takeaways

* We measure similarity between **observations** by calculating **distances**.

* It is important that all our **features** be on the same **scale** for distances to be meaningful.

* We can use `scikit-learn` functions to **fit** and **transform** data, and to compute pairwise distances.

* There are many options of ways to *scale* data; most common is **standardizing**

* There are many options of ways to *measure distances*; most common is **Euclidean distance**.



::: lectitle
# Dummy Variables and Column Tranformers

[Slides](slides-06-preprocessing.html)
:::

---
title: "Dummy variables and Column Transformers"
format:
  revealjs: 
    theme: [default, spring.scss]
    incremental: true 
    output-file: slides-06-preprocessing.html
    shift-heading-level-by: -1
execute:
  echo: true
---

```{python}

```

```{r}
#| include: false
library(reticulate)
use_python("/usr/local/bin/python3")
```

```{python}
#| include: false
import numpy as np
import pandas as pd
from plotnine import *
```

## The story this week

### Distances


* We measure similarity between **observations** by calculating **distances**.

* **Euclidean distance**: sum of squared differences, then square root

* **Manhattan distance**: sum of absolute differences

* In **scikit-learn**, use the `pairwise_distances` function to get back a *2D numpy array* of distances.

### Scaling

* It is important that all our **features** be on the same **scale** for distances to be meaningful.

* **Standardize:**  Subtract the *mean* (of the column) and divide by the *standard deviation* (of the column).

* **MinMax:** Subtract the *minimum* value, divide by the *range*.

* In `scikit-learn`, use the `StandardScaler()` or `MinMaxScaler()` functions.

* Follow the **specify** - **fit** - **transform** code structure.

### Recall: AMES Housing data


``` {python}
df = pd.read_table("https://datasci112.stanford.edu/data/housing.tsv")
features = ["Gr Liv Area", "Bedroom AbvGr", "Full Bath", "Half Bath", "Bldg Type", "Neighborhood"]
df[features].head()
```

## Distances and Categorical Variables

### What about categorical variables?

Suppose we want to include the variable `Bldg Type` in our distance calculation...

```{python}
df["Bldg Type"].value_counts()
```
What is "single family minus townhouse squared"?

### Converting to binary

Let's instead think about "Single family, or not"

```{python}
df["is_single_fam"] = df["Bldg Type"] == "1Fam"
df["is_single_fam"].value_counts()
```
### Converting to binary

Recall that `True/False` is the same as `1/0` for computers:

```{python}
df["is_single_fam"] = df["is_single_fam"].astype("int")
df["is_single_fam"].value_counts()
```
We call this a **dummy variable** or a **one-hot-encoding**.

### Now we can do math!

```{python}
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import pairwise_distances
scaler = StandardScaler()

df_orig = df[['Gr Liv Area', 'Bedroom AbvGr', 'is_single_fam']]
scaler.fit(df_orig)

df_scaled = scaler.transform(df_orig)

dists = pairwise_distances(df_scaled[[1707]], df_scaled)
best = dists.argsort().flatten()[1:10]
df_orig.iloc[best]
```

### Quick look back

#### Where have you seen some one-hot-encoded variables already?


### Let's reset the dataset now...

```{python}
df = pd.read_table("https://datasci112.stanford.edu/data/housing.tsv")
```


## Dummifying Variables

### Dummifying Variables

* What if we don't just want to study `is_single_fam`, but rather, *all* categories of the `Bldg Type` variable?

* In principle, we just make **dummy variables** for **each category**:  `is_single_fam`, `is_twnhse`, etc.

* Each **category** becomes one **column**, with 0's and 1's to show if the *observation in that row* matches that *category*.

* This is called **dummifying** or **one-hot-encoding** a **categorical variable**

* Luckily, we have shortcuts in both `pandas` *and* `sklearn`...

### Dummifying in Pandas

```{python}
pd.get_dummies(df[["Bldg Type"]])
```

### Dummifying in Pandas

Some things to notice here...

1.  What is the **naming convention** for the new columns?

2. Does this change the original dataframe `df`?  If not, what would you need to do to add this information back in?

3. What happens if you put the whole dataframe into the `get_dummies` function?  What problems might arise from this?

### Dummifying in sklearn

```{python}
from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder()

encoder.fit(df[["Bldg Type"]])

df_bldg = encoder.transform(df[["Bldg Type"]])

df_bldg
```


### Dummifying in sklearn

```{python}
df_bldg.todense()
```

### Dummifying in sklearn

Things to notice:

1. What **object type** was the result?

2. Does this change the original dataframe `df`?  If not, what would you need to do to add this information back in?

3. What happens if you fit the whole dataframe with the OneHotEncoder?  What problems might arise from this?

4. What pros and cons do you see for the `pandas` approach vs the `sklearn` approach?


## Column Transformers

### Preprocessing

* We have now seen two **preprocessing** steps that might need to happen to do an analysis of distances:
    + **Scaling** the quantitative variables
    + **Dummifying** the categorical variables
    
* **Preprocessing** steps are things you do *only to make the following analysis/visualization better*.
    
* This is not the same as **data cleaning**, which are changes you make to *fix* the data.

* This is not the same as **data wrangling**, which are changes you make to *restructure* the data; i.e., adding or deleting rows or columns to reflect what you are trying to study.

### Quick quiz

Identify the following as *cleaning*, *wrangling*, or *preprocessing*:

1. Removing the `$` symbol from a column and converting it to numeric.

2. Narrowing your data down to only first class Titanic passengers, because you are not studying the others.

3. Converting a `Zip Code` variable from numeric to categorical using `.astype()`.

4. Creating a new column called `n_investment` that counts the number of people who invested in a project.

5. Log-transforming a column because it is very skewed.

### Preprocessing in `sklearn`

* Unlike **cleaning** and **wrangling**, the **preprocessing** steps are "temporary" changes to the dataframe.

* It would be nice if we could trigger these changes as part of our analysis, instead of doing them "by hand".

* This is why the **specify** - **fit** - **transform** process is useful!

* We will first specify **all** our preprocessing steps.

* Then we will **fit** the whole preprocess

* Then we will save the **transform** step for only when we need it.

### Column Transformers

```{python}
from sklearn.compose import make_column_transformer

preproc = make_column_transformer(
    (OneHotEncoder(), ["Bldg Type", "Neighborhood"]),
    remainder="passthrough")
    
preproc.fit(df[features])

preproc.transform(df[features])
```

### Column Transformers

Things to notice...

1. What submodule did we import `make_column_transformer` from?

2. What are the **two** arguments to the `make_column_transformer` function?  What **object structures** are they?

3. What happens if you **fit** and **transform** on the whole dataset, not just `df[features]`?  Why might this be useful?


### Column Transformers

Try the following:

1. What happens if you change `remainder = "passthrough"` to `remainder = "drop"`? 

2. What happens if you add the argument `sparse_output=False` to the `OneHotEncoder()` function?

3. What happens if you add this line before the *transform* step: *(keep the `sparse_output=False` when you try this)* 
```
preproc.set_output(transform = "pandas")
```

4. Look at the `preproc` object.  What does it show you?

### Multiple preprocessing steps

Why are **column transformers** so useful?  

Well, now we can do **multiple preprocessing steps** at once!

```{python}
from sklearn.preprocessing import StandardScaler

preproc = make_column_transformer(
        (StandardScaler(), ["Gr Liv Area"]),
        (OneHotEncoder(sparse_output=False), ["Bldg Type",
                                          "Neighborhood"]),
    remainder="passthrough")
    
preproc.fit(df[features])

preproc.set_output(transform = "pandas")

df_transformed = preproc.transform(df[features])
df_transformed
```

### Finding all variables

What if we just want to say "Please dummify **all** categorical variables?"

Use a `selector` instead of exact column names.

```{python}
from sklearn.compose import make_column_selector

preproc = make_column_transformer(
    (StandardScaler(),  make_column_selector(dtype_include=np.number)),
    (OneHotEncoder(sparse_output=False), make_column_selector(dtype_include=object)),
    remainder="passthrough")
    
preproc.fit(df[features])

preproc.set_output(transform = "pandas")

df_transformed = preproc.transform(df[features])
df_transformed

```


### Think about it

* What are the *advantages* of using a selector?

* What are the possible *disadvantages* of using a selector?

* Does the *order* matter when using selectors?  Try switching the steps and see what happens!


## Takeaways

### Takeaways

* We **dummify** or **one-hot-encode** categorical variables to make them numbers.

* We can do this with `pd.get_dummies()` or with `OneHotEncoder()`

* **Column Transformers** let us apply multiple preprocessing steps all together.
    + Think about *which variables* you want to apply the steps to
    + Think about *options* for the steps, like sparseness
    + Think about `passthrough` in your transformer



::: lectitle
# Text Data and TF-IDF

[Slides](slides-07-text_data.html)
:::

---
title: "Bag-of-words and TF-IDF"
format:
  revealjs: 
    theme: [default, spring.scss]
    incremental: true 
    output-file: slides-07-text_data.html
    shift-heading-level-by: -1
execute:
  echo: true
---

```{python}

```

```{r}
#| include: false
library(reticulate)
use_python("/usr/local/bin/python3")
```

```{python}
#| include: false
import numpy as np
import pandas as pd
from plotnine import *
```

## The story so far

### Distances


* We measure similarity between **observations** by calculating **distances**.

* **Euclidean distance**: sum of squared differences, then square root

* **Manhattan distance**: sum of absolute differences

* In **scikit-learn**, use the `pairwise_distances` function to get back a *2D numpy array* of distances.

### Scaling

* It is important that all our **features** be on the same **scale** for distances to be meaningful.

* **Standardize:**  Subtract the *mean* (of the column) and divide by the *standard deviation* (of the column).

* **MinMax:** Subtract the *minimum* value, divide by the *range*.

* In `scikit-learn`, use the `StandardScaler()` or `MinMaxScaler()` functions.

* Follow the **specify** - **fit** - **transform** code structure.

## Bag of Words

### Text data


A textual data set consists of multiple texts. Each text is called a
**document**. The collection of texts is called a **corpus**.

Example Corpus:

0.  `"I am Sam\n\nI am Sam\nSam I..."`

1.  `"The sun did not shine.\nIt was..."`

2.  `"Fox\nSocks\nBox\nKnox\n\nKnox..."`

3.  `"Every Who\nDown in Whoville\n..."`

4.  `"UP PUP Pup is up.\nCUP PUP..."`

5.  `"On the fifteenth of May, in the..."`

6.  `"Congratulations!\nToday is your..."`

7.  `"One fish, two fish, red fish..."`

### Reading Text Data

Reading in Textual Data

Documents are usually stored in different files.

``` {python}
seuss_dir = "http://dlsun.github.io/pods/data/drseuss/"
seuss_files = [
    "green_eggs_and_ham.txt", "cat_in_the_hat.txt",
    "fox_in_socks.txt", "how_the_grinch_stole_christmas.txt",
    "hop_on_pop.txt", "horton_hears_a_who.txt",
    "oh_the_places_youll_go.txt", "one_fish_two_fish.txt"]
```

We have to read them in one by one.

``` {python}
import requests

docs = {}
for filename in seuss_files:
    response = requests.get(seuss_dir + filename, "r")
    docs[filename] = response.text
```

### Reading Text Data

```{python}
docs.keys()
```

### Bag-of-Words Representation

In the **bag-of-words** representation in this data, each column represents a word, and the
values in the column are the word counts for that document.

First, we need to count the words in each document.

``` {python}
from collections import Counter
Counter(docs["hop_on_pop.txt"].split())
```

### Bag-of-Words Representation

... then, we put these counts into a `Series` and stack them into a
`DataFrame`.

This is called **bag of words** data.

``` {python}
pd.DataFrame(
    [pd.Series(Counter(doc.split())) for doc in docs.values()],
    index=docs.keys())
```

### Bag-of-Words in Scikit-Learn

Alternatively, we can use `CountVectorizer` in `scikit-learn` to
produce a bag-of-words matrix.

``` {python}
from sklearn.feature_extraction.text import CountVectorizer

vec = CountVectorizer()

vec.fit(docs.values())

vec.transform(docs.values())
```

### Vocabulary

The set of words across a corpus is called the **vocabulary**. We can
view the vocabulary in a fitted `CountVectorizer` as follows:

``` {python}
vec.vocabulary_["fish"]
vec.vocabulary_["pop"]
vec.vocabulary_["eggs"]
```

### Text Normalizing

What's wrong with the way we counted words originally?

    Counter({'UP': 1, 'PUP': 3, 'Pup': 4, 'is': 10, 'up.': 2, ...})

* It's usually good to **normalize** for punctuation and capitalization.

* Normalization options are specified when you initialize the
`CountVectorizer`. 

* By default, Scikit-Learn strips punctuation
and converts all characters to lowercase.

### Text Normalizing in sklearn

* If you don't want Scikit-Learn to normalize for punctuation and
capitalization, you can do the following:

``` {python}
vec = CountVectorizer(lowercase=False, token_pattern=r"[\S]+")
vec.fit(docs.values())
vec.transform(docs.values())
```

## N-grams

### The Shortcomings of Bag-of-Words

Bag-of-words is easy to understand and easy to implement.

What are its disadvantages?

Consider the following documents:

1.  "The dog bit her owner."

2.  "Her dog bit the owner."

Both documents have the same exact bag-of-words representation, but they mean something quite different!

### N-grams{.smaller}

* An **n-gram** is a sequence of $n$ words.

* N-grams allow us to capture more of the meaning.

* For example, if we count **bigrams** (2-grams) instead of words, we can distinguish the two documents from before:

1.  "The dog bit her owner."

2.  "Her dog bit the owner."

$$\begin{array}{l|ccccccc}
& \text{the,dog} & \text{her,dog} & \text{dog,bit} & \text{bit,the} & \text{bit,her} & \text{the,owner} & \text{her,owner} \\
\hline
\text{1} & 1 & 0 & 1 & 0 & 1 & 0 & 1 \\
\text{2} & 0 & 1 & 1 & 1 & 0 & 1 & 0 \\
\end{array}$$

### N-grams in Scikit-Learn

Scikit-Learn can create n-grams.

Pass in `ngram_range=` to the `CountVectorizer`.

To get bigrams, we set the range to `(2, 2)` ...

``` {python}
vec = CountVectorizer(ngram_range=(2, 2))
vec.fit(docs.values())
vec.transform(docs.values())
```

### N-grams in Scikit-learn

... or we can also get individual words (unigrams) alongside the bigrams:

```{python}
vec = CountVectorizer(ngram_range=(1, 2))
vec.fit(docs.values())
vec.transform(docs.values())
```


## Text Data and Distances

### Similar documents

Now, we can use this **bag-of-words** data to measure **similarities** between documents!

``` {python}
from sklearn.metrics import pairwise_distances

vec = CountVectorizer(ngram_range=(1, 2))
vec.fit(docs.values())
dat = vec.transform(docs.values())

dists = pairwise_distances(dat)
dists
```

### Similar documents

```{python}
dists[0].argsort()
docs.keys()
```
* This is how data scientists do **authorship identification**!


## Activity

### Activity 1

Using bi-grams, unigrams, and tri-grams, which Dr. Seuss document is closest to "One Fish Two Fish"?

```{python}
#| include: false

vec = CountVectorizer(ngram_range=(1,3))
vec.fit(docs.values())
bow_seuss = vec.transform(docs.values())
pairwise_distances(bow_seuss)[7]
```

## Motivating example

### Issues with the distance approach

**BUT WAIT!**

* Don't we care more about *word choice* than *total words used*?

* Wouldn't a *longer document* have *more words*, and thus be able to "match" other documents?

* Wouldn't *more common words* appear in more documents, and thus cause them to "match"?

* Recall: We have many options for **scaling**

* Recall: We have many options for **distance metrics**.

### Example{.smaller}

**Document A:**

> "Whoever has hate for his brother is in the darkness and walks in the darkness."

**Document B:**

> "Hello darkness, my old friend, I've come to talk with you again."

**Document C:**

> "Returning hate for hate multiplies hate, adding deeper darkness to
a night already devoid of stars. Darkness cannot drive out darkness; only light can do that."

**Document D:**

> "Happiness can be found in the darkest of times, if only one remembers to turn on the light."

### Example

```{python}
#| code-fold: true
documents = [
    "whoever has hate for his brother is in the darkness and walks in the darkness",
    "hello darkness my old friend",
    "returning hate for hate multiplies hate adding deeper darkness to a night already devoid of stars darkness cannot drive out darkness only light can do that",
    "happiness can be found in the darkest of times if only one remembers to turn on the light"
]
```

```{python}
#| code-fold: true
from sklearn.feature_extraction.text import CountVectorizer

vec = CountVectorizer(token_pattern=r"\w+")
vec.fit(documents)
bow_matrix = vec.transform(documents)
bow_matrix
```

### Example

```{python}
#| warning: false
bow_dataframe = pd.DataFrame(bow_matrix.todense(), columns=vec.get_feature_names_out())
bow_dataframe[["darkness", "light"]]
```

### Measuring similarity

```{python}
#| code-fold: true
from sklearn.metrics import pairwise_distances

pairwise_distances(bow_matrix)
```

## Cosine Distance

### Choosing your distance metric

Is **euclidean distance** really the best choice?!

> My name is James Bond, James Bond is my name.

> My name is James Bond.

> My name is James.

* If we count words the second two will be the most similar.

* The first document is longer, so it has "double" counts.

* But, it has the exact same words as the first document!

* Solution: **cosine distance** (on the board)

### Cosine Distance

As a rule, **cosine distance** is a better choice for bag-of-words data!

```{python}
from sklearn.metrics.pairwise import cosine_distances
cosine_distances(bow_matrix)
```

## TF-IDF


### Scaling terms

Which of these seems most important for measuring similarity?

* Document B, C, D all have the word "to"

* Documents A, B, and C all have the word **darkness**.

* Document A and Document C both have the word "hate"

* Document C and Document D both have the word "light"

* We would like to **scale** our **word counts** by the **document length** (TF).

* We would also like to **scale** our **word counts** by the **number of documents they appear in**. (IDF)


### Term Frequencies (TF)

* If a document is longer, it is more likely to share words.

* Let's use *frequencies* instead of *counts*


```{python}
bow_totals = bow_dataframe.sum(axis = 1)
bow_totals
bow_tf = bow_dataframe.divide(bow_totals, axis = 0)
bow_tf
```

### Term Frequencies (TF)

```{python}
cosine_distances(bow_tf)
```


### Inverse Document Frequency (IDF)

* In principle, if two documents **share rarer words** they are **more similar**.

* What matters is not *overall word frequency* but **how many of the documents** have that word.

* Compute document frequency:

```{python}
has_word = (bow_dataframe > 0)
has_word[["darkness", "light"]]
```

### IDF

```{python}
bow_df = has_word.sum(axis = 0)/4
bow_df
```

### IDF

Adjust for inverse document frequencies:

```{python}
bow_log_idf = np.log(1/bow_df)
bow_tf_idf = bow_tf.multiply(bow_log_idf, axis = 1)
bow_tf_idf[["darkness", "light"]]
```
### TF-IDF

```{python}
cosine_distances(bow_tf_idf).round(decimals = 2)
```

### TF-IDF in Sklearn

```{python}
from sklearn.feature_extraction.text import TfidfVectorizer

# The options ensure that the numbers match our example above.
vec = TfidfVectorizer(smooth_idf=False, norm=None)
vec.fit(documents)
tfidf_matrix = vec.transform(documents)

cosine_distances(tfidf_matrix)
```


## Activity 2

### Activity


Using bi-grams, unigrams, and tri-grams, which Dr. Seuss document is closest to "One Fish Two Fish"?

```{python}
#| include: false
from sklearn.metrics import pairwise_distances

vec = TfidfVectorizer(smooth_idf=False, norm=None, ngram_range=(1,3))
vec.fit(docs.values())
bow_seuss = vec.transform(docs.values())
cosine_distances(bow_seuss)[7]
```

## Takeaways

### Takeaways

* We represent **text data** as a **bag-of-words** or **bag-of-n-grams** matrix.

* Each row is a **document** in the **corpus**.

* We typically use **cosine distance** to measure similarity, because it captures **patterns of word choice**

* We apply **TF-IDF** transformations to **scale** the bag-of-words data, so that words that **appear in fewer documents** are **more important**



::: lectitle
# K Nearest Neighbors

[Slides](slides-08-knn.html)
:::

---
title: "K-Nearest-Neighbors"
format:
  revealjs: 
    theme: [default, spring.scss]
    incremental: true 
    output-file: slides-08-knn.html
    shift-heading-level-by: -1
execute:
  echo: true
---

```{python}

```

```{r}
#| include: false
library(reticulate)
use_python("/usr/local/bin/python3")
```

```{python}
#| include: false
import numpy as np
import pandas as pd
from plotnine import *
```

## The story so far

### Steps for data analysis

* **Read** and then **clean** the data
  + Are there missing values?  Will we drop those rows, or replace the missing values with something?
  + Are there *quantitative* variables that python thinks are *categorical*?
  + Are there *categorical* variables that python thinks are *quantitative*?
  + Are there any *anomalies* in the data that concern you?
  
### Steps for data analysis (cont'd)

* **Explore** the data by **visualizing** and **summarizing**.
  + Different approaches for different combos of *quantitative* and *categorical* variables
  + Think about *conditional* calculations (split-apply-combine)
  
### Steps for data analysis (cont'd)

* Identify a **research question** of interest.

* Perform **preprocessing** steps
  + Should we *scale* the quantitative variables?
  + Should we *one-hot-encode* the categorical variables?
  + Should we *log-transform* any variables?


* Measure similarity between **observations** by calculating **distances**.
  + Which *features* should be included?
  + Which *distance metric* should we use?

## Predicting wine prices

### Data:  Wine qualities

```{python}
df = pd.read_csv("https://dlsun.github.io/pods/data/bordeaux.csv")
df
```

### Data: Wine qualities

* Our goal is to **predict** what will be the quality (price) of wines in a **future year**.

* Idea:  Wines with similar **features** probably have similar **quality**.

* **Inputs**: Summer temperature, harvest rainfall, september temperature, winter rainfall, age of wine.

* **Output:** Price in 1992.

* "Inputs" = "Features" = "Predictors" = "Independent variables"

* "Output" = "Target" = "Dependent variable"

### Similar wines

Which wines have similar summer temps and winter rainfall to the 1989 vintage?

```{python}
#| code-fold: true
from plotnine import *

(ggplot(df, aes(x = "summer", y = "win"))
+ geom_point(color = "white")
+ geom_text(aes(label = "year"))
+ theme_classic())
```

### Predicting 1989

```{python}
df[df['year'] == 1990]

df[df['year'] == 1976]
```


### Training and test data

* The data for which we **know the target** is called the **training data**.

* The data for which we **don't know the target**  (and want to predict it) is calledthe **test data**.

``` {python}
known_prices = df['year'] < 1981
to_predict = df['year'] == 1989

df_train = df[known_prices].copy()
df_test = df[to_predict].copy()
```

### Specify steps

First we make a column transformer...

```{python}
from sklearn.compose import make_column_transformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler

preproc = make_column_transformer(
  (StandardScaler(), ['summer', 'har', 'sep', 'win','age']),
  remainder = "drop"
)
```

### Fit the preprocesser

Then we **fit** it on the **training data**

```{python}
preproc.fit(df_train)

preproc.named_transformers_['standardscaler'].mean_
preproc.named_transformers_['standardscaler'].var_
```
### Prep the data

Then we **tranform** the **training data** AND the **test data**:

```{python}
train_new = preproc.transform(df_train)
test_new = preproc.transform(df_test)

test_new
```

### Fitting vs transforming

What if we had fit on the test data?

```{python}
preproc.fit(df_test)

preproc.named_transformers_['standardscaler'].mean_
preproc.named_transformers_['standardscaler'].var_
```

### Fitting vs transforming

What if we had fit on the test data?

```{python}
preproc.transform(df_test)
```

### All together:

```{python}
preproc = make_column_transformer(
  (StandardScaler(), ['summer', 'har', 'sep', 'win','age']),
  remainder = "drop"
)

preproc.fit(df_train)

train_new = preproc.transform(df_train)
test_new = preproc.transform(df_test)
```


### Find the closest *k*

```{python}
from sklearn.metrics import pairwise_distances

pairwise_distances(test_new, train_new)
```

### Find the closest *k*

```{python}
dists = pairwise_distances(test_new, train_new)

dists[0].argsort()
```

### Find the closest *k*

```{python}
ranked_train = df_train.loc[dists[0].argsort()]
ranked_train
```
### Predict from the closest *k*

If $k = 1$ ...

```{python}
ranked_train = df_train.loc[dists[0].argsort()]
ranked_train.iloc[0]['price']
```


### Predict from the closest *k*

If $k = 100$ ...

```{python}
ranked_train = df_train.loc[dists[0].argsort()]
ranked_train.iloc[0:99]['price'].mean()
```


### Predict from the closest *k*

If $k = 5$ ...

```{python}
ranked_train = df_train.loc[dists[0].argsort()]
ranked_train.iloc[0:4]['price'].mean()
```


## Activity

### Activity 1

Find the predicted 1992 price for *all* the unknown wines, with

* $k = 1$

* $k = 5$

* $k = 10$

How close was each prediction to the right answer?

*(Optional hint: Write a function to help you!)*

### Activity 2 (together)

Find the predicted 1992 price for all the *training data*, with

* $k = 1$

* $k = 5$

* $k = 10$

How close was each prediction to the right answer?

*(Optional hint: Write a function to help you!)*


## K-Nearest-Neighbors

### KNN{.smaller}

We have existing observations

$$(X_1, y_1), ... (X_n, y_n)$$
Where $X_i$ is a set of features, and $y_i$ is a target value.

Given a new observation $X_{new}$, how do we predict $y_{new}$?

1.  Find the $k$ values in $(X_1, ..., X_n)$ that are closest to $X_{new}$

2.  Take the average of the corresponding $y_i$'s to our five closest $X_i$'s.

3. Predict $\hat{y}_{new}$ = average of these $y_i$'s

### KNN

To perform **K-Nearest-Neighbors**, we choose the **K** closest observations to our *target*, and we average their *response* values.

The Big Questions:

* What is our definition of **closest**?

* What number should we use for **K**?

* How do we evaluate the **success** of this approach?


### KNN in sklearn

```{python}
from sklearn.neighbors import KNeighborsRegressor
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(
  preproc,
  KNeighborsRegressor(n_neighbors=5)
  )
          
pipeline
```
### KNN in sklearn

```{python}
pipeline.fit(y = df_train['price'], X = df_train)
pipeline.predict(X=df_test)
```



### Activity 2

Find the predicted 1992 price for *all wines*, with

* $k = 1$

* $k = 5$

* $k = 10$

How close was each prediction to the right answer?




::: lectitle
# Modeling and Machine Learning

[Slides](slides-09-modeling.html)
:::

---
title: "Modeling"
format:
  revealjs: 
    theme: [default, spring.scss]
    incremental: true 
    output-file: slides-09-modeling.html
    shift-heading-level-by: -1
execute:
  echo: true
---

```{python}

```

```{r}
#| include: false
library(reticulate)
use_python("/usr/local/bin/python3")
```

```{python}
#| include: false
import numpy as np
import pandas as pd
from plotnine import *
```

## The story so far

### Steps for data analysis

* **Read** and then **clean** the data
  + Are there missing values?  Will we drop those rows, or replace the missing values with something?
  + Are there *quantitative* variables that python thinks are *categorical*?
  + Are there *categorical* variables that python thinks are *quantitative*?
  + Are there any *anomalies* in the data that concern you?
  
### Steps for data analysis (cont'd)

* **Explore** the data by **visualizing** and **summarizing**.
  + Different approaches for different combos of *quantitative* and *categorical* variables
  + Think about *conditional* calculations (split-apply-combine)
  
### Steps for data analysis (cont'd)

* Identify a **research question** of interest.

* Perform **preprocessing** steps
  + Should we *scale* the quantitative variables?
  + Should we *one-hot-encode* the categorical variables?
  + Should we *log-transform* any variables?


* Measure similarity between **observations** by calculating **distances**.
  + Which *features* should be included?
  + Which *distance metric* should we use?



## Machine Learning and Statistical Modeling

### Modeling

Every analysis we will do assumes a structure like:

::: {style="font-size: 150%;"}
(**output**) = f(**input**) + (noise)
:::

... or, if you prefer...

::: {style="font-size: 150%;"}
(**target**) = f(**predictors**) + (noise)
:::

### Generative process

In any case: we are trying to reconstruct information in **data**, and we are hindered by **random noise**.

The function $f$ might be very simple...

$$y_i = \mu + \epsilon_i$$

"A person's height is the true average height of people in the world, plus some randomness."

### Generative process

... or more complex...

$$y_i = 0.5*x_{1i} + 0.5*x_{2i} + \epsilon_i$$

"A person's height is equal to the average of their biological mother's height and biological father's height, plus some randomness"

* Do you think there is "more randomness" in the first function or this one?

### Generative process

... or extremely, ridiculously complex...

![](./images/complex_function.png)

### Generative process

... and it doesn't have to be a *mathematical* function at all; just a procedure:

$$y_i = \text{(average of heights of 5 people with most similar weights)} + \epsilon_i$$

### Modeling

* Our goal is to **reconstruct** or **estimate** or **approximate** the function/process $f$ based on **training data**.

* For example: Instead of the 5 most similar weights *in the whole world*, we can estimate with the 5 most similar weights *in our training set*.

* Instead of committing to one $f$ to estimate, we might propose **many** options and see which one "leaves behind" the least randomness.

## Data: Wine price prediction

### Setup

```{python}
import pandas as pd
from sklearn.compose import make_column_transformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler


import pandas as pd

df = pd.read_csv("https://dlsun.github.io/pods/data/bordeaux.csv",
                 index_col="year")
                 
df_train = df.loc[:1980].copy()
df_unknown = df.loc[1981:].copy()
```

### KNN revisited

```{python}
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsRegressor

ct = make_column_transformer(
  (StandardScaler(), ['summer', 'har', 'sep', 'win', 'age']),
  remainder = "drop"
)

pipeline = make_pipeline(
          ct,
          KNeighborsRegressor(n_neighbors=5))
pipeline.fit(X=df_train, y=df_train['price'])
pipeline.predict(X=df_unknown)
```

### Measuring error

The most common way to measure "leftover noise" is the **sum of squared error** or equivalently, the **mean squared error**

```{python}
#| code-fold: true
pred_y_train = pipeline.predict(X=df_train)
results = pd.DataFrame({
  "real_prices": df_train['price'],
  "predicted_prices": pred_y_train,
})
results["error"] = results["predicted_prices"] - results["real_prices"]
results["squared error"]= (results["error"])**2
results
```

### Measuring error

The most common way to measure "leftover noise" is the **sum of squared error** or equivalently, the **mean squared error**

```{python}
results["squared error"].mean()
```

### Best K

Now let's try it for some different values of $k$

```{python}
for k in [1, 3, 5, 10, 25]:
#| code-fold: true
  pipeline = make_pipeline(
    ct,
    KNeighborsRegressor(n_neighbors=k))
  pipeline = pipeline.fit(X=df_train, y=df_train['price'])
  pred_y_train = pipeline.predict(X=df_train)
  ((df_train['price'] - pred_y_train)**2).mean()

```

### Training error vs test error

* Oh no! Why did we get an error of 0 for $k = 1$?

* Because the closest wine in the training set is... itself.

* So, our problem is:
    + If we predict on the **new data**, we don't know the **true prices** and we can't **evaluate** our models
    + If we predict on the **training data**, we are "cheating", because we are using the data to both **train** and **test**.
    
* Solution: Let's make a pretend **test data** set!

### Test/Training split

```{python}
df_train = df.loc[:1970].copy()
df_test = df.loc[1971:1980].copy()
```

* We will **train** on the years up to 1970

* We will **test** on the years 1971 to 1980

* We will **evaluate** based on model performance on the **test data**.

### Try again: Best K

```{python}
for k in range(1,15):
#| code-fold: true
  pipeline = make_pipeline(
    ct,
    KNeighborsRegressor(n_neighbors=k))
  pipeline = pipeline.fit(X=df_train, y=df_train['price'])
  
  pred_y_test = pipeline.predict(X=df_test)
  ((df_test['price'] - pred_y_test)**2).mean()
```

### Tuning

* Here we tried the same **type** of model (KNN) each time.

* But we tried different **models** because we used different values of $k$

* This is called **tuning**

### Activity

Perform tuning for a KNN model, but with **all possible values of k**.

Do this for three *recipes* or *column transformers*:

1. Using all predictors.

2. Using just winter rainfall and summer temperature.

3. Using only age.

Which of the many model options performed best?

```{python}
ct2 = make_column_transformer(
  (StandardScaler(), ['summer', 'win']),
  remainder = "drop"
)
ct3 = make_column_transformer(
  (StandardScaler(), ['age']),
  remainder = "drop"
)
#| include: false
for k in range(1,17):
#| code-fold: true
  pipeline = make_pipeline(
    ct,
    KNeighborsRegressor(n_neighbors=k))
  pipeline = pipeline.fit(X=df_train, y=df_train['price'])
  pred_y_test = pipeline.predict(X=df_test)
  print(str(k) + ":" + str(((df_test['price'] - pred_y_test)**2).mean()))
  
for k in range(1,17):
#| code-fold: true
  pipeline = make_pipeline(
    ct2,
    KNeighborsRegressor(n_neighbors=k))
  pipeline = pipeline.fit(X=df_train, y=df_train['price'])
  pred_y_test = pipeline.predict(X=df_test)
  print(str(k) + ":" + str(((df_test['price'] - pred_y_test)**2).mean()))
  
for k in range(1,17):
#| code-fold: true
  pipeline = make_pipeline(
    ct3,
    KNeighborsRegressor(n_neighbors=k))
  pipeline = pipeline.fit(X=df_train, y=df_train['price'])
  pred_y_test = pipeline.predict(X=df_test)
  print(str(k) + ":" + str(((df_test['price'] - pred_y_test)**2).mean()))
```

### Things to think about{.smaller}

* What other **types of models** could we have tried?

* *Linear regression*, *decision tree*, *neural network*, ...

* What other **column transformers** could we have tried?

* *Different combinations of variables, different standardizing, log transforming...*

* What if we measure **error** differently?

* *Mean absolute error*, *log-error*, *percent error*, ...

* What if we had used a **different test set**?

* *Coming soon: Cross-validation*

* What if our target variable was **categorical**?


## Modeling: General Procedure

### Modeling{.smaller}

We apply the process:

<u>For each model proposed: </u>

* Establish a **pipeline** with **transformers** and a **model**.

* **Fit** the pipeline on the **training data** (with known outcome)

* **Predict** with the fitted pipeline on **test data** (with known outcome)

* **Evaluate** our success; i.e., measure noise "left over"

<u>Then: </u>

* **Select** the best model

* **Fit** on *all* the data

* **Predict** on any future data (with unknown outcome)

### Big decisions

* Which models to try

* Which column transformers to try

* How much to tune

* How to measure "success" of a model




::: lectitle
# Regression and Cross-Validation

[Slides](slides-10-cross_val.html)
:::

---
title: "Cross-Validation and Grid Search"
format:
  revealjs: 
    theme: [default, spring.scss]
    incremental: true 
    output-file: slides-10-cross_val.html
    shift-heading-level-by: -1
execute:
  echo: true
---

```{python}

```

```{r}
#| include: false
library(reticulate)
use_python("/usr/local/bin/python3")
```

```{python}
#| include: false
import numpy as np
import pandas as pd
from plotnine import *
```

## The story so far

### Modeling

We assume some process $f$ is *generating* our target variable:

::: {style="font-size: 150%;"}
(**target**) = f(**predictors**) + (noise)
:::

Our goal is to come up with an approximation of $f$.


### Test error vs training error{.smaller}

* We don't need to know how well our model does on *training data*.

* We want to know how well it will do on *test data*.

* In general, test error $>$ training error.

> Analogy: A professor posts a practice exam before an exam.

> -   If the actual exam is the same as the practice exam, how many points will students miss? That's training error.

> -   If the actual exam is different from the practice exam, how many points will students miss? That's test error.

> It's always easier to answer questions that you've seen before than
questions you haven't seen.


### Modeling Procedure {.smaller}

We apply the process:

<u>For each model proposed: </u>

* Establish a **pipeline** with **transformers** and a **model**.

* **Fit** the pipeline on the **training data** (with known outcome)

* **Predict** with the fitted pipeline on **test data** (with known outcome)

* **Evaluate** our success; i.e., measure noise "left over"

<u>Then: </u>

* **Select** the best model

* **Fit** on *all* the data

* **Predict** on any future data (with unknown outcome)


## Linear Regression

### Simple Linear Model

We assume that the target ($Y$) is generated from **an equation** of the predictor ($X$), plus random noise ($\epsilon$)

$$Y = \beta_0 + \beta_1 X + \epsilon$$


Goal:  Use observations $(x_1, y_1), ..., (x_n, y_n)$ to estimate $\beta_0$ and $\beta_1$.

### Measures of success

What is the "best" choice of $\hat{\beta}_0$ and $\hat{\beta}_1$?

* The ones that are **statistically most justified**, under certain assumptions about $Y$ and $X$?

* The ones that are "closest to" the observed points?

    + $|\hat{y}_i - y_i|$?
    + $(\hat{y}_i - y_i)^2$?
    + $(\hat{y}_i - y_i)^4$?
    
### Example: Wine data

```{python}
#| code-fold: true

df = pd.read_csv("https://dlsun.github.io/pods/data/bordeaux.csv",
                 index_col="year")
                 
df_known = df.loc[:1980].copy()
df_unknown = df.loc[1981:].copy()

(ggplot(df_known, aes(x = "age", y = "price")) 
+ geom_point())

```

### "Candidate" lines

Condsider five possible regression equations:

$$\text{price} = 25 + 0*\text{age}$$
$$\text{price} = 0 + 10*\text{age}$$
$$\text{price} = 20 + 1*\text{age}$$
$$\text{price} = -40 + 3*\text{age}$$

Which one do you think will be "closest" to the points on the scatterplot?

### "Candidate" lines

```{python}
(ggplot(df_known, aes(x = "age", y = "price")) 
+ geom_point()
+ geom_abline(intercept = 25, slope = 0)
+ geom_abline(intercept = 0, slope = 1)
+ geom_abline(intercept = 20, slope = 1)
+ geom_abline(intercept = -40, slope = 3))
```

### The "best" slope and intercept

* It's clear that some of these lines are better than others.

* How to choose the best?  **Math**

* We'll let the computer do it for us.

* **Important:** The slope and intercept are calculated from the **training data** at the `.fit()` step.

### Linear Regression in `sklearn`

```{python}
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline


pipeline = make_pipeline(
    LinearRegression())
    
pipeline.fit(X=df_known[['age']], y=df_known['price'])

pipeline.named_steps['linearregression'].intercept_
pipeline.named_steps['linearregression'].coef_
```


### Fitting and predicting

To **predict** from a linear regression, we just plug in the values to the equation...

```{python}
-0.3 + 1.16*df_unknown["age"] 
```

### Fitting and predicting

To **predict** from a linear regression, we just plug in the values to the equation...

```{python}
pipeline.predict(df_unknown[['age']])
```
### Questions to ask yourself{.smaller}

* **Q:** Is there only one "best" regression line?

* **A:** No, you can justify many choices of slope and intercept!  But there is a generally accepted approach called **Least Squares Regression** that we will always use.

* **Q:** How do you know which *variables* to include in the equation? 
* **A:** Try them all, and see what predicts best.

* **Q:** How do you know whether to use *linear regression* or *KNN* to predict?

* **A:** Try them both, and see what predicts best


## Cross-Validation

### Resampling methods

* We saw that a "fair" way to evaluate models was to **randomly split** into **training** and **test** sets.

* But what if this **randomness** was misleading?  *(e.g., a major outlier in one of the sets)*

* What do we usually do in statistics to address randomness?  Take **many samples** and compute an **average**!

* A **resampling method** is when we take *many* random test/training splits and *average* the resulting metrics.

### Resampling method example

Import all our functions:

```{python}
from sklearn.compose import make_column_transformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error


```

### Resampling method example

```{python}
X_train, X_test, y_train, y_test = train_test_split(df_known, df_known['price'], test_size = 0.1)

ct = make_column_transformer(
  (StandardScaler(), ['summer', 'har', 'sep', 'win', 'age']),
  remainder = "drop"
)

pipeline = make_pipeline(
    ct,
    LinearRegression())
    
pipeline = pipeline.fit(X=X_train, y=y_train)

pred_y_test = pipeline.predict(X=X_test)

mean_squared_error(y_test, pred_y_test)
```


### Resampling method example

```{python}

X_train, X_test, y_train, y_test = train_test_split(df_known, df_known['price'], test_size = 0.1)

ct = make_column_transformer(
  (StandardScaler(), ['summer', 'har', 'sep', 'win', 'age']),
  remainder = "drop"
)

pipeline = make_pipeline(
    ct,
    LinearRegression())
pipeline = pipeline.fit(X=X_train, y=y_train)
pred_y_test = pipeline.predict(X=X_test)
mean_squared_error(y_test, pred_y_test)
```

### Cross-Validation

* It makes sense to do test/training many times...

* But!  Remember the original reason for test/training: we don't want to use the **same data** in *fitting* and *evaluation*. 

* Idea: Let's make sure that each observation only gets to be in the test set **once**

* **Cross-validation:** Divide the data into 10 random "folds".  Each fold gets a "turn" as the test set.

### Cross-Validation

![](./images/k_fold_pic.png)

### Cross-Validation in `sklearn`

```{python}
from sklearn.model_selection import cross_val_score

cross_val_score(pipeline, X = df_known, y = df_known['price'], cv = 10)
```

* `sklearn` chooses a **default metric** for you based on the model.

* In this case: *r-squared* for Linear Regression; *negative root mean square error* for KNN.

* (Why negative?  So that we want to *maximize* this score)

### Cross-Validation in `sklearn`

* What if you want **MSE**?

```{python}
cv_scores = cross_val_score(pipeline, X = df_known, y = df_known['price'], cv = 10, scoring = "neg_mean_squared_error")
cv_scores
```

### Cross-Validation: FAQ{.smaller}

* **Q:** How many cross validations should we do?

* **A:** It doesn't matter much!  Typical choices are 5 or 10.  

* **A:** Think about the trade-offs:  larger *training sets* = *more accurate models* but smaller *test sets* = *more uncertainty in evaluation*

* **Q:** What metric should we use?

* **A:** This is your choice! What captures your idea of "successful prediction"?  (But MSE/RMSE is a good default.)

* **Q:** I took statistics before, and I remember some things like "adjusted R-Squared" or "AIC" for model selection.  What about those?

* **A:** Those are Old School, from a time when computers were not powerful enough to do cross-val.  Modern data science uses resampling.



## Activity

### Your turn

1. Use **cross-validation** to choose between Linear Regression and KNN with k = 7, for:

    + Using all predictors.
    + Using just winter rainfall and summer temperature.
    + Using only age.
    
2. Re-run #1, but instead use **mean absolute error**. (You will need to look in the documentation of `cross_val_score` for this!)

## Tuning with `GridSearchCV`

### Tuning

* In previous classes, we tried many different values of $k$ for KNN.

* We also mentioned using **absolute distance** instead of **euclidean distance**.

* Now, we would like to use **cross-validation** to decide between these options.

* `sklearn` provides a nice shortcut for this!

### `GridSearchCV`

```{python}
ct = make_column_transformer(
  (StandardScaler(), ['summer', 'har', 'sep', 'win', 'age']),
  remainder = "drop"
)

pipeline = make_pipeline(
    ct,
    KNeighborsRegressor())
    
from sklearn.model_selection import GridSearchCV

grid_cv = GridSearchCV(
    pipeline,
    param_grid={
        "kneighborsregressor__n_neighbors": range(1, 7),
        "kneighborsregressor__metric": ["euclidean", "manhattan"],
    },
    scoring="neg_mean_squared_error", cv=5)

grid_cv.fit(df_known, df_known['price'])
```

* **How many times did a model get `.fit()` to some data?**


### `GridSearchCV`

```{python}
pd.DataFrame(grid_cv.cv_results_)
```

### `GridSearchCV`

```{python}
pd.DataFrame(grid_cv.cv_results_)[['param_kneighborsregressor__metric', 'param_kneighborsregressor__n_neighbors', 'mean_test_score']]
```

### `GridSearchCV`

```{python}
grid_cv.best_params_
```



### Model evaluation

You have now encountered **three types of decisions** for finding your best model:

1. Which *predictors* should we include, and how should we preprocess them?  (**feature selection**)

2. Should we use *Linear Regression* or *KNN* or something else?  (**Model selection**)

3. Which value of $k$ should we use?  (**hyperparameter tuning**)


### Model evaluation

Think of this like a college sports bracket:

* Gather all your **candidate pipelines** (combinations of *column transformers* and *model specifications*)

* **Tune** each pipeline with cross-validation (regional championships!)

* Determine the **best model type** for each **feature set** (state championships!)

* Determine the **best pipeline** (national championships!)


### Challenge!{.larger}

Of all these options, what is the **number one best model** for wine price prediction, in your opinion?




::: lectitle
# Classification with KNN

[Slides](slides-11-classification.html)
:::

---
title: "Classification"
format:
  revealjs: 
    theme: [default, spring.scss]
    incremental: true 
    output-file: slides-11-classification.html
    shift-heading-level-by: -1
execute:
  echo: true
---

```{python}

```

```{r}
#| include: false
library(reticulate)
use_python("/usr/local/bin/python3")
```

```{python}
#| include: false
import numpy as np
import pandas as pd
from plotnine import *
```

## The story so far

### Choosing a Best Model

* We select a **best model** - aka best *prediction procedure* - by **cross-validation**.

* **Feature selection:** Which *predictors* should we include, and how should we preprocess them? 

* **Model selection:** Should we use *Linear Regression* or *KNN* or *Decision Trees* or something else? 

* **Hyperparameter tuning:** Choosing model-specific settings, like $k$ for KNN.

* Each candidate is a **pipeline**; use `GridSearchCV` or `cross_val_score` to score the options

## Case Study: Breast Tissue Classification

### Breast Tissue Classification

Electrical signals can be used to detect whether tissue is cancerous.

::: center
![image](./images/breast_diagram.jpeg){width=".4\\textwidth"}
:::

The goal is to determine whether a sample of breast tissue is:

::::{.columns}
:::{.column}
**Not cancerous**
1. connective tissue
2. adipose tissue
3. glandular tissue
:::

:::{.column}
**Cancerous**
4. carcinoma
5. fibro-adenoma
6. mastopathy
:::



### Reading in the Data

We will focus on two features:

-   $I_0$: impedivity at 0 kHz,
-   $PA_{500}$: phase angle at 500 kHz.

``` {python}
import pandas as pd
df = pd.read_csv("https://datasci112.stanford.edu/data/BreastTissue.csv")
df
```


### Visualizing the Data

```{python}
#| code-fold: true
from plotnine import *


(ggplot(df, aes(x = "I0", y = "PA500", fill = "Class"))
+ geom_point())
```

## K-Nearest Neighbors Classification

### K-Nearest Neighbors

What would we predict for someone with an $I_0$ of 400 and a $PA_{500}$ of 0.18?

``` {python}
X_train = df[["I0", "PA500"]]
y_train = df["Class"]

X_unknown = pd.DataFrame({"I0": [400], "PA500": [.18]})
X_unknown
```

### K-Nearest Neighbors

```{python}
#| code-fold: true
from plotnine import *

(ggplot()
+ geom_point(df, aes(x = "I0", y = "PA500", fill = "Class"))
+ geom_point(X_unknown, aes(x = "I0", y = "PA500"), size = 4)
)
```


### K-Nearest Neighbors

This process is *almost* identical to KNN *Regression*:

``` {python}
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(
    StandardScaler(),
    KNeighborsClassifier(n_neighbors=5, metric="euclidean"))

pipeline = pipeline.fit(X_train, y_train)
pipeline.predict(X_unknown)
```

### Probabilities

Which of these two unknown points would we be **more sure** about in our guess?

```{python}
#| code-fold: true
from plotnine import *

X_unknown = pd.DataFrame({"I0": [400, 2200], "PA500": [.18, 0.05]})

(ggplot()
+ geom_point(df, aes(x = "I0", y = "PA500", fill = "Class"))
+ geom_point(X_unknown, aes(x = "I0", y = "PA500"), size = 4)
)
```


### Probabilities

Instead of returning a single predicted class, we can ask it to return
the predicted probabilities for each class.

``` {python}
pipeline.predict_proba(X_unknown)
```

``` {python}
pipeline.classes_
```

How did Scikit-Learn calculate these predicted probabilities?


### Cross-Validation for Classification

We need a different **scoring method** for classification. A simple one is
**accuracy**:

$$\text{accuracy} = \frac{\text{# correct predictions}}{\text{# predictions}}.$$

### Cross-Validation for Classification

``` {python}
from sklearn.model_selection import cross_val_score

scores = cross_val_score(
    pipeline, X_train, y_train,
    scoring="accuracy",
    cv=10)
    
scores
```

    array([0.63636364, 0.81818182, 0.45454545, 0.54545455, 0.63636364,
           0.54545455, 0.5       , 0.6       , 0.4       , 0.7       ])

### Cross-Validation for Classification

As before, we can get an overall estimate of test accuracy by averaging
the cross-validation accuracies:

``` {.python bgcolor="gray"}
scores.mean()
```

But!  Accuracy is not always the best measure of a classification model!

### Confusion matrix

```{python}
from sklearn.metrics import confusion_matrix
pipeline = pipeline.fit(X_train, y_train)
y_train_predicted = pipeline.predict(X_train)
confusion_matrix(y_train, y_train_predicted)
```
### Confusion matrix

```{python}
pd.DataFrame(confusion_matrix(y_train, y_train_predicted), columns = pipeline.classes_, index = pipeline.classes_)
```
## Activity

### Activity

Use a **grid search** and the **accuracy** score to find the *best* k-value for this modeling problem.

## Classification Metrics

### Case Study: Credit Card Fraud

Data set of credit card transactions from Vesta.

Goal: Predict `isFraud`, where 1 indicates a fraudulent transaction.

``` {python}
df_fraud = pd.read_csv("https://datasci112.stanford.edu/data/fraud.csv")
df_fraud
```


### Classification Model

We can use $k$-nearest neighbors for classification:

``` {python}
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import make_pipeline
from sklearn.compose import make_column_transformer

pipeline = make_pipeline(
    make_column_transformer(
        (OneHotEncoder(handle_unknown="ignore", sparse_output=False),
         ["card4", "card6", "P_emaildomain"]),
        remainder="passthrough"),
    StandardScaler(),
    KNeighborsClassifier(n_neighbors=5))
```

### Training a Classifier

``` {python}
X_train = df_fraud.drop("isFraud", axis="columns")
y_train = df_fraud["isFraud"]
```

```{python}
from sklearn.model_selection import cross_val_score

cross_val_score(
    pipeline,
    X=X_train, y=y_train,
    scoring="accuracy",
    cv=10
).mean()
```


How is the accuracy so high?

### A Closer Look

Let's take a closer look at the labels.

``` {python}
y_train.value_counts()
```

The vast majority of transactions are normal (0)!


### Imbalanced data

If we just predicted that every transaction is normal, the accuracy
would be $1 - \frac{2119}{59054} = .964$.

Even though such predictions would be accurate *overall*, it is
inaccurate for fraudulent transactions. A good model is "accurate for
every class".


## Precision and Recall

* We need a score that measures "accuracy for class $c$".

* There are at least two reasonable definitions:

-   **precision**: $P(\text{correct} | \text{predicted class } c)$

    Among the observations that were predicted to be in class $c$, what
    proportion actually were?

-   **recall**: $P(\text{correct} | \text{actual class} c)$.

    Among the observations that were actually in class $c$, what
    proportion were predicted to be?


### Precision and Recall by Hand

To check our understanding of these definitions, let's calculate a few
precisions and recalls by hand.

First, summarize the results by the **confusion matrix**.

```{python}
#| code-fold: true
from sklearn.metrics import confusion_matrix
pipeline.fit(X_train, y_train)
y_train_ = pipeline.predict(X_train)
confusion_matrix(y_train, y_train_)
```

-   What is the (training) accuracy?

-   What's the precision for normal transactions?

-   What's the recall for normal transactions?

-   What's the precision for fraudulent transactions?

-   What's the recall for fraudulent transactions?

### Tradeoff between Precision and Recall

Can you imagine a classifier that always has 100% recall for class $c$,
no matter the data?

In general, if the model classifies more observations as $c$,

-   recall (for class $c$) $\uparrow$

-   precision (for class $c$) $\downarrow$

How do we compare two classifiers, if one has higher precision and the
other has higher recall?

### F1 Score

The **F1 score** combines precision and recall into a single score:

$$\text{F1 score} = \text{harmonic mean of precision and recall}$$
$$= \frac{2} {\left( \frac{1}{\text{precision}} + \frac{1}{\text{recall}}\right)}$$

* To achieve a high F1 score, both precision and recall have to be high.

* If either is low, then the harmonic mean will be low.

### Estimating Test Precision, Recall, and F1

* Remember that each class has its own precision, recall, and F1.

* But Scikit-Learn requires that the `scoring=` parameter be a
single number.

* For this, we can average the score over the classes:
    + `"precision_macro"`
    + `"recall_macro"`
    + `"f1_macro"`



### F1 Score

``` {python}
cross_val_score(
    pipeline,
    X=X_train, y=y_train,
    scoring="f1_macro",
    cv=10
).mean()
```


### Precision-Recall Curve

Another way to illustrate the tradeoff between precision and recall is
to graph the **precision-recall curve**.

First, we need the predicted probabilities.

``` {python}
y_train_probs_ = pipeline.predict_proba(X_train)
y_train_probs_
```


### Precision-Recall Curve

* By default, Scikit-Learn classifies a transaction as fraud if this
probability is $> 0.5$.

* What if we instead used a threshold $t$ other than $0.5$?

* Depending on which $t$ we pick, we'll get a different precision and
recall. We can graph this tradeoff.


### Precision-Recall Curve

Let's graph the precision-recall curve together in a Colab.

[Link](https://colab.research.google.com/drive/1w-4iGAiL3iXqAl3332teOT0gXv1wNxio?usp=sharing)


## Takeaways

### Takeaways

* We can do **KNN for Classification** by letting the nearest neighbors "vote"

* The number of votes is a "probability"

* A **classification model** must be evaluated differently than a **regression model**.

* One possible metric is **accuracy**, but this is a bad choice in situations with **imbalanced data**.

* **Precision** measures "if we say it's in Class A, is it really?"

* **Recall** measures "if it's really in Class A, did we find it?"

* **F1 Score** is a balance of precision and recall

* **Macro F1 Score** averages the F1 scores of all classes





::: lectitle
# Logistic Regression

[Slides](slides-12-logistic.html)
:::

---
title: "Logistic Regression"
format:
  revealjs: 
    theme: [default, spring.scss]
    incremental: true 
    output-file: slides-12-logistic.html
    shift-heading-level-by: -1
execute:
  echo: true
---

```{python}

```

```{r}
#| include: false
library(reticulate)
use_python("/usr/local/bin/python3")
```

```{python}
#| include: false
import numpy as np
import pandas as pd
from plotnine import *
```

## The story so far

### Classification

* We can do **KNN for Classification** by letting the nearest neighbors "vote"

* The number of votes is a "probability"

* A **classification model** must be evaluated differently than a **regression model**.

* One possible metric is **accuracy**, but this is a bad choice in situations with **imbalanced data**.

* **Precision** measures "if we say it's in Class A, is it really?"

* **Recall** measures "if it's really in Class A, did we find it?"

* **F1 Score** is a balance of precision and recall

* **Macro F1 Score** averages the F1 scores of all classes

## Revisiting the Breast Cancer Data


### Breast Tissue Classification

Electrical signals can be used to detect whether tissue is cancerous.

::: center
![image](./images/breast_diagram.jpeg){width=".4\\textwidth"}
:::

The goal is to determine whether a sample of breast tissue is:

::::{.columns}
:::{.column}
**Not cancerous**
1. connective tissue
2. adipose tissue
3. glandular tissue
:::

:::{.column}
**Cancerous**
4. carcinoma
5. fibro-adenoma
6. mastopathy
:::

### Binary response: Cancer or not

Let's read the data, and also make a new variable called "Cancerous".

``` {python}
import pandas as pd
df = pd.read_csv("https://datasci112.stanford.edu/data/BreastTissue.csv")

df['Cancerous'] = (df['Class'] == "car") | (df['Class'] == "fad") | (df['Class'] == "mas")

df.head()
```
## Why not use "regular" regression?

**You should NOT use ordinary regression for a classification problem!  This slide section is to show you why it does NOT work.**

### Counter-Example: Linear Regression

We know that in computers, `True` = `1` and `False` = `0`.  So, why not convert our response variable, `Cancerous`, to numbers and fit a regression?

```{python}
df['Cancerous'] = df['Cancerous'].astype('int')
df.head()
```
### Example: OLS Regression

```{python}
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(
  LinearRegression())

pipeline = pipeline.fit(df[["I0", "PA500"]], df['Cancerous'])
```

### Counter-Example: Linear Regression

**Problem 1:** How do we predict *categories*???

```{python}
pred_cancer = pipeline.predict(df[["I0", "PA500"]])
pred_cancer
```


### Counter-Example: Linear Regression

**Problem 2:** Was the relationship really *linear*???

```{python}
#| code-fold: true

from plotnine import *
(ggplot(df, aes(x = "I0", y = "Cancerous")) 
+ geom_point()
+ geom_smooth(method = "lm", se = False)
+ theme_classic())
```


### Counter-Example: Linear Regression

**Problem 3:** Was the error really *random*???

```{python}
#| code-fold: true
residuals = df['Cancerous'] - pred_cancer

(ggplot(df, aes(x = "I0", y = residuals))
+ geom_point())
```

## Logistic Regression

### Logistic Regression

* **Idea:** Instead of predicting 0 or 1, try to predict the *probability* of cancer.

* **Problem:** We don't observe probabilities before diagnosis; we only know if that person ended up with cancer or not.

* **Solution:** (Fancy statistics and math.)

* Why is it called **Logistic Regression**?

* Because the "fancy math" uses a *logistic function* in it.

### Logistic Regression{.smaller}

What you need to know:

* It's used for **binary classification** problems.

* The predicted **values** are the "log-odds" of having cancer, i.e.

$$\text{log-odds} = \log \left(\frac{p}{1-p}\right)$$

* We are more interested in the **predicted probabilities**.

* As with KNN, we predict **categories** by choosing a **threshold**

* By default:  Probability > 0.5 -> Predict cancerous.

### Logistic Regression in `sklearn`

```{python}
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(
  LogisticRegression(penalty=None))

pipeline.fit(df[["I0", "PA500"]], df['Cancerous'])
```

### Logistic Regression in `sklearn`

```{python}
pred_cancer = pipeline.predict(df[["I0", "PA500"]])
pred_cancer
```

## Precison and Recall Revisited

### Confusion matrix{.smaller}

```{python}
from sklearn.metrics import confusion_matrix
pd.DataFrame(confusion_matrix(df['Cancerous'], pred_cancer), columns = pipeline.classes_, index = pipeline.classes_)
```

* Calculate the *precision* for predicting cancer.

* Calculate the *recall* for predicting cancer.

* Calculate the *precision* for predicting non-cancer.

* Calculate the *recall* for predicting non-cancer.

### Threshold

What if we had used different cutoffs besides $p > 0.5$?

```{python}
prob_cancer = pipeline.predict_proba(df[["I0", "PA500"]])
prob_cancer.round(2)[1:5]
```

### Higher Threshold

What we used $p > 0.7$?

```{python}
prob_cancer = pipeline.predict_proba(df[["I0", "PA500"]])
pred_cancer_70 = prob_cancer[:,1] > .7
pred_cancer_70[1:5]
```


### Higher Threshold

What we used $p > 0.7$?

```{python}
conf_mat = confusion_matrix(df['Cancerous'], pred_cancer_70)
pd.DataFrame(conf_mat, columns = pipeline.classes_, index = pipeline.classes_)
```

```{python}
precision_1 = conf_mat[1,1]/conf_mat[:,1].sum()
precision_1

recall_1 = conf_mat[1,1]/conf_mat[1, :].sum()
recall_1
```


### Lower Threshold

What we used $p > 0.2$?

```{python}
prob_cancer = pipeline.predict_proba(df[["I0", "PA500"]])
pred_cancer_20 = prob_cancer[:,1] > .2
pred_cancer_20[1:5]
```

### Lower Threshold

```{python}
conf_mat = confusion_matrix(df['Cancerous'], pred_cancer_20)
pd.DataFrame(conf_mat, columns = pipeline.classes_, index = pipeline.classes_)
```

```{python}
precision_1 = conf_mat[1,1]/conf_mat[:,1].sum()
precision_1

recall_1 = conf_mat[1,1]/conf_mat[1, :].sum()
recall_1
```

### Precision-Recall Curve

```{python}
from sklearn.metrics import precision_recall_curve

precision, recall, thresholds = precision_recall_curve(
    df['Cancerous'], prob_cancer[:, 1])
    
df_pr = pd.DataFrame({
  "precision": precision,
  "recall": recall
})

```


### Precision-Recall Curve

```{python}

(ggplot(df_pr, aes(x = "recall", y = "precision"))
+ geom_point())
```


## Your turn

### Activity{.smaller}

Suppose you want to predict Cancer vs. No Cancer from breast tissue using a Logistic Regression. Should you use...

* Just `I0` and `PA500`?

* Just `DA` and `DP`?

* `I0`, `PA500`, `DA`, and `DP`?

* or all predictors?


Use **cross-validation** with **F1 Score** to decide!  

Then, fit your **final model** and report the **confusion matrix**.


*Completely optional - you will not be tested on this:  change `penalty = None` to `penalty = 'l1'` or `penalty = 'l2'` and tune the `alpha` parameter for your Logistic Regression models*

## Interpreting Logistic Regression

### Looking at coefficients

```{python}
pd.DataFrame({
  "Coefficients":pipeline['logisticregression'].coef_[0],
  "Column": ["I0", "PA500"]
  })
```


* "For every unit of I0 higher, we predict 0.003 **lower** log-odds of cancer"

* "For every unit of PA500 higher, we predict 11.73 **higher** log-odds of cancer."

### Feature Importance

* Does this mean that `PA500` is *more important* than `I0`?

::::{.columns}
:::{.column width="50%"}

```{python}
#| code-fold: true
(ggplot(df, aes(x = "PA500", group = "Cancerous", fill = "Cancerous"))
+ geom_density(alpha = 0.5, show_legend=False))
```

:::

:::{.column width="50%"}

```{python}
#| code-fold: true
(ggplot(df, aes(x = "I0", group = "Cancerous", fill = "Cancerous"))
+ geom_density(alpha = 0.5, show_legend=False))
```

:::

::::

### Standardization

* Does this mean that `PA500` is *more important* than `I0`?

* **Not necessarily.**  They have different *units* and so the coefficients mean different things.

* "For every **1000** units of I0 higher, we predict **3.0 lower** log-odds of cancer"

* "For every **0.1** unit of PA500 higher, we predict **1.1 higher** log-odds of cancer."

* What if we had *standardized* `I0` and `PA500`?

### Standardization{.smaller}

```{python}
from sklearn.preprocessing import StandardScaler

pipeline2 = make_pipeline(
  StandardScaler(),
  LogisticRegression(penalty=None)
  )

pipeline2 = pipeline2.fit(df[["I0", "PA500"]], df['Cancerous'])

pd.DataFrame({
  "Coefficients":pipeline2['logisticregression'].coef_[0],
  "Column": ["I0", "PA500"]
  })

```


* "For every **standard deviation above the mean** someone's `I0` is, we predict **2.3 lower** log-odds of cancer"

* "For every **standard deviation above the mean** someone's `PA500` is, we predict **0.80 higher** log-odds of cancer."

* Therefore, `I0` is more important!

### Standardization: Do you need it?

But - did this approach change our predictions at all?

```{python}
old_probs = pipeline.predict_proba(df[["I0", "PA500"]])
new_probs = pipeline2.predict_proba(df[["I0", "PA500"]])

pd.DataFrame({
  "without_stdize": old_probs[:,1], 
  "with_stdize": new_probs[:,1]
  }).head()
```

### Standardization: Do you need it?{.smaller}

* Standardizing will **not change the predictions** for Linear or Logistic Regression!

* This is because the **coefficients** are chosen relative to the **units** of the predictors.  (Unlike in KNN!)

* Advantage of *not* standardizing:  More interpretable coefficients

* "For each unit of... " instead of "For each sd above the mean..."

* Advantage of *standardizing*:  Compare relative importance of predictors

* **It's up to you!**

* *(Don't use cross-validation to decide - you'll get the same metrics for both!)*

## Your turn

### Activity

For your Logistic Regression using **all** predictors, which two were most important?

How would you interpret the coefficients?

```{python}
#| include: false
#| eval: false

from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline

pipeline3 = make_pipeline(
  StandardScaler(),
  LogisticRegression(penalty=None)
  )

pipeline3 = pipeline3.fit(df[["I0", "PA500", 'HFS', 'DA', 'Area', 'A/DA', 'Max IP',
       'DR', 'P']], df['Cancerous'])

pd.DataFrame({
  "Coefficients":pipeline3['logisticregression'].coef_[0],
  "Column": ["I0", "PA500", 'HFS', 'DA', 'Area', 'A/DA', 'Max IP',
       'DR', 'P']
  })
```



## Takeaways

### Takeaways

* To fit a **regression model** (i.e., coefficients times predictors) to a **categorical response**, we use **Logistic Regression**.

* Coefficients are interpreted as "One unit increase in predictor is associated with a [something] increase in the log-odds of Category 1"

* We still use **cross-validated metrics** to decide between KNN and Logistic regression, and between different feature sets.

* We still report **confusion matrices** and sometimes **precision-recall curves** of our final model.



::: lectitle
# Clustering with K-Means

[Slides](slides-13-kmeans.html)
:::

---
title: "Unsupervised Learning with K-Means"
format:
  revealjs: 
    theme: [default, spring.scss]
    incremental: true 
    output-file: slides-13-kmeans.html
    shift-heading-level-by: -1
execute:
  echo: true
---

```{python}

```

```{r}
#| include: false
library(reticulate)
use_python("/usr/local/bin/python3")
```

```{python}
#| include: false
import numpy as np
import pandas as pd
from plotnine import *
```

## The story so far

### Predictive modeling

* In **predictive modeling**, a.k.a. **supervised machine learning**, we have a **target variable** we want to predict.

* We expect to have observations where we know the *predictors* but not the *target*.

* Our goal is to choose a **modeling procedure** to guess targets from predictors.

* We use **cross-validation** to estimate the **test error** of various procedure options.

* We might compare different:
    + feature sets
    + preprocessing choices
    + model specifications/algorithms
    + tuning parameters
    
    
## Unsupervised Learning

### Unsupervised Learning

* In **unsupervised** situations, we do not have a target variable $y$

* We *do* still have **features** that we observe.

* Our goal: Automatically find *interesting structure* in the features.

* (Think of children playing with Legos.  They might be *supervised* by parents who help them follow instructions, or they might be left alone to build whatever they want!)

### Clustering

* Nearly all unsupervised learning algorithms can be called **clustering**.

* The goal is to use the **observed features** (columns) to sort the **observations** (rows) into similar **clusters** (groups).

* For example: Suppose I take all of your grades in the gradebook as *features* and then use these to find *clusters* of students.  These clusters might represent...
    + people who studied together
    + people who are in the same section
    + people who have the same major or background
    + ... or none of the above!
    
    
### Uses of clustering

*   **Ecology:** An ecologist wants to group organisms into types to define different species.  *(rows = organisms; features = habitat, size, etc.)*

* **Biology:** A geneticist wants to know which groups of genes tend to be activated at the same time. *(rows = genes; features = activation at certain times)*

*   **Market Segmentation:** A business wants to group their customers
    into types.  *(rows = customers, features = age, location, etc.)*

*   **Language:** A linguist might want to identify different uses of
    ambiguous words like "set" or "run". *(rows = words; features = other words they are used with)*
    
*  **Documents:** A historian might want to find groups of articles that are on similar topics.  *(rows = articles; features = tf-idf transformed n-grams)*
    
    
    
## K-Means

### The K-Means Algorithm

* **Idea:** Two observations are *similar* if they are *close* in distance.

* (Just like with K-Nearest-Neighbors!)

* We should look for groups of observations that are close to the same **centroid**.


### The K-Means Algorithm

[Demo](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/)

Procedure (3-means):

1. Choose 3 random observations to be the **initial centroids**.

2. For each observation, determine which is the **closest centroid**.

3. Create 3 **clusters** by closest centroid.

4. Find the **new centroid** of each cluster.

5. Repeat until the clusters don't change.


### Example: Penguins data

```{python}
import numpy as np
import pandas as pd

data_dir = "https://dlsun.github.io/stats112/data/"
df_penguins = pd.read_csv(data_dir + "penguins.csv").dropna()
df_penguins
```

### Example: Penguins data

```{python}
#| code-fold: true
from plotnine import *
(ggplot(df_penguins, aes(x = "bill_length_mm", y = "flipper_length_mm"))
+ geom_point())
```

### Step 0: Standardize the data

**Why is this important?**

```{python}
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler().set_output(transform = "pandas")
df_scaled = scaler.fit_transform(df_penguins[["bill_length_mm", "flipper_length_mm"]]).dropna()

df_scaled.head()
```

### Step 1: Choose 3 random points to be centroids

```{python}
centroids = df_scaled.sample(3, random_state=1234)
centroids.index = ["orange", "purple", "green"]

centroids
```

### Step 1: Choose 3 random points to be centroids

```{python}
#| code-fold: true
from plotnine import *
(ggplot(df_scaled, aes(x = "bill_length_mm", y = "flipper_length_mm"))
+ geom_point()
+ geom_point(centroids, color = centroids.index, size = 3)
+ theme_classic())
```

### Step 2:  Assign each point to nearest centroid

```{python}
from sklearn.metrics import pairwise_distances

dists = pairwise_distances(df_scaled, centroids)
dists[1:5]
```

### Step 2:  Assign each point to nearest centroid

```{python}
closest_centroid = dists.argsort()[:,0]
closest_centroid
```

### Step 2:  Assign each point to nearest centroid

```{python}
df_scaled.index = centroids.index[closest_centroid]
df_scaled.head()
```

### Step 2:  Assign each point to nearest centroid

```{python}
#| code-fold: true
from plotnine import *
(ggplot(df_scaled, aes(x = "bill_length_mm", y = "flipper_length_mm"))
+ geom_point(color = df_scaled.index)
+ geom_point(centroids, color = centroids.index, size = 3)
+ theme_classic())
```

### Step 3:  Find new centroids

```{python}
centroids = df_scaled.groupby(df_scaled.index).mean()
centroids
```

### Step 3:  Find new centroids

```{python}
#| code-fold: true
from plotnine import *
(ggplot(df_scaled, aes(x = "bill_length_mm", y = "flipper_length_mm"))
+ geom_point(color = df_scaled.index)
+ geom_point(centroids, color = centroids.index, size = 3)
+ theme_classic())
```

### Step 4:  Repeat over and over!

```{python}
for i in range(1,3):
  dists = pairwise_distances(df_scaled, centroids)
  df_scaled.index = centroids.index[closest_centroid]
  centroids = df_scaled.groupby(df_scaled.index).mean()
  print(centroids)
```

### K-means in sklearn

```{python}
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

model = KMeans(n_clusters=3, random_state=1234)

pipeline = make_pipeline(
    StandardScaler(),
    model
)

pipeline.fit(df_penguins[["bill_length_mm", "flipper_length_mm"]].dropna())
```

### K-means in sklearn

```{python}
centroids = model.cluster_centers_
clusters = model.labels_

clusters
```
### Interpreting K-Means

The key takeaway here is the **cluster centers**:

```{python}
centroids
```

* Cluster 1 has long bill, shortish flipper

* Cluster 2 has longish bill, long flipper

* Cluster 3 has short bill, short flipper

### Interpreting K-Means

We also might check if these clusters match any labels that we already know:

```{python}
results = pd.DataFrame({
  "cluster": clusters,
  "species": df_penguins['species']
})

results
```

### Interpreting K-Means

We also might check if these clusters match any labels that we already know:

```{python}
results.value_counts(["cluster", "species"]).unstack()
```
## Your Turn

### Activity

* Fit a 3-means model using *all* the numeric predictors in the penguins data.

* Describe each cluster

* Do these clusters match up to the species?

* Then fit a 6-means model

* Do those clusters match up to species and island?


## Takeaways

### Takeaways

* **unsupervised learning** is a way to find *structure* in data

* **k-means** is the most common **clustering method**

* We **have to choose K ahead of time**

* (This is a big problem!  Why can't we tune?)




::: lectitle
# Merging Data

[Slides](slides-14-merge.html)
:::

---
title: "Combining Datasets"
format:
  revealjs: 
    theme: [default, spring.scss]
    incremental: true 
    output-file: slides-14-merge.html
    shift-heading-level-by: -1
execute:
  echo: true
---

```{python}

```

```{r}
#| include: false
library(reticulate)
use_python("/usr/local/bin/python3")
```

```{python}
#| include: false
import numpy as np
import pandas as pd
from plotnine import *
```

## The story so far

### Data analysis: the whole game

* **Acquire** data and **clean** it by fixing variable types, dropping or replacing missing data, and looking for other issues.

* **Explore** the dataset by making summaries and plots of *one variable*.

* Establish **research questions** to answer with this data.

* Create **visualizations** of *two or more variables* that address simple questions.

* Fit **predictive models** to address more complex questions, and/or to prepare for prediction on future data.

* Fit **unsupervised models** to answer open-ended questions.

## Joining on a Key

### Example: Planes and flights

* Sometimes, information is spread across multiple data sets.

* For example, suppose we want to know which manufacturer's planes made the most flights in November 2013.

* One data set contains information about flights in Nov. 2013...

``` {python}
import pandas as pd
data_dir = "https://datasci112.stanford.edu/data/nycflights13/"
df_flights = pd.read_csv(f"{data_dir}/flights11.csv")
df_flights.head()
```


### Example: Planes and Flights

...while another contains information about planes.

``` {python}
df_planes = pd.read_csv(f"{data_dir}/planes.csv")
df_planes.head()
```

In order to answer the question of which manufacturer made the most
flights, we have to join these two data sets together.

### Keys

* A **primary key** is a column (or a set of columns) that uniquely
identifies observations in a data frame.

* The **primary key** is the column(s) you would think of as the **index**.

* A **foreign key** is a column (or a set of columns) that points to the primary key of another data frame.

* Planes are uniquely identified by their *tail number*
(`tailnum`).

### Joining on a Key

Each value of the **primary key** should only appear once, but it could appear many times in a **foreign key**.

```{python}
df_flights['tailnum'].value_counts().head()
df_planes['tailnum'].value_counts().head()
```


### Joining on a Key

The Pandas function `.merge()` can be used to join two
`DataFrame`s on a key.

``` {python}
df_joined = df_flights.merge(df_planes, on="tailnum")
df_joined.head()   
```



### Overlapping Column Names


* Joining two data frames results in a *wider* data frame, with more columns.

* By default, Pandas adds the suffixes `_x` and `_y` to overlapping column names, but this can be customized.

``` {python}
df_joined = df_flights.merge(df_planes, on="tailnum",
                             suffixes=("_flight", "_plane"))
df_joined.columns
```


### Analyzing the Joined Data

Now that we have joined the two data sets, we can answer the question: which manufacturer's planes made the most flights?

``` {python}
df_joined["manufacturer"].value_counts()
```

## Joining on Multiple Keys

### Example: Weather and Flights

* Research question: What weather factors cause flight delays?

* Here is a data set containing hourly weather data at each airport
in 2013.

``` {python}
df_weather = pd.read_csv(f"{data_dir}/weather.csv")
df_weather.head()
```

**What is the primary key of this data set?**


### A Key with Multiple Columns

Let's start by looking at flights out of JFK only, for simplicity.

We need to join to the weather data on year, month, day, and hour.

``` {python}
df_flights_jfk = df_flights[df_flights["origin"] == "JFK"]
df_weather_jfk = df_weather[df_weather["airport"] == "JFK"]

df_jfk = df_flights_jfk.merge(df_weather_jfk, on=("year","month","day","hour"))

df_jfk.head()
```


### Let's see how rain affects departure delays.

``` {python}
#| code-fold: true
from plotnine import *

(ggplot(df_jfk, aes(x="precip", y="dep_delay")) +
geom_point(alpha=0.2) +
theme_classic())
```

### Joining on Keys with Different Names

* Sometimes, the join keys have different names in the two data sets. 

* This frequently happens if the data sets come from different sources.

* For example, if we want to join the (entire) flights data to the weather data, we would need to include the *airport* in the key.

* But the airport is called `origin` in `df_flights`
and `airport` in `df_weather`.

* The `.merge()` function provides `left_on=` and
`right_on=` arguments for specifying different column names in
the **left** (first) and **right** (second) data frames.

### Joining on Keys with Different Names

``` {python}
df_flights_weather = df_flights.merge(
    df_weather,
    left_on=("origin", "year", "month", "day", "hour"),
    right_on=("airport", "year", "month", "day", "hour"))
```


### Rain and delays by airport

Now we can visualize how rain impacts delays at each airport:

```{python}
#| code-fold: true
df_rain = df_flights_weather.groupby(["airport", "precip"])["dep_delay"].mean().to_frame().reset_index()

(ggplot(df_rain, aes(x='precip', y='dep_delay', color = 'airport')) +
geom_point() +
geom_line() +
theme_classic())
```


## Joins with missing keys

### Example: Baby names

The data below contains counts of names for babies born in 1920 and 2020:

``` {python}
import pandas as pd
data_dir = "http://dlsun.github.io/pods/data/names/"

df_1920 = pd.read_csv(data_dir + "yob1920.txt", header=None,
                      names=["Name", "Sex", "Count"])
df_2020 = pd.read_csv(data_dir + "yob2020.txt", header=None,
                      names=["Name", "Sex", "Count"])
                      
df_1920.head()
df_2020.head()
```

### Joins

We can merge these two datasets on a primary key...

```{python}
df_joined = df_1920.merge(df_2020, on=["Name", "Sex"], suffixes=("_1920", "_2020"))
df_joined.head()
```



### Missing Keys?

... but what happened to some of the names?

``` {python}
df_joined[df_joined["Name"] == "Maya"]
```

### Missing keys

* Why isn't Maya in the joined data? 

It is there in the 2020 data...

```{python}
df_2020[df_2020["Name"] == "Maya"]
```


...but not in the 1920 data.

```{python}
df_1920[df_1920["Name"] == "Maya"]
```



### Missing keys


* How does the `merge()` function determine which keys get kept?

* By default, in order to appear in the joined data, a key must be present in *both* tables.


## Other Types of Joins


### Types of Joins

* How can we customize the behavior of joins for missing keys?

* By default, Pandas does an **inner join**, which only keeps keys that are present in *both* tables.

* An **outer join** keeps any key that is present in either table.

* A **left join** keeps all keys in the left table, even if they are not in the right table. But any keys that are only in the right table are dropped.

* A **right join** keeps all keys in the right table, even if they are not in the left table. But any keys that are only in the left table are dropped.

### Types of joins

We can customize the type of join using the `how=` parameter of
`.merge()`. By default, `how="inner"`.

``` {python}
df_joined_outer = df_1920.merge(df_2020, on=["Name", "Sex"],
                                how="outer")
df_joined_outer[df_joined_outer["Name"] == "Maya"]
```

### Types of Joins

* Note the missing values for other columns, like Count, for 1920!

* What other type of join would have produced this output in the Maya row?

### Types of Joins

* Note the missing values for other columns, like Count, for 1920!

* What other type of join would have produced this output in the Maya row?

``` {python}
df_joined_right = df_1920.merge(df_2020, on=["Name", "Sex"],
                                how="right")
df_joined_right[df_joined_right["Name"] == "Maya"]
```

## Types of Joins

![](./images/joins.png)

### Quick Quiz

Which type of join would be best suited for each case?

1. We want to determine the names that have increased in popularity the most between 1920 and 2020.

2.  We want to graph the popularity of names over time.

3.  We want to determine the names that have decreased in popularity the most between 1920 and 2020.

## Many-to-Many Joins

### Many-to-Many Relationships{.smaller}

* So far, the keys we've joined on have been the primary key of (at least) one table.

* If we join to the primary key of another table, then the relationship is **one-to-one** (since primary keys uniquely identify rows).

* If we join to the foreign key of another table, then the relationship is **one-to-many**.

* What if we join on a key that is not a primary key?

* That is, what if the key does not uniquely identify rows in either table so that each value of the key might appear multiple times?

* This relaionship is called **many-to-many**.

### Many-to-Many Example

What if we only joined on the name?

```{python}
df_1920.merge(df_2020, on="Name")
```

### Preventing Bugs

* Most of the time, many-to-many joins are a bug, caused by a
misunderstanding about the primary key.

* Pandas allows us to specify the relationship we are expecting. It will fail with an error if the relationship is a different kind.

* For example, suppose we thought that "name" was the primary key of the baby name tables.

```{python}
#| eval: false
df_1920.merge(df_2020, on="Name",
              validate="one_to_one")
```

```
MergeError: Merge keys are not unique in either left or right dataset; not a one-to-one merge
```

Errors are (sometimes) your friend. They can prevent you from making
even bigger mistakes!

## Filtering Joins

### Filtering Joins{.smaller}

* Inner, outer, left, and right are known as **mutating joins**, because they create new combined datasets.

* There are two other types of joins that we use for **filtering** to get rid of some rows:

* A **semi-join** tells us which keys in the *left* **are** present in the *right*.

* An **anti-join** tells us which keys in the left **are not** present in the right.

* In Pandas, we can't do these using `.merge()`

### Filtering joins

Which names existed in 1920 but don't in 2020?

```{python}
in_both = df_1920['Name'].isin(df_2020['Name'])
df_1920.loc[~in_both, 'Name']
```

### Your turn

* Did your name exist in 2020 but not 1920?

* If so, how has the popularity changed?

## Takeaways

### Takeaways

* A **primary key** is one or more columns that uniquely indentify the rows.

* We can **join** (a.k.a. **merge**) datasets if they share a primary key, or if one has a **foreign key**.

* The default of `.merge()` is an *inner join*: only keys in both datasets are kept.

* We can instead specify a *left join*, *right join*, or *outer join*; think about which rows we want to keep.

* **Filterting joins** like *anti-join* and *semi-join* can help you answer questions about the data.  

* Use `.isin()` to see which keys in one dataset exist in the other.





::: lectitle
# Hierarchical Data and APIs

[Slides](slides-15-hierarchical.html)
:::

---
title: "Hierarchical data"
format:
  revealjs: 
    theme: [default, spring.scss]
    incremental: true 
    output-file: slides-15-hierarchical.html
    shift-heading-level-by: -1
execute:
  echo: true
---

```{python}

```

```{r}
#| include: false
library(reticulate)
use_python("/usr/local/bin/python3")
```

```{python}
#| include: false
import numpy as np
import pandas as pd
from plotnine import *
```

## The story so far

### Tabular data and indices/keys

* Thus far, we have only dealt with **tabular data** that can be represented with **rows** and **columns**.

* It is important to determine what **each row represents** because...

    + You want to make sure you understand your data
    + You might want to set an **index**
    + You might want to find a **key** to merge on
    
* So far, we have read data in from `.csv` files or `.txt` files.

## JSON data

### JSON data

* The most common non-tabular data file type is called **JSON** (*JavaScript Object Notation*)

* Information is stored in *nested dictionaries*.

* This type of data is called **hierarchical**, because it has multiple levels

* It can be helpful to picture the data like a **tree**


* You can think of this a bit like folders on your computer

### Example

```
{"teaching":
  {"colleges": [
      {
        "name": "Stanford",
        "location": "Palo Alto",
        "classes": [
        {
          "name": "Principles of Data Science",
          "number": "DataSci 112",
          "enrollment": 90,
          "quarter": "Spring 2024",
          "students": [
            {
              "name": "Regina George",
              "id": 12345
            },
            {
              "name": "Cady Heron",
              "id": 38292
            },
            (... more students in 112)
          ]
        }, (end of 112 entry)
        {
          "name": "Introduction to Statistical Learning",
          "number": "Stat 216V",
          "enrollment": 40,
          "quarter": "Summer 2024",
          "students": [
            {
              "name": "Tommy Oliver",
              "id": 99999
            },
            {
              "name": "Trini Kwan",
              "id": 88888
            },
            (... more students in 216V)
          ]
        } end of 112 entry
        (... more classes)
        ]
      }, (end of Stanford info)
      {
        ... (Cal Poly info)
      }
    ], (end of "colleges" list)
    "high schools": [
      (... high school info)
    ]
  } (end of "teaching")
} (end of file)
```

### Quick quiz

* What would be the **primary key** at the *colleges* level?

* What would be the **primary key** at the *classes* level?

* What would be the **primary key** at the *students* level?

### JSON: Things to notice

* This looks exactly like a *dictionary* in python, so it is easy to read in as data.

* When we have *nested* structure, we have a *list* of *dictionaries*, e.g. `{"students": [{"name": "Regina George", ...}, {"name":"Cady Heron", ...}, ...]}`

* There can be *keys* with the same name at different *levels*

### Check out some real data

[Colab link](https://colab.research.google.com/drive/12LyAEhX4uoZWlumtLsmI3n354cR-2Boi?usp=sharing)

## XML

### XML

* Another type of hierarchical data is **XML** (*eXtensible markup language*)

* It is the same concept as **JSON**, but a bit less commonly used.

* Instead of *dictionaries*, data is recorded with *tags*.

### XML Example


```
<teaching>
  <colleges>
    <college>
      <name>Stanford</name>
      <location>Palo Alto</location>
      <classes>
        <class>
          <name>Principles of Data Science</name>
          <number>DataSci 112</number>
          <enrollment>90</enrollment>
           <quarter>Spring 2024</quarter>
           <students>
            <student>
               <name>Regina George</name>
               <id>12345</id>
            </student>
             <student>
               <name>Cady Heron</name>
              <id>38292</name>
            </student>
             (... more students in 112)
          </students>
        </class>
        (... more classes)
       </classes>
     </college>
     <college>
       ... (Cal Poly info)
     </college>
   </colleges>
   <highschools>
     (... high school info)
   </highschools>
</teaching>
```

### XML: things to notice

* It is easy to see where each *level* starts and ends

* When we have a *nested* data structure, we have plural/singular, e.g. `<students> <student> ... </student> <student> ... </student> </students>`

* This *tagging* structure looks kind of like **HTML**, which is how websites are constructed

### Check out some real data

[Colab link](https://colab.research.google.com/drive/12LyAEhX4uoZWlumtLsmI3n354cR-2Boi?usp=sharing)

### XML vs. JSON

* XML matches *website* code; JSON matches *python* (and other languages) code.

* XML makes it easy to see start and finish; JSON is more compact.

* There is no objective reason one is better than the other.

* Both are **just text files** made with specific structure!

## Relational Data

### Relational Data

* You will sometimes see *hierarchical data* stored as many **tabular datasets** in a **relational database**.

* (This is the main use of things like **SQL**)

* The important thing is **key matching**

* Example:
  + Dataset 1: List of colleges
  + Dataset 2: List of courses, and "college" column as *foreign key*
  + Dataset 3: List of students, and "course" column as *foreign key*
  
## APIS

### APIs{.smaller}

* The main reason we need to know how to handle *relational data* is because this is how data is commonly shared online.

* Some organizations/websites provide an **API** (*Application Programming Interface*) where you can "easily" access data.

* Typically, this data is shared as JSON or XML.

* APIs have multiple **endpoints**: subsections of the site where you can access particular *levels* or *categories* of data.

* [Example: Magic the Gathering](https://docs.magicthegathering.io/)

* The hard part of APIs is figuring out which URL to go to for a particular dataset.  Then you're just snagging a JSON or XML as text!

* (You'll practice this in Section.)

## Takeaways

### Takeaways{.smaller}

* Data might be stored in **hierarchical** form rather than **tabular** form.

* The structure of text files for this data is typically either **JSON** or **XML**.

* It is important to think about **levels** of the data (i.e. layers of the tree) and what their **keys** are

* We can use the `.json` and `json_normalize` functions to turn JSON levels in to tabular data

* We can use `.find` or `.findall` or `.parent` or `.child` to search/navigate the tree in XML data.  (Then we have to build our tabular datasets from scratch.)

* The collection of tabular datasets from each level might be stored in a **relational database**

* The place where you will usually see hierarchical data is when querying **APIs**.



::: lectitle
# Webscraping

[Slides](slides-16-webscraping.html)
:::

---
title: "Webscraping"
format:
  revealjs: 
    theme: [default, spring.scss]
    incremental: true 
    output-file: slides-16-webscraping.html
    shift-heading-level-by: -1
execute:
  echo: true
---

```{python}

```

```{r}
#| include: false
library(reticulate)
use_python("/usr/local/bin/python3")
```

```{python}
#| include: false
import numpy as np
import pandas as pd
from plotnine import *
```


## HTML

### HyperText Markup Language (HTML){.smaller}

*   HTML is the standard language for describing the layout of webpages.

*   It is like XML, with special "tags" for hyperlinks, tables, images, etc.

* You don't need to be an HTML expert to scrape webpages, but you do need to know a few basics.
    
* You can view the HTML of a website by:
    + Putting `view-source:` in front of the url
    + Right-click and choose "View Page Source" 

* You can explore HTML "interactively" by 
    + Right-click and "inspect" 
    + Ctrl/Cmd+Shift+I

### Hyperlinks

The `<a>` **tag** indicates a (hyper)link.

-   The `href=` **attribute** contains the URL.

-   The displayed text is within the `<a>` tag.

-   <a href="www.kelly-bodwin.com">This is a link</a>


### Tables

The `<table>` tag indicates a table.

* The `<tr>` tag indicates a row.

* The `<th>` and `<td>` tags indicate a cell within a row.

* Don't forget that each tag also has a *closing tag* (`</table>`) at the end!

### Tables

::::{.columns}

:::{.column width="50%"}

```
<table>
  <tr>
    <th>Rank</th>
    <th>Player</th>
    <th>Saves</th>
  </tr>
  <tr>
    <td>1</td>
    <td>Mariano Rivera</td>
    <td>652</td>
  </tr>
  <tr>
    <td>2</td>
    <td>Trevor Hoffman</td>
    <td>601</td>
  </tr>
</table>
```

:::

:::{.column width="50%"}

<table>
  <tr>
    <th>Rank</th>
    <th>Player</th>
    <th>Saves</th>
  </tr>
  <tr>
    <td>1</td>
    <td>Mariano Rivera</td>
    <td>652</td>
  </tr>
  <tr>
    <td>2</td>
    <td>Trevor Hoffman</td>
    <td>601</td>
  </tr>
</table>

:::
::::

## Web Scraping

### Web Scraping{.smaller}

* **Web scraping** is the process of getting information from a website to a dataset, without an API.

* We "simply" search through the raw HTML text, using tags to guides us to what we want.

* (What if what you want isn't nicely in tags?  Regular expressions.)

* The `python` function used to make searching HTML is easier is `BeautifulSoup`

* The challenging parts of webscraping are
    + Identifying the **tag structure** or other *consistent structure* that contains your data.
    + Looping through to find the data items and put them in the right place in a dataset

### Let's try it!

Let's use what we've just learned to scrape some data!

[Colab link](https://colab.research.google.com/drive/1neQvH5uqoX1j74rgCbperbi-HV3uLd8N?usp=sharing)


## Web Scraping and GenAI

### Using GenAI

* GenAI tools can help you "fish" through messy HTML

* As the AI to **write code** for you (which you then test and edit, of course!), not to **scrape the data** for you.

* Why?  
    + It's not great at scraping from multiple pages
    + Hallucinations!

* (Example)

## Ethics of Web Scraping

### Ethical Considerations

* Website owners have to pay a small amount each time you visit a webpage.

* This is usually offset by advertising.

* But when you do web scraping:
    + it is easy to rack up a huge number of webpage visits,
    + and you don't see any ads to offset this cost.
    
* Heavy scraping can hit the website so much or so frequently that it crashes!
    
    
### robots.txt

* Most websites have a `robots.txt` file in the home directory that
indicate which bots are allowed to scrape and which pages they can scrape.

* Here are a few examples:

    -   <http://www.espn.com/robots.txt>

    -   <http://www.nytimes.com/robots.txt>

* However, `robots.txt` is informational only. It doesn't *prevent*
    bots from scraping a webpage.

### Preventing Web Scraping

* Some websites take more drastic measures to prevent web scraping...

![image](./images/scraping.png)
## Takeaways

### Takeaways

* Web pages are made with **HTML**, and we can use that structure to **scrape** data automatically

* Best case scenario: Your data lives in a `<table>`

* Next best case:  There is some other consistent tag structure

* (Worst case:  Need to use regular expressions)

* **Always** check `robots.txt` before scraping a site!

* **Always** build a small pause into your loop for scraping.

* GenAI can help you with the "fiddly" part of identifying HTML structure.  (But **always** check and test its results!)


