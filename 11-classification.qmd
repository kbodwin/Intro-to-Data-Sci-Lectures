---
title: "Classification"
format:
  revealjs: 
    theme: [default, spring.scss]
    incremental: true 
    output-file: slides-11-classification.html
    shift-heading-level-by: -1
execute:
  echo: true
---

```{python}

```

```{r}
#| include: false
library(reticulate)
use_python("/usr/local/bin/python3")
```

```{python}
#| include: false
import numpy as np
import pandas as pd
from plotnine import *
```

## The story so far

### Choosing a Best Model

* We select a **best model** - aka best *prediction procedure* - by **cross-validation**.

* **Feature selection:** Which *predictors* should we include, and how should we preprocess them? 

* **Model selection:** Should we use *Linear Regression* or *KNN* or *Decision Trees* or something else? 

* **Hyperparameter tuning:** Choosing model-specific settings, like $k$ for KNN.

* Each candidate is a **pipeline**; use `GridSearchCV` or `cross_val_score` to score the options

## Case Study: Breast Tissue Classification

### Breast Tissue Classification

Electrical signals can be used to detect whether tissue is cancerous.

::: center
![image](./images/breast_diagram){width=".4\\textwidth"}
:::

The goal is to determine whether a sample of breast tissue is:

::::{.columns}
:::{.column}
1. connective tissue
2. adipose tissue
3. glandular tissue
:::

:::{.column}

4. carcinoma
5. fibro-adenoma
6. mastopathy
:::



### Reading in the Data

We will focus on two features:

-   $I_0$: impedivity at 0 kHz,
-   $PA_{500}$: phase angle at 500 kHz.

``` {python}
import pandas as pd
df = pd.read_csv("https://datasci112.stanford.edu/data/BreastTissue.csv")
df
```


### Visualizing the Data

```{python}
#| code-fold: true
from plotnine import *


(ggplot(df, aes(x = "I0", y = "PA500", fill = "Class"))
+ geom_point())
```

## K-Nearest Neighbors Classification

### K-Nearest Neighbors

What would we predict for someone with an $I_0$ of 400 and a $PA_{500}$ of 0.18?

``` {python}
X_train = df[["I0", "PA500"]]
y_train = df["Class"]

X_unknown = pd.DataFrame({"I0": [400], "PA500": [.18]})
X_unknown
```

### K-Nearest Neighbors

```{python}
#| code-fold: true
from plotnine import *

(ggplot()
+ geom_point(df, aes(x = "I0", y = "PA500", fill = "Class"))
+ geom_point(X_unknown, aes(x = "I0", y = "PA500"), size = 4)
)
```


### K-Nearest Neighbors

This process is *almost* identical to KNN *Regression*:

``` {python}
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(
    StandardScaler(),
    KNeighborsClassifier(n_neighbors=5, metric="euclidean"))

pipeline = pipeline.fit(X_train, y_train)
pipeline.predict(X_unknown)
```

### Probabilities

Which of these two unknown points would we be **more sure** about in our guess?

```{python}
#| code-fold: true
from plotnine import *

X_unknown = pd.DataFrame({"I0": [400, 2200], "PA500": [.18, 0.05]})

(ggplot()
+ geom_point(df, aes(x = "I0", y = "PA500", fill = "Class"))
+ geom_point(X_unknown, aes(x = "I0", y = "PA500"), size = 4)
)
```


### Probabilities

Instead of returning a single predicted class, we can ask it to return
the predicted probabilities for each class.

``` {python}
pipeline.predict_proba(X_unknown)
```

``` {python}
pipeline.classes_
```

How did Scikit-Learn calculate these predicted probabilities?


### Cross-Validation for Classification

We need a different **scoring method** for classification. A simple one is
**accuracy**:

$$\text{accuracy} = \frac{\text{# correct predictions}}{\text{# predictions}}.$$

### Cross-Validation for Classification

``` {python}
from sklearn.model_selection import cross_val_score

scores = cross_val_score(
    pipeline, X_train, y_train,
    scoring="accuracy",
    cv=10)
    
scores
```

    array([0.63636364, 0.81818182, 0.45454545, 0.54545455, 0.63636364,
           0.54545455, 0.5       , 0.6       , 0.4       , 0.7       ])

### Cross-Validation for Classification

As before, we can get an overall estimate of test accuracy by averaging
the cross-validation accuracies:

``` {.python bgcolor="gray"}
scores.mean()
```

But!  Accuracy is not always the best measure of a classification model!

### Confusion matrix

```{python}
from sklearn.metrics import confusion_matrix
pipeline = pipeline.fit(X_train, y_train)
y_train_predicted = pipeline.predict(X_train)
confusion_matrix(y_train, y_train_predicted)
```
### Confusion matrix

```{python}
pd.DataFrame(confusion_matrix(y_train, y_train_predicted), columns = pipeline.classes_, index = pipeline.classes_)
```
## Activity

### Activity

Use a **grid search** and the **accuracy** score to find the *best* k-value for this modeling problem.

## Classification Metrics

### Case Study: Credit Card Fraud

Data set of credit card transactions from Vesta.

Goal: Predict `isFraud`, where 1 indicates a fraudulent transaction.

``` {python}
df_fraud = pd.read_csv("https://datasci112.stanford.edu/data/fraud.csv")
df_fraud
```


### Classification Model

We can use $k$-nearest neighbors for classification:

``` {python}
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import make_pipeline
from sklearn.compose import make_column_transformer

pipeline = make_pipeline(
    make_column_transformer(
        (OneHotEncoder(handle_unknown="ignore", sparse_output=False),
         ["card4", "card6", "P_emaildomain"]),
        remainder="passthrough"),
    StandardScaler(),
    KNeighborsClassifier(n_neighbors=5))
```

### Training a Classifier

``` {python}
X_train = df_fraud.drop("isFraud", axis="columns")
y_train = df_fraud["isFraud"]
```

```{python}
from sklearn.model_selection import cross_val_score

cross_val_score(
    pipeline,
    X=X_train, y=y_train,
    scoring="accuracy",
    cv=10
).mean()
```


How is the accuracy so high?

### A Closer Look

Let's take a closer look at the labels.

``` {python}
y_train.value_counts()
```

The vast majority of transactions are normal (0)!


### Imbalanced data

If we just predicted that every transaction is normal, the accuracy
would be $1 - \frac{2119}{59054} = .964$.

Even though such predictions would be accurate *overall*, it is
inaccurate for fraudulent transactions. A good model is "accurate for
every class".


## Precision and Recall

* We need a score that measures "accuracy for class $c$".

* There are at least two reasonable definitions:

-   **precision**: $P(\text{correct} | \text{predicted class } c)$

    Among the observations that were predicted to be in class $c$, what
    proportion actually were?

-   **recall**: $P(\text{correct} | \text{actual class} c)$.

    Among the observations that were actually in class $c$, what
    proportion were predicted to be?


### Precision and Recall by Hand

To check our understanding of these definitions, let's calculate a few
precisions and recalls by hand.

First, summarize the results by the **confusion matrix**.

```{python}
#| code-fold: true
from sklearn.metrics import confusion_matrix
pipeline.fit(X_train, y_train)
y_train_ = pipeline.predict(X_train)
confusion_matrix(y_train, y_train_)
```

-   What is the (training) accuracy?

-   What's the precision for normal transactions?

-   What's the recall for normal transactions?

-   What's the precision for fraudulent transactions?

-   What's the recall for fraudulent transactions?

### Tradeoff between Precision and Recall

Can you imagine a classifier that always has 100% recall for class $c$,
no matter the data?

In general, if the model classifies more observations as $c$,

-   recall (for class $c$) $\uparrow$

-   precision (for class $c$) $\downarrow$

How do we compare two classifiers, if one has higher precision and the
other has higher recall?

### F1 Score

The **F1 score** combines precision and recall into a single score:

$$\text{F1 score} = \text{harmonic mean of precision and recall}$$
$$= \frac{2} {\left( \frac{1}{\text{precision}} + \frac{1}{\text{recall}}\right)}$$

* To achieve a high F1 score, both precision and recall have to be high.

* If either is low, then the harmonic mean will be low.

### Estimating Test Precision, Recall, and F1

* Remember that each class has its own precision, recall, and F1.

* But Scikit-Learn requires that the `scoring=` parameter be a
single number.

* For this, we can average the score over the classes:
    + `"precision_macro"`
    + `"recall_macro"`
    + `"f1_macro"`



### F1 Score

``` {python}
cross_val_score(
    pipeline,
    X=X_train, y=y_train,
    scoring="f1_macro",
    cv=10
).mean()
```


### Precision-Recall Curve

Another way to illustrate the tradeoff between precision and recall is
to graph the **precision-recall curve**.

First, we need the predicted probabilities.

``` {python}
y_train_probs_ = pipeline.predict_proba(X_train)
y_train_probs_
```


### Precision-Recall Curve

* By default, Scikit-Learn classifies a transaction as fraud if this
probability is $> 0.5$.

* What if we instead used a threshold $t$ other than $0.5$?

* Depending on which $t$ we pick, we'll get a different precision and
recall. We can graph this tradeoff.


### Precision-Recall Curve

Let's graph the precision-recall curve together in a Colab.

[Link](https://colab.research.google.com/drive/1w-4iGAiL3iXqAl3332teOT0gXv1wNxio?usp=sharing)


## Takeaways

### Takeaways

* We can do **KNN for Classification** by letting the nearest neighbors "vote"

* The number of votes is a "probability"

* A **classification model** must be evaluated differently than a **regression model**.

* One possible metric is **accuracy**, but this is a bad choice in situations with **imbalanced data**.

* **Precision** measures "if we say it's in Class A, is it really?"

* **Recall** measures "if it's really in Class A, did we find it?"

* **F1 Score** is a balance of precision and recall

* **Macro F1 Score** averages the F1 scores of all classes


