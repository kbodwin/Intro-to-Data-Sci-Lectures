---
title: "Unsupervised Learning with K-Means"
format:
  revealjs: 
    theme: [default, spring.scss]
    incremental: true 
    output-file: slides-13-kmeans.html
    shift-heading-level-by: -1
execute:
  echo: true
---

```{python}

```

```{r}
#| include: false
library(reticulate)
use_python("/usr/local/bin/python3")
```

```{python}
#| include: false
import numpy as np
import pandas as pd
from plotnine import *
```

## The story so far

### Predictive modeling

* In **predictive modeling**, a.k.a. **supervised machine learning**, we have a **target variable** we want to predict.

* We expect to have observations where we know the *predictors* but not the *target*.

* Our goal is to choose a **modeling procedure** to guess targets from predictors.

* We use **cross-validation** to estimate the **test error** of various procedure options.

* We might compare different:
    + feature sets
    + preprocessing choices
    + model specifications/algorithms
    + tuning parameters
    
    
## Unsupervised Learning

### Unsupervised Learning

* In **unsupervised** situations, we do not have a target variable $y$

* We *do* still have **features** that we observe.

* Our goal: Automatically find *interesting structure* in the features.

* (Think of children playing with Legos.  They might be *supervised* by parents who help them follow instructions, or they might be left alone to build whatever they want!)

### Clustering

* Nearly all unsupervised learning algorithms can be called **clustering**.

* The goal is to use the **observed features** (columns) to sort the **observations** (rows) into similar **clusters** (groups).

* For example: Suppose I take all of your grades in the gradebook as *features* and then use these to find *clusters* of students.  These clusters might represent...
    + people who studied together
    + people who are in the same section
    + people who have the same major or background
    + ... or none of the above!
    
    
### Uses of clustering

*   **Ecology:** An ecologist wants to group organisms into types to define different species.  *(rows = organisms; features = habitat, size, etc.)*

* **Biology:** A geneticist wants to know which groups of genes tend to be activated at the same time. *(rows = genes; features = activation at certain times)*

*   **Market Segmentation:** A business wants to group their customers
    into types.  *(rows = customers, features = age, location, etc.)*

*   **Language:** A linguist might want to identify different uses of
    ambiguous words like "set" or "run". *(rows = words; features = other words they are used with)*
    
*  **Documents:** A historian might want to find groups of articles that are on similar topics.  *(rows = articles; features = tf-idf transformed n-grams)*
    
    
    
## K-Means

### The K-Means Algorithm

* **Idea:** Two observations are *similar* if they are *close* in distance.

* (Just like with K-Nearest-Neighbors!)

* We should look for groups of observations that are close to the same **centroid**.


### The K-Means Algorithm

[Demo](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/)

Procedure (3-means):

1. Choose 3 random observations to be the **initial centroids**.

2. For each observation, determine which is the **closest centroid**.

3. Create 3 **clusters** by closest centroid.

4. Find the **new centroid** of each cluster.

5. Repeat until the clusters don't change.


### Example: Penguins data

```{python}
import numpy as np
import pandas as pd

data_dir = "https://dlsun.github.io/stats112/data/"
df_penguins = pd.read_csv(data_dir + "penguins.csv").dropna()
df_penguins
```

### Example: Penguins data

```{python}
#| code-fold: true
from plotnine import *
(ggplot(df_penguins, aes(x = "bill_length_mm", y = "flipper_length_mm"))
+ geom_point())
```

### Step 0: Standardize the data

**Why is this important?**

```{python}
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler().set_output(transform = "pandas")
df_scaled = scaler.fit_transform(df_penguins[["bill_length_mm", "flipper_length_mm"]]).dropna()

df_scaled.head()
```

### Step 1: Choose 3 random points to be centroids

```{python}
centroids = df_scaled.sample(3, random_state=1234)
centroids.index = ["orange", "purple", "green"]

centroids
```

### Step 1: Choose 3 random points to be centroids

```{python}
#| code-fold: true
from plotnine import *
(ggplot(df_scaled, aes(x = "bill_length_mm", y = "flipper_length_mm"))
+ geom_point()
+ geom_point(centroids, color = centroids.index, size = 3)
+ theme_classic())
```

### Step 2:  Assign each point to nearest centroid

```{python}
from sklearn.metrics import pairwise_distances

dists = pairwise_distances(df_scaled, centroids)
dists
```

### Step 2:  Assign each point to nearest centroid

```{python}
closest_centroid = dists.argsort()[:,0]
closest_centroid
```

### Step 2:  Assign each point to nearest centroid

```{python}
df_scaled.index = centroids.index[closest_centroid]
df_scaled.head()
```

### Step 2:  Assign each point to nearest centroid

```{python}
#| code-fold: true
from plotnine import *
(ggplot(df_scaled, aes(x = "bill_length_mm", y = "flipper_length_mm"))
+ geom_point(color = df_scaled.index)
+ geom_point(centroids, color = centroids.index, size = 3)
+ theme_classic())
```

### Step 3:  Find new centroids

```{python}
centroids = df_scaled.groupby(df_scaled.index).mean()
centroids
```

### Step 3:  Find new centroids

```{python}
#| code-fold: true
from plotnine import *
(ggplot(df_scaled, aes(x = "bill_length_mm", y = "flipper_length_mm"))
+ geom_point(color = df_scaled.index)
+ geom_point(centroids, color = centroids.index, size = 3)
+ theme_classic())
```

### Step 4:  Repeat over and over!

```{python}
for i in range(1,3):
  dists = pairwise_distances(df_scaled, centroids)
  df_scaled.index = centroids.index[closest_centroid]
  centroids = df_scaled.groupby(df_scaled.index).mean()
  print(centroids)
```

### K-means in sklearn

```{python}
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

model = KMeans(n_clusters=3, random_state=1234)

pipeline = make_pipeline(
    StandardScaler(),
    model
)

pipeline.fit(df_penguins[["bill_length_mm", "flipper_length_mm"]].dropna())
```

### K-means in sklearn

```{python}
centroids = model.cluster_centers_
clusters = model.labels_

clusters
```
### Interpreting K-Means

The key takeaway here is the **cluster centers**:

```{python}
centroids
```

* Cluster 1 has long bill, shortish flipper

* Cluster 2 has longish bill, long flipper

* Cluster 3 has short bill, short flipper

### Interpreting K-Means

We also might check if these clusters match any labels that we already know:

```{python}
results = pd.DataFrame({
  "cluster": clusters,
  "species": df_penguins['species']
})

results
```

### Interpreting K-Means

We also might check if these clusters match any labels that we already know:

```{python}
results.value_counts(["cluster", "species"]).unstack()
```
## Your Turn

### Activity

* Fit a 3-means model using *all* the numeric predictors in the penguins data.

* Describe each cluster

* Do these clusters match up to the species?

* Then fit a 6-means model

* Do those clusters match up to species and island?


## Takeaways

### Takeaways

* **unsupervised learning** is a way to find *structure* in data

* **k-means** is the most common **clustering method**

* We **have to choose K ahead of time**

* (This is a big problem!  Why can't we tune?)

