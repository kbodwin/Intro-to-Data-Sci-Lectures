---
title: "Dummy variables and Column Transformers"
format:
  revealjs: 
    theme: [default, spring.scss]
    incremental: true 
    output-file: slides-06-preprocessing.html
    shift-heading-level-by: -1
execute:
  echo: true
---

```{python}

```

```{r}
#| include: false
library(reticulate)
use_python("/usr/local/bin/python3")
```

```{python}
#| include: false
import numpy as np
import pandas as pd
from plotnine import *
```

## The story this week

### Distances


* We measure similarity between **observations** by calculating **distances**.

* **Euclidean distance**: sum of squared differences, then square root

* **Manhattan distance**: sum of absolute differences

* In **scikit-learn**, use the `pairwise_distances` function to get back a *2D numpy array* of distances.

### Scaling

* It is important that all our **features** be on the same **scale** for distances to be meaningful.

* **Standardize:**  Subtract the *mean* (of the column) and divide by the *standard deviation* (of the column).

* **MinMax:** Subtract the *minimum* value, divide by the *range*.

* In `scikit-learn`, use the `StandardScaler()` or `MinMaxScaler()` functions.

* Follow the **specify** - **fit** - **transform** code structure.

### Recall: AMES Housing data


``` {python}
df = pd.read_table("https://datasci112.stanford.edu/data/housing.tsv")
features = ["Gr Liv Area", "Bedroom AbvGr", "Full Bath", "Half Bath", "Bldg Type", "Neighborhood"]
df[features].head()
```

## Distances and Categorical Variables

### What about categorical variables?

Suppose we want to include the variable `Bldg Type` in our distance calculation...

```{python}
df["Bldg Type"].value_counts()
```
What is "single family minus townhouse squared"?

### Converting to binary

Let's instead think about "Single family, or not"

```{python}
df["is_single_fam"] = df["Bldg Type"] == "1Fam"
df["is_single_fam"].value_counts()
```
### Converting to binary

Recall that `True/False` is the same as `1/0` for computers:

```{python}
df["is_single_fam"] = df["is_single_fam"].astype("int")
df["is_single_fam"].value_counts()
```
We call this a **dummy variable** or a **one-hot-encoding**.

### Now we can do math!

```{python}
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import pairwise_distances
scaler = StandardScaler()

df_orig = df[['Gr Liv Area', 'Bedroom AbvGr', 'is_single_fam']]
scaler.fit(df_orig)

df_scaled = scaler.transform(df_orig)

dists = pairwise_distances(df_scaled[[1707]], df_scaled)
best = dists.argsort().flatten()[1:10]
df_orig.iloc[best]
```

### Quick look back

#### Where have you seen some one-hot-encoded variables already?


### Let's reset the dataset now...

```{python}
df = pd.read_table("https://datasci112.stanford.edu/data/housing.tsv")
```


## Dummifying Variables

### Dummifying Variables

* What if we don't just want to study `is_single_fam`, but rather, *all* categories of the `Bldg Type` variable?

* In principle, we just make **dummy variables** for **each category**:  `is_single_fam`, `is_twnhse`, etc.

* Each **category** becomes one **column**, with 0's and 1's to show if the *observation in that row* matches that *category*.

* This is called **dummifying** or **one-hot-encoding** a **categorical variable**

* Luckily, we have shortcuts in both `pandas` *and* `sklearn`...

### Dummifying in Pandas

```{python}
pd.get_dummies(df[["Bldg Type"]])
```

### Dummifying in Pandas

Some things to notice here...

1.  What is the **naming convention** for the new columns?

2. Does this change the original dataframe `df`?  If not, what would you need to do to add this information back in?

3. What happens if you put the whole dataframe into the `get_dummies` function?  What problems might arise from this?

### Dummifying in sklearn

```{python}
from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder()

encoder.fit(df[["Bldg Type"]])

df_bldg = encoder.transform(df[["Bldg Type"]])

df_bldg
```


### Dummifying in sklearn

```{python}
df_bldg.todense()
```

### Dummifying in sklearn

Things to notice:

1. What **object type** was the result?

2. Does this change the original dataframe `df`?  If not, what would you need to do to add this information back in?

3. What happens if you fit the whole dataframe with the OneHotEncoder?  What problems might arise from this?

4. What pros and cons do you see for the `pandas` approach vs the `sklearn` approach?


## Column Transformers

### Preprocessing

* We have now seen two **preprocessing** steps that might need to happen to do an analysis of distances:
    + **Scaling** the quantitative variables
    + **Dummifying** the categorical variables
    
* **Preprocessing** steps are things you do *only to make the following analysis/visualization better*.
    
* This is not the same as **data cleaning**, which are changes you make to *fix* the data.

* This is not the same as **data wrangling**, which are changes you make to *restructure* the data; i.e., adding or deleting rows or columns to reflect what you are trying to study.

### Quick quiz

Identify the following as *cleaning*, *wrangling*, or *preprocessing*:

1. Removing the `$` symbol from a column and converting it to numeric.

2. Narrowing your data down to only first class Titanic passengers, because you are not studying the others.

3. Converting a `Zip Code` variable from numeric to categorical using `.astype()`.

4. Creating a new column called `n_investment` that counts the number of people who invested in a project.

5. Log-transforming a column because it is very skewed.

### Preprocessing in `sklearn`

* Unlike **cleaning** and **wrangling**, the **preprocessing** steps are "temporary" changes to the dataframe.

* It would be nice if we could trigger these changes as part of our analysis, instead of doing them "by hand".

* This is why the **specify** - **fit** - **transform** process is useful!

* We will first specify **all** our preprocessing steps.

* Then we will **fit** the whole preprocess

* Then we will save the **transform** step for only when we need it.

### Column Transformers

```{python}
from sklearn.compose import make_column_transformer

preproc = make_column_transformer(
    (OneHotEncoder(), ["Bldg Type", "Neighborhood"]),
    remainder="passthrough")
    
preproc.fit(df[features])

preproc.transform(df[features])
```

### Column Transformers

Things to notice...

1. What submodule did we import `make_column_transformer` from?

2. What are the **two** arguments to the `make_column_transformer` function?  What **object structures** are they?

3. What happens if you **fit** and **transform** on the whole dataset, not just `df[features]`?  Why might this be useful?


### Column Transformers

Try the following:

1. What happens if you change `remainder = "passthrough"` to `remainder = "drop"`? 

2. What happens if you add the argument `sparse_output=False` to the `OneHotEncoder()` function?

3. What happens if you add this line before the *transform* step: *(keep the `sparse_output=False` when you try this)* 
```
preproc.set_output(transform = "pandas")
```

4. Look at the `preproc` object.  What does it show you?

### Multiple preprocessing steps

Why are **column transformers** so useful?  

Well, now we can do **multiple preprocessing steps** at once!

```{python}
from sklearn.preprocessing import StandardScaler

preproc = make_column_transformer(
        (StandardScaler(), ["Gr Liv Area"]),
        (OneHotEncoder(sparse_output=False), ["Bldg Type",
                                          "Neighborhood"]),
    remainder="passthrough")
    
preproc.fit(df[features])

preproc.set_output(transform = "pandas")

df_transformed = preproc.transform(df[features])
df_transformed
```

### Finding all variables

What if we just want to say "Please dummify **all** categorical variables?"

Use a `selector` instead of exact column names.

```{python}
from sklearn.compose import make_column_selector

preproc = make_column_transformer(
    (StandardScaler(),  make_column_selector(dtype_include=np.number)),
    (OneHotEncoder(sparse_output=False), make_column_selector(dtype_include=object)),
    remainder="passthrough")
    
preproc.fit(df[features])

preproc.set_output(transform = "pandas")

df_transformed = preproc.transform(df[features])
df_transformed

```


### Think about it

* What are the *advantages* of using a selector?

* What are the possible *disadvantages* of using a selector?

* Does the *order* matter when using selectors?  Try switching the steps and see what happens!


## Takeaways

### Takeaways

* We **dummify** or **one-hot-encode** categorical variables to make them numbers.

* We can do this with `pd.get_dummies()` or with `OneHotEncoder()`

* **Column Transformers** let us apply multiple preprocessing steps all together.
    + Think about *which variables* you want to apply the steps to
    + Think about *options* for the steps, like sparseness
    + Think about `passthrough` in your transformer
